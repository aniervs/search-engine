{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "0CqJ8NLuTokR"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from pprint import pprint\n",
    "from glob import glob\n",
    "import random\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  wiki_data.zip\n",
      "   creating: wiki_data/\n",
      "  inflating: wiki_data/texts390.txt  \n",
      "  inflating: wiki_data/texts975.txt  \n",
      "  inflating: wiki_data/texts60.txt   \n",
      "  inflating: wiki_data/texts774.txt  \n",
      "  inflating: wiki_data/texts373.txt  \n",
      "  inflating: wiki_data/texts728.txt  \n",
      "  inflating: wiki_data/texts852.txt  \n",
      "  inflating: wiki_data/texts418.txt  \n",
      "  inflating: wiki_data/texts634.txt  \n",
      "  inflating: wiki_data/texts311.txt  \n",
      "  inflating: wiki_data/texts1024.txt  \n",
      "  inflating: wiki_data/texts858.txt  \n",
      "  inflating: wiki_data/texts4.txt    \n",
      "  inflating: wiki_data/texts86.txt   \n",
      "  inflating: wiki_data/texts775.txt  \n",
      "  inflating: wiki_data/texts869.txt  \n",
      "  inflating: wiki_data/texts613.txt  \n",
      "  inflating: wiki_data/texts849.txt  \n",
      "  inflating: wiki_data/texts442.txt  \n",
      "  inflating: wiki_data/texts871.txt  \n",
      "  inflating: wiki_data/texts415.txt  \n",
      "  inflating: wiki_data/texts696.txt  \n",
      "  inflating: wiki_data/texts937.txt  \n",
      "  inflating: wiki_data/texts17.txt   \n",
      "  inflating: wiki_data/texts646.txt  \n",
      "  inflating: wiki_data/texts527.txt  \n",
      "  inflating: wiki_data/texts653.txt  \n",
      "  inflating: wiki_data/texts460.txt  \n",
      "  inflating: wiki_data/texts579.txt  \n",
      "  inflating: wiki_data/texts348.txt  \n",
      "  inflating: wiki_data/texts1039.txt  \n",
      "  inflating: wiki_data/texts932.txt  \n",
      "  inflating: wiki_data/texts785.txt  \n",
      "  inflating: wiki_data/texts192.txt  \n",
      "  inflating: wiki_data/texts62.txt   \n",
      "  inflating: wiki_data/texts487.txt  \n",
      "  inflating: wiki_data/texts628.txt  \n",
      "  inflating: wiki_data/texts327.txt  \n",
      "  inflating: wiki_data/texts589.txt  \n",
      "  inflating: wiki_data/texts207.txt  \n",
      "  inflating: wiki_data/texts721.txt  \n",
      "  inflating: wiki_data/texts176.txt  \n",
      "  inflating: wiki_data/texts279.txt  \n",
      "  inflating: wiki_data/texts1000.txt  \n",
      "  inflating: wiki_data/texts1013.txt  \n",
      "  inflating: wiki_data/texts1029.txt  \n",
      "  inflating: wiki_data/texts743.txt  \n",
      "  inflating: wiki_data/texts649.txt  \n",
      "  inflating: wiki_data/texts624.txt  \n",
      "  inflating: wiki_data/texts117.txt  \n",
      "  inflating: wiki_data/texts178.txt  \n",
      "  inflating: wiki_data/texts1023.txt  \n",
      "  inflating: wiki_data/texts860.txt  \n",
      "  inflating: wiki_data/texts598.txt  \n",
      "  inflating: wiki_data/texts189.txt  \n",
      "  inflating: wiki_data/texts471.txt  \n",
      "  inflating: wiki_data/texts173.txt  \n",
      "  inflating: wiki_data/texts273.txt  \n",
      "  inflating: wiki_data/texts930.txt  \n",
      "  inflating: wiki_data/texts1.txt    \n",
      "  inflating: wiki_data/texts7.txt    \n",
      "  inflating: wiki_data/texts797.txt  \n",
      "  inflating: wiki_data/texts677.txt  \n",
      "  inflating: wiki_data/texts645.txt  \n",
      "  inflating: wiki_data/texts738.txt  \n",
      "  inflating: wiki_data/texts1019.txt  \n",
      "  inflating: wiki_data/texts688.txt  \n",
      "  inflating: wiki_data/texts502.txt  \n",
      "  inflating: wiki_data/texts560.txt  \n",
      "  inflating: wiki_data/texts531.txt  \n",
      "  inflating: wiki_data/texts840.txt  \n",
      "  inflating: wiki_data/texts31.txt   \n",
      "  inflating: wiki_data/texts940.txt  \n",
      "  inflating: wiki_data/texts288.txt  \n",
      "  inflating: wiki_data/texts120.txt  \n",
      "  inflating: wiki_data/texts609.txt  \n",
      "  inflating: wiki_data/texts156.txt  \n",
      "  inflating: wiki_data/texts796.txt  \n",
      "  inflating: wiki_data/texts707.txt  \n",
      "  inflating: wiki_data/texts317.txt  \n",
      "  inflating: wiki_data/texts720.txt  \n",
      "  inflating: wiki_data/texts394.txt  \n",
      "  inflating: wiki_data/texts455.txt  \n",
      "  inflating: wiki_data/texts21.txt   \n",
      "  inflating: wiki_data/texts953.txt  \n",
      "  inflating: wiki_data/texts906.txt  \n",
      "  inflating: wiki_data/texts859.txt  \n",
      "  inflating: wiki_data/texts61.txt   \n",
      "  inflating: wiki_data/texts884.txt  \n",
      "  inflating: wiki_data/texts99.txt   \n",
      "  inflating: wiki_data/texts792.txt  \n",
      "  inflating: wiki_data/texts936.txt  \n",
      "  inflating: wiki_data/texts780.txt  \n",
      "  inflating: wiki_data/texts280.txt  \n",
      "  inflating: wiki_data/texts1003.txt  \n",
      "  inflating: wiki_data/texts769.txt  \n",
      "  inflating: wiki_data/texts678.txt  \n",
      "  inflating: wiki_data/texts122.txt  \n",
      "  inflating: wiki_data/texts959.txt  \n",
      "  inflating: wiki_data/texts1005.txt  \n",
      "  inflating: wiki_data/texts284.txt  \n",
      "  inflating: wiki_data/texts699.txt  \n",
      "  inflating: wiki_data/texts798.txt  \n",
      "  inflating: wiki_data/texts949.txt  \n",
      "  inflating: wiki_data/texts844.txt  \n",
      "  inflating: wiki_data/texts740.txt  \n",
      "  inflating: wiki_data/texts973.txt  \n",
      "  inflating: wiki_data/texts433.txt  \n",
      "  inflating: wiki_data/texts546.txt  \n",
      "  inflating: wiki_data/texts124.txt  \n",
      "  inflating: wiki_data/texts725.txt  \n",
      "  inflating: wiki_data/texts905.txt  \n",
      "  inflating: wiki_data/texts406.txt  \n",
      "  inflating: wiki_data/texts66.txt   \n",
      "  inflating: wiki_data/texts698.txt  \n",
      "  inflating: wiki_data/texts154.txt  \n",
      "  inflating: wiki_data/texts356.txt  \n",
      "  inflating: wiki_data/texts900.txt  \n",
      "  inflating: wiki_data/texts342.txt  \n",
      "  inflating: wiki_data/texts1017.txt  \n",
      "  inflating: wiki_data/texts488.txt  \n",
      "  inflating: wiki_data/texts957.txt  \n",
      "  inflating: wiki_data/texts121.txt  \n",
      "  inflating: wiki_data/texts757.txt  \n",
      "  inflating: wiki_data/texts1008.txt  \n",
      "  inflating: wiki_data/texts741.txt  \n",
      "  inflating: wiki_data/texts919.txt  \n",
      "  inflating: wiki_data/texts540.txt  \n",
      "  inflating: wiki_data/texts948.txt  \n",
      "  inflating: wiki_data/texts779.txt  \n",
      "  inflating: wiki_data/texts360.txt  \n",
      "  inflating: wiki_data/texts424.txt  \n",
      "  inflating: wiki_data/texts938.txt  \n",
      "  inflating: wiki_data/texts142.txt  \n",
      "  inflating: wiki_data/texts363.txt  \n",
      "  inflating: wiki_data/texts827.txt  \n",
      "  inflating: wiki_data/texts912.txt  \n",
      "  inflating: wiki_data/texts103.txt  \n",
      "  inflating: wiki_data/texts641.txt  \n",
      "  inflating: wiki_data/texts183.txt  \n",
      "  inflating: wiki_data/texts714.txt  \n",
      "  inflating: wiki_data/texts753.txt  \n",
      "  inflating: wiki_data/texts891.txt  \n",
      "  inflating: wiki_data/texts148.txt  \n",
      "  inflating: wiki_data/texts10.txt   \n",
      "  inflating: wiki_data/texts981.txt  \n",
      "  inflating: wiki_data/texts583.txt  \n",
      "  inflating: wiki_data/texts170.txt  \n",
      "  inflating: wiki_data/texts807.txt  \n",
      "  inflating: wiki_data/texts833.txt  \n",
      "  inflating: wiki_data/texts495.txt  \n",
      "  inflating: wiki_data/texts283.txt  \n",
      "  inflating: wiki_data/texts22.txt   \n",
      "  inflating: wiki_data/texts218.txt  \n",
      "  inflating: wiki_data/texts805.txt  \n",
      "  inflating: wiki_data/texts842.txt  \n",
      "  inflating: wiki_data/texts38.txt   \n",
      "  inflating: wiki_data/texts666.txt  \n",
      "  inflating: wiki_data/texts195.txt  \n",
      "  inflating: wiki_data/texts434.txt  \n",
      "  inflating: wiki_data/texts480.txt  \n",
      "  inflating: wiki_data/texts929.txt  \n",
      "  inflating: wiki_data/texts659.txt  \n",
      "  inflating: wiki_data/texts57.txt   \n",
      "  inflating: wiki_data/texts42.txt   \n",
      "  inflating: wiki_data/texts89.txt   \n",
      "  inflating: wiki_data/texts243.txt  \n",
      "  inflating: wiki_data/texts129.txt  \n",
      "  inflating: wiki_data/texts593.txt  \n",
      "  inflating: wiki_data/texts627.txt  \n",
      "  inflating: wiki_data/texts93.txt   \n",
      "  inflating: wiki_data/texts45.txt   \n",
      "  inflating: wiki_data/texts205.txt  \n",
      "  inflating: wiki_data/texts451.txt  \n",
      "  inflating: wiki_data/texts525.txt  \n",
      "  inflating: wiki_data/texts499.txt  \n",
      "  inflating: wiki_data/texts3.txt    \n",
      "  inflating: wiki_data/texts144.txt  \n",
      "  inflating: wiki_data/texts211.txt  \n",
      "  inflating: wiki_data/texts485.txt  \n",
      "  inflating: wiki_data/texts54.txt   \n",
      "  inflating: wiki_data/texts1022.txt  \n",
      "  inflating: wiki_data/texts49.txt   \n",
      "  inflating: wiki_data/texts215.txt  \n",
      "  inflating: wiki_data/texts404.txt  \n",
      "  inflating: wiki_data/texts902.txt  \n",
      "  inflating: wiki_data/texts519.txt  \n",
      "  inflating: wiki_data/texts883.txt  \n",
      "  inflating: wiki_data/texts556.txt  \n",
      "  inflating: wiki_data/texts335.txt  \n",
      "  inflating: wiki_data/texts293.txt  \n",
      "  inflating: wiki_data/texts423.txt  \n",
      "  inflating: wiki_data/texts464.txt  \n",
      "  inflating: wiki_data/texts1007.txt  \n",
      "  inflating: wiki_data/texts967.txt  \n",
      "  inflating: wiki_data/texts762.txt  \n",
      "  inflating: wiki_data/texts941.txt  \n",
      "  inflating: wiki_data/texts222.txt  \n",
      "  inflating: wiki_data/texts557.txt  \n",
      "  inflating: wiki_data/texts358.txt  \n",
      "  inflating: wiki_data/texts972.txt  \n",
      "  inflating: wiki_data/texts1031.txt  \n",
      "  inflating: wiki_data/texts382.txt  \n",
      "  inflating: wiki_data/texts378.txt  \n",
      "  inflating: wiki_data/texts48.txt   \n",
      "  inflating: wiki_data/texts512.txt  \n",
      "  inflating: wiki_data/texts1045.txt  \n",
      "  inflating: wiki_data/texts604.txt  \n",
      "  inflating: wiki_data/texts313.txt  \n",
      "  inflating: wiki_data/texts169.txt  \n",
      "  inflating: wiki_data/texts1044.txt  \n",
      "  inflating: wiki_data/texts914.txt  \n",
      "  inflating: wiki_data/texts574.txt  \n",
      "  inflating: wiki_data/texts267.txt  \n",
      "  inflating: wiki_data/texts438.txt  \n",
      "  inflating: wiki_data/texts657.txt  \n",
      "  inflating: wiki_data/texts146.txt  \n",
      "  inflating: wiki_data/texts391.txt  \n",
      "  inflating: wiki_data/texts27.txt   \n",
      "  inflating: wiki_data/texts29.txt   \n",
      "  inflating: wiki_data/texts770.txt  \n",
      "  inflating: wiki_data/texts945.txt  \n",
      "  inflating: wiki_data/texts676.txt  \n",
      "  inflating: wiki_data/texts939.txt  \n",
      "  inflating: wiki_data/texts78.txt   \n",
      "  inflating: wiki_data/texts989.txt  \n",
      "  inflating: wiki_data/texts562.txt  \n",
      "  inflating: wiki_data/texts379.txt  \n",
      "  inflating: wiki_data/texts53.txt   \n",
      "  inflating: wiki_data/texts563.txt  \n",
      "  inflating: wiki_data/texts351.txt  \n",
      "  inflating: wiki_data/texts194.txt  \n",
      "  inflating: wiki_data/texts359.txt  \n",
      "  inflating: wiki_data/texts920.txt  \n",
      "  inflating: wiki_data/texts92.txt   \n",
      "  inflating: wiki_data/texts907.txt  \n",
      "  inflating: wiki_data/texts76.txt   \n",
      "  inflating: wiki_data/texts711.txt  \n",
      "  inflating: wiki_data/texts1010.txt  \n",
      "  inflating: wiki_data/texts715.txt  \n",
      "  inflating: wiki_data/texts392.txt  \n",
      "  inflating: wiki_data/texts301.txt  \n",
      "  inflating: wiki_data/texts82.txt   \n",
      "  inflating: wiki_data/texts943.txt  \n",
      "  inflating: wiki_data/texts115.txt  \n",
      "  inflating: wiki_data/texts1006.txt  \n",
      "  inflating: wiki_data/texts596.txt  \n",
      "  inflating: wiki_data/texts172.txt  \n",
      "  inflating: wiki_data/texts151.txt  \n",
      "  inflating: wiki_data/texts969.txt  \n",
      "  inflating: wiki_data/texts439.txt  \n",
      "  inflating: wiki_data/texts569.txt  \n",
      "  inflating: wiki_data/texts231.txt  \n",
      "  inflating: wiki_data/texts291.txt  \n",
      "  inflating: wiki_data/texts814.txt  \n",
      "  inflating: wiki_data/texts712.txt  \n",
      "  inflating: wiki_data/texts126.txt  \n",
      "  inflating: wiki_data/texts65.txt   \n",
      "  inflating: wiki_data/texts1015.txt  \n",
      "  inflating: wiki_data/texts706.txt  \n",
      "  inflating: wiki_data/texts523.txt  \n",
      "  inflating: wiki_data/texts528.txt  \n",
      "  inflating: wiki_data/texts622.txt  \n",
      "  inflating: wiki_data/texts368.txt  \n",
      "  inflating: wiki_data/texts986.txt  \n",
      "  inflating: wiki_data/texts817.txt  \n",
      "  inflating: wiki_data/texts934.txt  \n",
      "  inflating: wiki_data/texts835.txt  \n",
      "  inflating: wiki_data/texts834.txt  \n",
      "  inflating: wiki_data/texts361.txt  \n",
      "  inflating: wiki_data/texts30.txt   \n",
      "  inflating: wiki_data/texts113.txt  \n",
      "  inflating: wiki_data/texts486.txt  \n",
      "  inflating: wiki_data/texts83.txt   \n",
      "  inflating: wiki_data/texts429.txt  \n",
      "  inflating: wiki_data/texts867.txt  \n",
      "  inflating: wiki_data/texts449.txt  \n",
      "  inflating: wiki_data/texts890.txt  \n",
      "  inflating: wiki_data/texts505.txt  \n",
      "  inflating: wiki_data/texts958.txt  \n",
      "  inflating: wiki_data/texts819.txt  \n",
      "  inflating: wiki_data/texts988.txt  \n",
      "  inflating: wiki_data/texts501.txt  \n",
      "  inflating: wiki_data/texts750.txt  \n",
      "  inflating: wiki_data/texts162.txt  \n",
      "  inflating: wiki_data/texts249.txt  \n",
      "  inflating: wiki_data/texts789.txt  \n",
      "  inflating: wiki_data/texts974.txt  \n",
      "  inflating: wiki_data/texts58.txt   \n",
      "  inflating: wiki_data/texts97.txt   \n",
      "  inflating: wiki_data/texts991.txt  \n",
      "  inflating: wiki_data/texts558.txt  \n",
      "  inflating: wiki_data/texts272.txt  \n",
      "  inflating: wiki_data/texts577.txt  \n",
      "  inflating: wiki_data/texts561.txt  \n",
      "  inflating: wiki_data/texts135.txt  \n",
      "  inflating: wiki_data/texts517.txt  \n",
      "  inflating: wiki_data/texts661.txt  \n",
      "  inflating: wiki_data/texts848.txt  \n",
      "  inflating: wiki_data/texts416.txt  \n",
      "  inflating: wiki_data/texts1034.txt  \n",
      "  inflating: wiki_data/texts185.txt  \n",
      "  inflating: wiki_data/texts255.txt  \n",
      "  inflating: wiki_data/texts71.txt   \n",
      "  inflating: wiki_data/texts427.txt  \n",
      "  inflating: wiki_data/texts187.txt  \n",
      "  inflating: wiki_data/texts832.txt  \n",
      "  inflating: wiki_data/texts174.txt  \n",
      "  inflating: wiki_data/texts756.txt  \n",
      "  inflating: wiki_data/texts147.txt  \n",
      "  inflating: wiki_data/texts992.txt  \n",
      "  inflating: wiki_data/texts407.txt  \n",
      "  inflating: wiki_data/texts901.txt  \n",
      "  inflating: wiki_data/texts893.txt  \n",
      "  inflating: wiki_data/texts1004.txt  \n",
      "  inflating: wiki_data/texts245.txt  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  inflating: wiki_data/texts274.txt  \n",
      "  inflating: wiki_data/texts386.txt  \n",
      "  inflating: wiki_data/texts87.txt   \n",
      "  inflating: wiki_data/texts737.txt  \n",
      "  inflating: wiki_data/texts277.txt  \n",
      "  inflating: wiki_data/texts791.txt  \n",
      "  inflating: wiki_data/texts1018.txt  \n",
      "  inflating: wiki_data/texts478.txt  \n",
      "  inflating: wiki_data/texts897.txt  \n",
      "  inflating: wiki_data/texts343.txt  \n",
      "  inflating: wiki_data/texts100.txt  \n",
      "  inflating: wiki_data/texts565.txt  \n",
      "  inflating: wiki_data/texts420.txt  \n",
      "  inflating: wiki_data/texts81.txt   \n",
      "  inflating: wiki_data/texts46.txt   \n",
      "  inflating: wiki_data/texts12.txt   \n",
      "  inflating: wiki_data/texts248.txt  \n",
      "  inflating: wiki_data/texts52.txt   \n",
      "  inflating: wiki_data/texts865.txt  \n",
      "  inflating: wiki_data/texts354.txt  \n",
      "  inflating: wiki_data/texts241.txt  \n",
      "  inflating: wiki_data/texts70.txt   \n",
      "  inflating: wiki_data/texts39.txt   \n",
      "  inflating: wiki_data/texts997.txt  \n",
      "  inflating: wiki_data/texts227.txt  \n",
      "  inflating: wiki_data/texts755.txt  \n",
      "  inflating: wiki_data/texts1020.txt  \n",
      "  inflating: wiki_data/texts877.txt  \n",
      "  inflating: wiki_data/texts68.txt   \n",
      "  inflating: wiki_data/texts467.txt  \n",
      "  inflating: wiki_data/texts824.txt  \n",
      "  inflating: wiki_data/texts74.txt   \n",
      "  inflating: wiki_data/texts166.txt  \n",
      "  inflating: wiki_data/texts437.txt  \n",
      "  inflating: wiki_data/texts746.txt  \n",
      "  inflating: wiki_data/texts296.txt  \n",
      "  inflating: wiki_data/texts286.txt  \n",
      "  inflating: wiki_data/texts212.txt  \n",
      "  inflating: wiki_data/texts180.txt  \n",
      "  inflating: wiki_data/texts636.txt  \n",
      "  inflating: wiki_data/texts91.txt   \n",
      "  inflating: wiki_data/texts904.txt  \n",
      "  inflating: wiki_data/texts492.txt  \n",
      "  inflating: wiki_data/texts503.txt  \n",
      "  inflating: wiki_data/texts626.txt  \n",
      "  inflating: wiki_data/texts590.txt  \n",
      "  inflating: wiki_data/texts35.txt   \n",
      "  inflating: wiki_data/texts985.txt  \n",
      "  inflating: wiki_data/texts888.txt  \n",
      "  inflating: wiki_data/texts287.txt  \n",
      "  inflating: wiki_data/texts150.txt  \n",
      "  inflating: wiki_data/texts276.txt  \n",
      "  inflating: wiki_data/texts654.txt  \n",
      "  inflating: wiki_data/texts328.txt  \n",
      "  inflating: wiki_data/texts128.txt  \n",
      "  inflating: wiki_data/texts458.txt  \n",
      "  inflating: wiki_data/texts26.txt   \n",
      "  inflating: wiki_data/texts88.txt   \n",
      "  inflating: wiki_data/texts331.txt  \n",
      "  inflating: wiki_data/texts164.txt  \n",
      "  inflating: wiki_data/texts977.txt  \n",
      "  inflating: wiki_data/texts856.txt  \n",
      "  inflating: wiki_data/texts232.txt  \n",
      "  inflating: wiki_data/texts818.txt  \n",
      "  inflating: wiki_data/texts866.txt  \n",
      "  inflating: wiki_data/texts282.txt  \n",
      "  inflating: wiki_data/texts568.txt  \n",
      "  inflating: wiki_data/texts668.txt  \n",
      "  inflating: wiki_data/texts336.txt  \n",
      "  inflating: wiki_data/texts182.txt  \n",
      "  inflating: wiki_data/texts453.txt  \n",
      "  inflating: wiki_data/texts304.txt  \n",
      "  inflating: wiki_data/texts466.txt  \n",
      "  inflating: wiki_data/texts322.txt  \n",
      "  inflating: wiki_data/texts594.txt  \n",
      "  inflating: wiki_data/texts24.txt   \n",
      "  inflating: wiki_data/texts555.txt  \n",
      "  inflating: wiki_data/texts846.txt  \n",
      "  inflating: wiki_data/texts923.txt  \n",
      "  inflating: wiki_data/texts862.txt  \n",
      "  inflating: wiki_data/texts736.txt  \n",
      "  inflating: wiki_data/texts586.txt  \n",
      "  inflating: wiki_data/texts365.txt  \n",
      "  inflating: wiki_data/texts235.txt  \n",
      "  inflating: wiki_data/texts314.txt  \n",
      "  inflating: wiki_data/texts457.txt  \n",
      "  inflating: wiki_data/texts614.txt  \n",
      "  inflating: wiki_data/texts545.txt  \n",
      "  inflating: wiki_data/texts167.txt  \n",
      "  inflating: wiki_data/texts461.txt  \n",
      "  inflating: wiki_data/texts56.txt   \n",
      "  inflating: wiki_data/texts217.txt  \n",
      "  inflating: wiki_data/texts242.txt  \n",
      "  inflating: wiki_data/texts123.txt  \n",
      "  inflating: wiki_data/texts136.txt  \n",
      "  inflating: wiki_data/texts916.txt  \n",
      "  inflating: wiki_data/texts16.txt   \n",
      "  inflating: wiki_data/texts380.txt  \n",
      "  inflating: wiki_data/texts19.txt   \n",
      "  inflating: wiki_data/texts240.txt  \n",
      "  inflating: wiki_data/texts203.txt  \n",
      "  inflating: wiki_data/texts302.txt  \n",
      "  inflating: wiki_data/texts542.txt  \n",
      "  inflating: wiki_data/texts587.txt  \n",
      "  inflating: wiki_data/texts132.txt  \n",
      "  inflating: wiki_data/texts961.txt  \n",
      "  inflating: wiki_data/texts552.txt  \n",
      "  inflating: wiki_data/texts324.txt  \n",
      "  inflating: wiki_data/texts417.txt  \n",
      "  inflating: wiki_data/texts621.txt  \n",
      "  inflating: wiki_data/texts419.txt  \n",
      "  inflating: wiki_data/texts631.txt  \n",
      "  inflating: wiki_data/texts630.txt  \n",
      "  inflating: wiki_data/texts857.txt  \n",
      "  inflating: wiki_data/texts620.txt  \n",
      "  inflating: wiki_data/texts783.txt  \n",
      "  inflating: wiki_data/texts924.txt  \n",
      "  inflating: wiki_data/texts32.txt   \n",
      "  inflating: wiki_data/texts637.txt  \n",
      "  inflating: wiki_data/texts228.txt  \n",
      "  inflating: wiki_data/texts479.txt  \n",
      "  inflating: wiki_data/texts63.txt   \n",
      "  inflating: wiki_data/texts716.txt  \n",
      "  inflating: wiki_data/texts516.txt  \n",
      "  inflating: wiki_data/texts1032.txt  \n",
      "  inflating: wiki_data/texts73.txt   \n",
      "  inflating: wiki_data/texts484.txt  \n",
      "  inflating: wiki_data/texts731.txt  \n",
      "  inflating: wiki_data/texts198.txt  \n",
      "  inflating: wiki_data/texts821.txt  \n",
      "  inflating: wiki_data/texts440.txt  \n",
      "  inflating: wiki_data/texts112.txt  \n",
      "  inflating: wiki_data/texts534.txt  \n",
      "  inflating: wiki_data/texts968.txt  \n",
      "  inflating: wiki_data/texts303.txt  \n",
      "  inflating: wiki_data/texts184.txt  \n",
      "  inflating: wiki_data/texts320.txt  \n",
      "  inflating: wiki_data/texts264.txt  \n",
      "  inflating: wiki_data/texts374.txt  \n",
      "  inflating: wiki_data/texts718.txt  \n",
      "  inflating: wiki_data/texts733.txt  \n",
      "  inflating: wiki_data/texts1038.txt  \n",
      "  inflating: wiki_data/texts810.txt  \n",
      "  inflating: wiki_data/texts425.txt  \n",
      "  inflating: wiki_data/texts318.txt  \n",
      "  inflating: wiki_data/texts823.txt  \n",
      "  inflating: wiki_data/texts610.txt  \n",
      "  inflating: wiki_data/texts1035.txt  \n",
      "  inflating: wiki_data/texts393.txt  \n",
      "  inflating: wiki_data/texts157.txt  \n",
      "  inflating: wiki_data/texts781.txt  \n",
      "  inflating: wiki_data/texts947.txt  \n",
      "  inflating: wiki_data/texts667.txt  \n",
      "  inflating: wiki_data/texts252.txt  \n",
      "  inflating: wiki_data/texts160.txt  \n",
      "  inflating: wiki_data/texts987.txt  \n",
      "  inflating: wiki_data/texts332.txt  \n",
      "  inflating: wiki_data/texts812.txt  \n",
      "  inflating: wiki_data/texts119.txt  \n",
      "  inflating: wiki_data/texts853.txt  \n",
      "  inflating: wiki_data/texts1043.txt  \n",
      "  inflating: wiki_data/texts85.txt   \n",
      "  inflating: wiki_data/texts190.txt  \n",
      "  inflating: wiki_data/texts18.txt   \n",
      "  inflating: wiki_data/texts633.txt  \n",
      "  inflating: wiki_data/texts473.txt  \n",
      "  inflating: wiki_data/texts554.txt  \n",
      "  inflating: wiki_data/texts341.txt  \n",
      "  inflating: wiki_data/texts384.txt  \n",
      "  inflating: wiki_data/texts158.txt  \n",
      "  inflating: wiki_data/texts639.txt  \n",
      "  inflating: wiki_data/texts259.txt  \n",
      "  inflating: wiki_data/texts481.txt  \n",
      "  inflating: wiki_data/texts532.txt  \n",
      "  inflating: wiki_data/texts181.txt  \n",
      "  inflating: wiki_data/texts292.txt  \n",
      "  inflating: wiki_data/texts825.txt  \n",
      "  inflating: wiki_data/texts665.txt  \n",
      "  inflating: wiki_data/texts69.txt   \n",
      "  inflating: wiki_data/texts605.txt  \n",
      "  inflating: wiki_data/texts475.txt  \n",
      "  inflating: wiki_data/texts822.txt  \n",
      "  inflating: wiki_data/texts436.txt  \n",
      "  inflating: wiki_data/texts625.txt  \n",
      "  inflating: wiki_data/texts764.txt  \n",
      "  inflating: wiki_data/texts701.txt  \n",
      "  inflating: wiki_data/texts828.txt  \n",
      "  inflating: wiki_data/texts364.txt  \n",
      "  inflating: wiki_data/texts1046.txt  \n",
      "  inflating: wiki_data/texts0.txt    \n",
      "  inflating: wiki_data/texts861.txt  \n",
      "  inflating: wiki_data/texts759.txt  \n",
      "  inflating: wiki_data/texts506.txt  \n",
      "  inflating: wiki_data/texts650.txt  \n",
      "  inflating: wiki_data/texts226.txt  \n",
      "  inflating: wiki_data/texts25.txt   \n",
      "  inflating: wiki_data/texts260.txt  \n",
      "  inflating: wiki_data/texts309.txt  \n",
      "  inflating: wiki_data/texts414.txt  \n",
      "  inflating: wiki_data/texts493.txt  \n",
      "  inflating: wiki_data/texts751.txt  \n",
      "  inflating: wiki_data/texts513.txt  \n",
      "  inflating: wiki_data/texts1002.txt  \n",
      "  inflating: wiki_data/texts376.txt  \n",
      "  inflating: wiki_data/texts140.txt  \n",
      "  inflating: wiki_data/texts470.txt  \n",
      "  inflating: wiki_data/texts911.txt  \n",
      "  inflating: wiki_data/texts326.txt  \n",
      "  inflating: wiki_data/texts673.txt  \n",
      "  inflating: wiki_data/texts483.txt  \n",
      "  inflating: wiki_data/texts679.txt  \n",
      "  inflating: wiki_data/texts216.txt  \n",
      "  inflating: wiki_data/texts337.txt  \n",
      "  inflating: wiki_data/texts262.txt  \n",
      "  inflating: wiki_data/texts903.txt  \n",
      "  inflating: wiki_data/texts251.txt  \n",
      "  inflating: wiki_data/texts847.txt  \n",
      "  inflating: wiki_data/texts965.txt  \n",
      "  inflating: wiki_data/texts786.txt  \n",
      "  inflating: wiki_data/texts747.txt  \n",
      "  inflating: wiki_data/texts794.txt  \n",
      "  inflating: wiki_data/texts978.txt  \n",
      "  inflating: wiki_data/texts225.txt  \n",
      "  inflating: wiki_data/texts233.txt  \n",
      "  inflating: wiki_data/texts13.txt   \n",
      "  inflating: wiki_data/texts489.txt  \n",
      "  inflating: wiki_data/texts196.txt  \n",
      "  inflating: wiki_data/texts910.txt  \n",
      "  inflating: wiki_data/texts570.txt  \n",
      "  inflating: wiki_data/texts202.txt  \n",
      "  inflating: wiki_data/texts831.txt  \n",
      "  inflating: wiki_data/texts925.txt  \n",
      "  inflating: wiki_data/texts98.txt   \n",
      "  inflating: wiki_data/texts993.txt  \n",
      "  inflating: wiki_data/texts854.txt  \n",
      "  inflating: wiki_data/texts234.txt  \n",
      "  inflating: wiki_data/texts265.txt  \n",
      "  inflating: wiki_data/texts597.txt  \n",
      "  inflating: wiki_data/texts559.txt  \n",
      "  inflating: wiki_data/texts635.txt  \n",
      "  inflating: wiki_data/texts80.txt   \n",
      "  inflating: wiki_data/texts892.txt  \n",
      "  inflating: wiki_data/texts371.txt  \n",
      "  inflating: wiki_data/texts447.txt  \n",
      "  inflating: wiki_data/texts8.txt    \n",
      "  inflating: wiki_data/texts922.txt  \n",
      "  inflating: wiki_data/texts572.txt  \n",
      "  inflating: wiki_data/texts995.txt  \n",
      "  inflating: wiki_data/texts809.txt  \n",
      "  inflating: wiki_data/texts441.txt  \n",
      "  inflating: wiki_data/texts40.txt   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  inflating: wiki_data/texts36.txt   \n",
      "  inflating: wiki_data/texts896.txt  \n",
      "  inflating: wiki_data/texts607.txt  \n",
      "  inflating: wiki_data/texts1021.txt  \n",
      "  inflating: wiki_data/texts289.txt  \n",
      "  inflating: wiki_data/texts345.txt  \n",
      "  inflating: wiki_data/texts290.txt  \n",
      "  inflating: wiki_data/texts258.txt  \n",
      "  inflating: wiki_data/texts933.txt  \n",
      "  inflating: wiki_data/texts808.txt  \n",
      "  inflating: wiki_data/texts603.txt  \n",
      "  inflating: wiki_data/texts816.txt  \n",
      "  inflating: wiki_data/texts347.txt  \n",
      "  inflating: wiki_data/texts1033.txt  \n",
      "  inflating: wiki_data/texts312.txt  \n",
      "  inflating: wiki_data/texts601.txt  \n",
      "  inflating: wiki_data/texts578.txt  \n",
      "  inflating: wiki_data/texts191.txt  \n",
      "  inflating: wiki_data/texts702.txt  \n",
      "  inflating: wiki_data/texts742.txt  \n",
      "  inflating: wiki_data/texts543.txt  \n",
      "  inflating: wiki_data/texts908.txt  \n",
      "  inflating: wiki_data/texts801.txt  \n",
      "  inflating: wiki_data/texts426.txt  \n",
      "  inflating: wiki_data/texts159.txt  \n",
      "  inflating: wiki_data/texts476.txt  \n",
      "  inflating: wiki_data/texts333.txt  \n",
      "  inflating: wiki_data/texts2.txt    \n",
      "  inflating: wiki_data/texts271.txt  \n",
      "  inflating: wiki_data/texts104.txt  \n",
      "  inflating: wiki_data/texts648.txt  \n",
      "  inflating: wiki_data/texts237.txt  \n",
      "  inflating: wiki_data/texts643.txt  \n",
      "  inflating: wiki_data/texts266.txt  \n",
      "  inflating: wiki_data/texts275.txt  \n",
      "  inflating: wiki_data/texts776.txt  \n",
      "  inflating: wiki_data/texts713.txt  \n",
      "  inflating: wiki_data/texts269.txt  \n",
      "  inflating: wiki_data/texts329.txt  \n",
      "  inflating: wiki_data/texts168.txt  \n",
      "  inflating: wiki_data/texts855.txt  \n",
      "  inflating: wiki_data/texts591.txt  \n",
      "  inflating: wiki_data/texts931.txt  \n",
      "  inflating: wiki_data/texts984.txt  \n",
      "  inflating: wiki_data/texts114.txt  \n",
      "  inflating: wiki_data/texts1016.txt  \n",
      "  inflating: wiki_data/texts946.txt  \n",
      "  inflating: wiki_data/texts800.txt  \n",
      "  inflating: wiki_data/texts444.txt  \n",
      "  inflating: wiki_data/texts806.txt  \n",
      "  inflating: wiki_data/texts201.txt  \n",
      "  inflating: wiki_data/texts338.txt  \n",
      "  inflating: wiki_data/texts826.txt  \n",
      "  inflating: wiki_data/texts980.txt  \n",
      "  inflating: wiki_data/texts521.txt  \n",
      "  inflating: wiki_data/texts726.txt  \n",
      "  inflating: wiki_data/texts581.txt  \n",
      "  inflating: wiki_data/texts94.txt   \n",
      "  inflating: wiki_data/texts482.txt  \n",
      "  inflating: wiki_data/texts996.txt  \n",
      "  inflating: wiki_data/texts9.txt    \n",
      "  inflating: wiki_data/texts37.txt   \n",
      "  inflating: wiki_data/texts84.txt   \n",
      "  inflating: wiki_data/texts402.txt  \n",
      "  inflating: wiki_data/texts804.txt  \n",
      "  inflating: wiki_data/texts316.txt  \n",
      "  inflating: wiki_data/texts950.txt  \n",
      "  inflating: wiki_data/texts510.txt  \n",
      "  inflating: wiki_data/texts421.txt  \n",
      "  inflating: wiki_data/texts109.txt  \n",
      "  inflating: wiki_data/texts811.txt  \n",
      "  inflating: wiki_data/texts507.txt  \n",
      "  inflating: wiki_data/texts691.txt  \n",
      "  inflating: wiki_data/texts101.txt  \n",
      "  inflating: wiki_data/texts408.txt  \n",
      "  inflating: wiki_data/texts979.txt  \n",
      "  inflating: wiki_data/texts210.txt  \n",
      "  inflating: wiki_data/texts491.txt  \n",
      "  inflating: wiki_data/texts616.txt  \n",
      "  inflating: wiki_data/texts754.txt  \n",
      "  inflating: wiki_data/texts772.txt  \n",
      "  inflating: wiki_data/texts509.txt  \n",
      "  inflating: wiki_data/texts611.txt  \n",
      "  inflating: wiki_data/texts864.txt  \n",
      "  inflating: wiki_data/texts508.txt  \n",
      "  inflating: wiki_data/texts533.txt  \n",
      "  inflating: wiki_data/texts405.txt  \n",
      "  inflating: wiki_data/texts876.txt  \n",
      "  inflating: wiki_data/texts656.txt  \n",
      "  inflating: wiki_data/texts680.txt  \n",
      "  inflating: wiki_data/texts79.txt   \n",
      "  inflating: wiki_data/texts760.txt  \n",
      "  inflating: wiki_data/texts671.txt  \n",
      "  inflating: wiki_data/texts353.txt  \n",
      "  inflating: wiki_data/texts1041.txt  \n",
      "  inflating: wiki_data/texts692.txt  \n",
      "  inflating: wiki_data/texts15.txt   \n",
      "  inflating: wiki_data/texts580.txt  \n",
      "  inflating: wiki_data/texts357.txt  \n",
      "  inflating: wiki_data/texts28.txt   \n",
      "  inflating: wiki_data/texts186.txt  \n",
      "  inflating: wiki_data/texts428.txt  \n",
      "  inflating: wiki_data/texts67.txt   \n",
      "  inflating: wiki_data/texts524.txt  \n",
      "  inflating: wiki_data/texts141.txt  \n",
      "  inflating: wiki_data/texts971.txt  \n",
      "  inflating: wiki_data/texts913.txt  \n",
      "  inflating: wiki_data/texts899.txt  \n",
      "  inflating: wiki_data/texts116.txt  \n",
      "  inflating: wiki_data/texts244.txt  \n",
      "  inflating: wiki_data/texts308.txt  \n",
      "  inflating: wiki_data/texts1009.txt  \n",
      "  inflating: wiki_data/texts294.txt  \n",
      "  inflating: wiki_data/texts803.txt  \n",
      "  inflating: wiki_data/texts690.txt  \n",
      "  inflating: wiki_data/texts446.txt  \n",
      "  inflating: wiki_data/texts14.txt   \n",
      "  inflating: wiki_data/texts468.txt  \n",
      "  inflating: wiki_data/texts137.txt  \n",
      "  inflating: wiki_data/texts515.txt  \n",
      "  inflating: wiki_data/texts152.txt  \n",
      "  inflating: wiki_data/texts982.txt  \n",
      "  inflating: wiki_data/texts498.txt  \n",
      "  inflating: wiki_data/texts133.txt  \n",
      "  inflating: wiki_data/texts352.txt  \n",
      "  inflating: wiki_data/texts330.txt  \n",
      "  inflating: wiki_data/texts469.txt  \n",
      "  inflating: wiki_data/texts450.txt  \n",
      "  inflating: wiki_data/texts771.txt  \n",
      "  inflating: wiki_data/texts670.txt  \n",
      "  inflating: wiki_data/texts879.txt  \n",
      "  inflating: wiki_data/texts887.txt  \n",
      "  inflating: wiki_data/texts535.txt  \n",
      "  inflating: wiki_data/texts298.txt  \n",
      "  inflating: wiki_data/texts983.txt  \n",
      "  inflating: wiki_data/texts915.txt  \n",
      "  inflating: wiki_data/texts377.txt  \n",
      "  inflating: wiki_data/texts1037.txt  \n",
      "  inflating: wiki_data/texts219.txt  \n",
      "  inflating: wiki_data/texts723.txt  \n",
      "  inflating: wiki_data/texts200.txt  \n",
      "  inflating: wiki_data/texts599.txt  \n",
      "  inflating: wiki_data/texts11.txt   \n",
      "  inflating: wiki_data/texts239.txt  \n",
      "  inflating: wiki_data/texts388.txt  \n",
      "  inflating: wiki_data/texts765.txt  \n",
      "  inflating: wiki_data/texts50.txt   \n",
      "  inflating: wiki_data/texts220.txt  \n",
      "  inflating: wiki_data/texts193.txt  \n",
      "  inflating: wiki_data/texts652.txt  \n",
      "  inflating: wiki_data/texts619.txt  \n",
      "  inflating: wiki_data/texts684.txt  \n",
      "  inflating: wiki_data/texts125.txt  \n",
      "  inflating: wiki_data/texts270.txt  \n",
      "  inflating: wiki_data/texts511.txt  \n",
      "  inflating: wiki_data/texts131.txt  \n",
      "  inflating: wiki_data/texts372.txt  \n",
      "  inflating: wiki_data/texts208.txt  \n",
      "  inflating: wiki_data/texts383.txt  \n",
      "  inflating: wiki_data/texts177.txt  \n",
      "  inflating: wiki_data/texts472.txt  \n",
      "  inflating: wiki_data/texts976.txt  \n",
      "  inflating: wiki_data/texts926.txt  \n",
      "  inflating: wiki_data/texts1025.txt  \n",
      "  inflating: wiki_data/texts387.txt  \n",
      "  inflating: wiki_data/texts952.txt  \n",
      "  inflating: wiki_data/texts435.txt  \n",
      "  inflating: wiki_data/texts788.txt  \n",
      "  inflating: wiki_data/texts585.txt  \n",
      "  inflating: wiki_data/texts77.txt   \n",
      "  inflating: wiki_data/texts161.txt  \n",
      "  inflating: wiki_data/texts777.txt  \n",
      "  inflating: wiki_data/texts6.txt    \n",
      "  inflating: wiki_data/texts541.txt  \n",
      "  inflating: wiki_data/texts674.txt  \n",
      "  inflating: wiki_data/texts962.txt  \n",
      "  inflating: wiki_data/texts878.txt  \n",
      "  inflating: wiki_data/texts44.txt   \n",
      "  inflating: wiki_data/texts454.txt  \n",
      "  inflating: wiki_data/texts229.txt  \n",
      "  inflating: wiki_data/texts638.txt  \n",
      "  inflating: wiki_data/texts681.txt  \n",
      "  inflating: wiki_data/texts709.txt  \n",
      "  inflating: wiki_data/texts567.txt  \n",
      "  inflating: wiki_data/texts717.txt  \n",
      "  inflating: wiki_data/texts644.txt  \n",
      "  inflating: wiki_data/texts799.txt  \n",
      "  inflating: wiki_data/texts873.txt  \n",
      "  inflating: wiki_data/texts1028.txt  \n",
      "  inflating: wiki_data/texts697.txt  \n",
      "  inflating: wiki_data/texts465.txt  \n",
      "  inflating: wiki_data/texts880.txt  \n",
      "  inflating: wiki_data/texts497.txt  \n",
      "  inflating: wiki_data/texts278.txt  \n",
      "  inflating: wiki_data/texts608.txt  \n",
      "  inflating: wiki_data/texts615.txt  \n",
      "  inflating: wiki_data/texts197.txt  \n",
      "  inflating: wiki_data/texts921.txt  \n",
      "  inflating: wiki_data/texts246.txt  \n",
      "  inflating: wiki_data/texts155.txt  \n",
      "  inflating: wiki_data/texts795.txt  \n",
      "  inflating: wiki_data/texts1040.txt  \n",
      "  inflating: wiki_data/texts744.txt  \n",
      "  inflating: wiki_data/texts623.txt  \n",
      "  inflating: wiki_data/texts551.txt  \n",
      "  inflating: wiki_data/texts964.txt  \n",
      "  inflating: wiki_data/texts369.txt  \n",
      "  inflating: wiki_data/texts344.txt  \n",
      "  inflating: wiki_data/texts398.txt  \n",
      "  inflating: wiki_data/texts990.txt  \n",
      "  inflating: wiki_data/texts397.txt  \n",
      "  inflating: wiki_data/texts894.txt  \n",
      "  inflating: wiki_data/texts90.txt   \n",
      "  inflating: wiki_data/texts602.txt  \n",
      "  inflating: wiki_data/texts139.txt  \n",
      "  inflating: wiki_data/texts59.txt   \n",
      "  inflating: wiki_data/texts767.txt  \n",
      "  inflating: wiki_data/texts704.txt  \n",
      "  inflating: wiki_data/texts773.txt  \n",
      "  inflating: wiki_data/texts793.txt  \n",
      "  inflating: wiki_data/texts127.txt  \n",
      "  inflating: wiki_data/texts401.txt  \n",
      "  inflating: wiki_data/texts520.txt  \n",
      "  inflating: wiki_data/texts395.txt  \n",
      "  inflating: wiki_data/texts41.txt   \n",
      "  inflating: wiki_data/texts230.txt  \n",
      "  inflating: wiki_data/texts951.txt  \n",
      "  inflating: wiki_data/texts705.txt  \n",
      "  inflating: wiki_data/texts430.txt  \n",
      "  inflating: wiki_data/texts729.txt  \n",
      "  inflating: wiki_data/texts43.txt   \n",
      "  inflating: wiki_data/texts362.txt  \n",
      "  inflating: wiki_data/texts917.txt  \n",
      "  inflating: wiki_data/texts1027.txt  \n",
      "  inflating: wiki_data/texts999.txt  \n",
      "  inflating: wiki_data/texts732.txt  \n",
      "  inflating: wiki_data/texts571.txt  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  inflating: wiki_data/texts514.txt  \n",
      "  inflating: wiki_data/texts346.txt  \n",
      "  inflating: wiki_data/texts459.txt  \n",
      "  inflating: wiki_data/texts573.txt  \n",
      "  inflating: wiki_data/texts584.txt  \n",
      "  inflating: wiki_data/texts448.txt  \n",
      "  inflating: wiki_data/texts375.txt  \n",
      "  inflating: wiki_data/texts462.txt  \n",
      "  inflating: wiki_data/texts221.txt  \n",
      "  inflating: wiki_data/texts257.txt  \n",
      "  inflating: wiki_data/texts927.txt  \n",
      "  inflating: wiki_data/texts5.txt    \n",
      "  inflating: wiki_data/texts214.txt  \n",
      "  inflating: wiki_data/texts889.txt  \n",
      "  inflating: wiki_data/texts928.txt  \n",
      "  inflating: wiki_data/texts632.txt  \n",
      "  inflating: wiki_data/texts895.txt  \n",
      "  inflating: wiki_data/texts47.txt   \n",
      "  inflating: wiki_data/texts763.txt  \n",
      "  inflating: wiki_data/texts153.txt  \n",
      "  inflating: wiki_data/texts553.txt  \n",
      "  inflating: wiki_data/texts179.txt  \n",
      "  inflating: wiki_data/texts206.txt  \n",
      "  inflating: wiki_data/texts305.txt  \n",
      "  inflating: wiki_data/texts422.txt  \n",
      "  inflating: wiki_data/texts582.txt  \n",
      "  inflating: wiki_data/texts490.txt  \n",
      "  inflating: wiki_data/texts672.txt  \n",
      "  inflating: wiki_data/texts642.txt  \n",
      "  inflating: wiki_data/texts477.txt  \n",
      "  inflating: wiki_data/texts838.txt  \n",
      "  inflating: wiki_data/texts188.txt  \n",
      "  inflating: wiki_data/texts504.txt  \n",
      "  inflating: wiki_data/texts761.txt  \n",
      "  inflating: wiki_data/texts719.txt  \n",
      "  inflating: wiki_data/texts410.txt  \n",
      "  inflating: wiki_data/texts399.txt  \n",
      "  inflating: wiki_data/texts111.txt  \n",
      "  inflating: wiki_data/texts518.txt  \n",
      "  inflating: wiki_data/texts145.txt  \n",
      "  inflating: wiki_data/texts872.txt  \n",
      "  inflating: wiki_data/edges.txt     \n",
      "  inflating: wiki_data/texts95.txt   \n",
      "  inflating: wiki_data/texts850.txt  \n",
      "  inflating: wiki_data/texts474.txt  \n",
      "  inflating: wiki_data/texts204.txt  \n",
      "  inflating: wiki_data/texts23.txt   \n",
      "  inflating: wiki_data/texts339.txt  \n",
      "  inflating: wiki_data/texts758.txt  \n",
      "  inflating: wiki_data/texts403.txt  \n",
      "  inflating: wiki_data/texts1036.txt  \n",
      "  inflating: wiki_data/texts749.txt  \n",
      "  inflating: wiki_data/texts51.txt   \n",
      "  inflating: wiki_data/texts171.txt  \n",
      "  inflating: wiki_data/texts640.txt  \n",
      "  inflating: wiki_data/texts724.txt  \n",
      "  inflating: wiki_data/texts334.txt  \n",
      "  inflating: wiki_data/texts820.txt  \n",
      "  inflating: wiki_data/texts500.txt  \n",
      "  inflating: wiki_data/texts411.txt  \n",
      "  inflating: wiki_data/texts134.txt  \n",
      "  inflating: wiki_data/texts1030.txt  \n",
      "  inflating: wiki_data/texts443.txt  \n",
      "  inflating: wiki_data/texts536.txt  \n",
      "  inflating: wiki_data/texts306.txt  \n",
      "  inflating: wiki_data/texts538.txt  \n",
      "  inflating: wiki_data/texts138.txt  \n",
      "  inflating: wiki_data/texts381.txt  \n",
      "  inflating: wiki_data/texts909.txt  \n",
      "  inflating: wiki_data/texts431.txt  \n",
      "  inflating: wiki_data/texts575.txt  \n",
      "  inflating: wiki_data/texts588.txt  \n",
      "  inflating: wiki_data/texts238.txt  \n",
      "  inflating: wiki_data/texts748.txt  \n",
      "  inflating: wiki_data/texts752.txt  \n",
      "  inflating: wiki_data/texts55.txt   \n",
      "  inflating: wiki_data/texts694.txt  \n",
      "  inflating: wiki_data/texts669.txt  \n",
      "  inflating: wiki_data/texts396.txt  \n",
      "  inflating: wiki_data/texts658.txt  \n",
      "  inflating: wiki_data/texts695.txt  \n",
      "  inflating: wiki_data/texts349.txt  \n",
      "  inflating: wiki_data/texts130.txt  \n",
      "  inflating: wiki_data/texts20.txt   \n",
      "  inflating: wiki_data/texts564.txt  \n",
      "  inflating: wiki_data/texts496.txt  \n",
      "  inflating: wiki_data/texts829.txt  \n",
      "  inflating: wiki_data/texts550.txt  \n",
      "  inflating: wiki_data/texts1011.txt  \n",
      "  inflating: wiki_data/texts955.txt  \n",
      "  inflating: wiki_data/texts1014.txt  \n",
      "  inflating: wiki_data/texts875.txt  \n",
      "  inflating: wiki_data/texts944.txt  \n",
      "  inflating: wiki_data/texts882.txt  \n",
      "  inflating: wiki_data/texts1001.txt  \n",
      "  inflating: wiki_data/texts735.txt  \n",
      "  inflating: wiki_data/texts409.txt  \n",
      "  inflating: wiki_data/texts885.txt  \n",
      "  inflating: wiki_data/nodes.txt     \n",
      "  inflating: wiki_data/texts629.txt  \n",
      "  inflating: wiki_data/texts285.txt  \n",
      "  inflating: wiki_data/texts108.txt  \n",
      "  inflating: wiki_data/texts576.txt  \n",
      "  inflating: wiki_data/texts998.txt  \n",
      "  inflating: wiki_data/texts299.txt  \n",
      "  inflating: wiki_data/texts315.txt  \n",
      "  inflating: wiki_data/texts813.txt  \n",
      "  inflating: wiki_data/texts600.txt  \n",
      "  inflating: wiki_data/texts863.txt  \n",
      "  inflating: wiki_data/texts253.txt  \n",
      "  inflating: wiki_data/texts722.txt  \n",
      "  inflating: wiki_data/texts956.txt  \n",
      "  inflating: wiki_data/texts689.txt  \n",
      "  inflating: wiki_data/texts734.txt  \n",
      "  inflating: wiki_data/texts321.txt  \n",
      "  inflating: wiki_data/texts1026.txt  \n",
      "  inflating: wiki_data/texts340.txt  \n",
      "  inflating: wiki_data/texts223.txt  \n",
      "  inflating: wiki_data/texts548.txt  \n",
      "  inflating: wiki_data/texts261.txt  \n",
      "  inflating: wiki_data/texts366.txt  \n",
      "  inflating: wiki_data/texts432.txt  \n",
      "  inflating: wiki_data/texts664.txt  \n",
      "  inflating: wiki_data/texts739.txt  \n",
      "  inflating: wiki_data/texts655.txt  \n",
      "  inflating: wiki_data/texts837.txt  \n",
      "  inflating: wiki_data/texts247.txt  \n",
      "  inflating: wiki_data/texts935.txt  \n",
      "  inflating: wiki_data/texts662.txt  \n",
      "  inflating: wiki_data/texts64.txt   \n",
      "  inflating: wiki_data/texts784.txt  \n",
      "  inflating: wiki_data/texts963.txt  \n",
      "  inflating: wiki_data/texts942.txt  \n",
      "  inflating: wiki_data/texts790.txt  \n",
      "  inflating: wiki_data/texts868.txt  \n",
      "  inflating: wiki_data/texts175.txt  \n",
      "  inflating: wiki_data/texts994.txt  \n",
      "  inflating: wiki_data/texts1012.txt  \n",
      "  inflating: wiki_data/texts209.txt  \n",
      "  inflating: wiki_data/texts281.txt  \n",
      "  inflating: wiki_data/texts370.txt  \n",
      "  inflating: wiki_data/texts647.txt  \n",
      "  inflating: wiki_data/texts778.txt  \n",
      "  inflating: wiki_data/texts300.txt  \n",
      "  inflating: wiki_data/texts768.txt  \n",
      "  inflating: wiki_data/texts412.txt  \n",
      "  inflating: wiki_data/texts592.txt  \n",
      "  inflating: wiki_data/texts33.txt   \n",
      "  inflating: wiki_data/texts700.txt  \n",
      "  inflating: wiki_data/texts323.txt  \n",
      "  inflating: wiki_data/texts874.txt  \n",
      "  inflating: wiki_data/texts675.txt  \n",
      "  inflating: wiki_data/texts730.txt  \n",
      "  inflating: wiki_data/texts355.txt  \n",
      "  inflating: wiki_data/texts682.txt  \n",
      "  inflating: wiki_data/texts350.txt  \n",
      "  inflating: wiki_data/texts297.txt  \n",
      "  inflating: wiki_data/texts703.txt  \n",
      "  inflating: wiki_data/texts787.txt  \n",
      "  inflating: wiki_data/texts456.txt  \n",
      "  inflating: wiki_data/texts1042.txt  \n",
      "  inflating: wiki_data/texts843.txt  \n",
      "  inflating: wiki_data/texts106.txt  \n",
      "  inflating: wiki_data/texts815.txt  \n",
      "  inflating: wiki_data/texts256.txt  \n",
      "  inflating: wiki_data/texts708.txt  \n",
      "  inflating: wiki_data/texts537.txt  \n",
      "  inflating: wiki_data/texts841.txt  \n",
      "  inflating: wiki_data/texts782.txt  \n",
      "  inflating: wiki_data/texts72.txt   \n",
      "  inflating: wiki_data/texts710.txt  \n",
      "  inflating: wiki_data/texts105.txt  \n",
      "  inflating: wiki_data/texts539.txt  \n",
      "  inflating: wiki_data/texts954.txt  \n",
      "  inflating: wiki_data/texts385.txt  \n",
      "  inflating: wiki_data/texts886.txt  \n",
      "  inflating: wiki_data/texts529.txt  \n",
      "  inflating: wiki_data/texts254.txt  \n",
      "  inflating: wiki_data/texts918.txt  \n",
      "  inflating: wiki_data/texts149.txt  \n",
      "  inflating: wiki_data/texts165.txt  \n",
      "  inflating: wiki_data/texts970.txt  \n",
      "  inflating: wiki_data/texts617.txt  \n",
      "  inflating: wiki_data/texts745.txt  \n",
      "  inflating: wiki_data/texts549.txt  \n",
      "  inflating: wiki_data/texts606.txt  \n",
      "  inflating: wiki_data/texts445.txt  \n",
      "  inflating: wiki_data/texts452.txt  \n",
      "  inflating: wiki_data/texts651.txt  \n",
      "  inflating: wiki_data/texts250.txt  \n",
      "  inflating: wiki_data/texts836.txt  \n",
      "  inflating: wiki_data/texts107.txt  \n",
      "  inflating: wiki_data/texts660.txt  \n",
      "  inflating: wiki_data/texts881.txt  \n",
      "  inflating: wiki_data/texts898.txt  \n",
      "  inflating: wiki_data/texts268.txt  \n",
      "  inflating: wiki_data/texts530.txt  \n",
      "  inflating: wiki_data/texts96.txt   \n",
      "  inflating: wiki_data/texts663.txt  \n",
      "  inflating: wiki_data/texts389.txt  \n",
      "  inflating: wiki_data/texts687.txt  \n",
      "  inflating: wiki_data/texts143.txt  \n",
      "  inflating: wiki_data/texts213.txt  \n",
      "  inflating: wiki_data/texts612.txt  \n",
      "  inflating: wiki_data/texts400.txt  \n",
      "  inflating: wiki_data/texts199.txt  \n",
      "  inflating: wiki_data/texts566.txt  \n",
      "  inflating: wiki_data/texts683.txt  \n",
      "  inflating: wiki_data/texts966.txt  \n",
      "  inflating: wiki_data/texts727.txt  \n",
      "  inflating: wiki_data/texts367.txt  \n",
      "  inflating: wiki_data/texts413.txt  \n",
      "  inflating: wiki_data/texts224.txt  \n",
      "  inflating: wiki_data/texts319.txt  \n",
      "  inflating: wiki_data/texts839.txt  \n",
      "  inflating: wiki_data/texts870.txt  \n",
      "  inflating: wiki_data/texts295.txt  \n",
      "  inflating: wiki_data/texts463.txt  \n",
      "  inflating: wiki_data/texts802.txt  \n",
      "  inflating: wiki_data/texts830.txt  \n",
      "  inflating: wiki_data/texts263.txt  \n",
      "  inflating: wiki_data/texts618.txt  \n",
      "  inflating: wiki_data/texts595.txt  \n",
      "  inflating: wiki_data/texts310.txt  \n",
      "  inflating: wiki_data/texts118.txt  \n",
      "  inflating: wiki_data/texts75.txt   \n",
      "  inflating: wiki_data/texts685.txt  \n",
      "  inflating: wiki_data/texts686.txt  \n",
      "  inflating: wiki_data/texts547.txt  \n",
      "  inflating: wiki_data/texts494.txt  \n",
      "  inflating: wiki_data/texts851.txt  \n",
      "  inflating: wiki_data/texts526.txt  \n",
      "  inflating: wiki_data/texts693.txt  \n",
      "  inflating: wiki_data/texts102.txt  \n",
      "  inflating: wiki_data/texts236.txt  \n",
      "  inflating: wiki_data/texts960.txt  \n",
      "  inflating: wiki_data/texts766.txt  \n",
      "  inflating: wiki_data/texts544.txt  \n",
      "  inflating: wiki_data/texts34.txt   \n",
      "  inflating: wiki_data/texts845.txt  \n",
      "  inflating: wiki_data/texts522.txt  \n",
      "  inflating: wiki_data/texts325.txt  \n",
      "  inflating: wiki_data/texts307.txt  \n",
      "  inflating: wiki_data/texts110.txt  \n",
      "  inflating: wiki_data/texts163.txt  \n"
     ]
    }
   ],
   "source": [
    "! unzip wiki_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "46165965826b46f88f58c0250bfdda9c",
      "05f41805986645f4902bbe6799542d78",
      "6d5b01aa4383457d8edf67ea93ce5036",
      "48dd34655649462c9386ffd71662ad56",
      "f3beb585873646888beb8a8c930295f3",
      "335ad58438124feca452795d8db25e05",
      "a7302dd57f914c55801ff49bf2d0a55a",
      "35987165f9bb495693d0623513d16e76",
      "79b4d4b2dc6742fd8da1b4b0d5449dd8",
      "90ee46515b75466e9bd0b824bb9a6ae4",
      "19df947696994810b806dfde8f3969f4"
     ]
    },
    "id": "1q9iK9FIbiiy",
    "outputId": "62e1f4ec-269d-43a5-d091-647886a5c992"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46165965826b46f88f58c0250bfdda9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1047 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1047,\n",
       " '= Carmichael number =In number theory, a Carmichael number is a composite numbern{\\\\displaystyle n}which satisfies the modular arithmetic congruence relation:bn11(modn){\\\\displaystyle b^{n-1}\\\\equiv 1{\\\\pmod {n}}}for all integersb{\\\\displaystyle b}which are relatively prime ton{\\\\displaystyle n}.They are named for Robert Carmichael.The Carmichael numbers are the subset K1 of the Kndel numbers.Equivalently, a Carmichael number is a composite numbern{\\\\displaystyle n}for whichbnb(modn){\\\\displaystyle b^{n}\\\\equiv b{\\\\pmod {n}}}for all integersb{\\\\displaystyle b}.OverviewFermat\\'s little theorem states that if p is a prime number, then for any integer b, the number bp  b is an integer multiple of p.  Carmichael numbers are composite numbers which have this property.  Carmichael numbers are also called Fermat pseudoprimes or absolute Fermat pseudoprimes. A Carmichael number will pass a Fermat primality test to every base b relatively prime to the number, even though it is not actually prime.This makes tests based on Fermat\\'s Little Theorem less effective than strong probable prime tests such as the BailliePSW primality test and the MillerRabin primality test.However, no Carmichael number is either an EulerJacobi pseudoprime or a strong pseudoprime to every base relatively prime to itso, in theory, either an Euler or a strong probable prime test could prove that a Carmichael number is, in fact, composite.Arnaultgives a 397-digit Carmichael numberN{\\\\displaystyle N}that is a strong pseudoprime to all prime bases less than 307:N=p(313(p1)+1)(353(p1)+1){\\\\displaystyle N=p\\\\cdot (313(p-1)+1)\\\\cdot (353(p-1)+1)}wherep={\\\\displaystyle p=}29674495668685510550154174642905332730771991799853043350995075531276838753171770199594238596428121188033664754218345562493168782883is a 131-digit prime.p{\\\\displaystyle p}is the smallest prime factor ofN{\\\\displaystyle N}, so this Carmichael number is also a (not necessarily strong) pseudoprime to all bases less thanp{\\\\displaystyle p}.As numbers become larger, Carmichael numbers become increasingly rare. For example, there are 20,138,200 Carmichael numbers between 1 and 1021 (approximately one in 50 trillion (51013) numbers).Korselt\\'s criterionAn alternative and equivalent definition of Carmichael numbers is given by Korselt\\'s criterion.Theorem (A. Korselt 1899): A positive composite integern{\\\\displaystyle n}is a Carmichael number if and only ifn{\\\\displaystyle n}is square-free, and for all prime divisorsp{\\\\displaystyle p}ofn{\\\\displaystyle n}, it is true thatp1n1{\\\\displaystyle p-1\\\\mid n-1}.It follows from this theorem that all Carmichael numbers are odd, since any even composite number that is square-free (and hence has only one prime factor of two) will have at least one odd prime factor, and thusp1n1{\\\\displaystyle p-1\\\\mid n-1}results in an even dividing an odd, a contradiction. (The oddness of Carmichael numbers also follows from the fact that1{\\\\displaystyle -1}is a Fermat witness for any even composite number.)From the criterion it also follows that Carmichael numbers are cyclic. Additionally, it follows that there are no Carmichael numbers with exactly two prime divisors.DiscoveryKorselt was the first who observed the basic properties of Carmichael numbers, but he did not give any examples. In 1910, Carmichael found the first and smallest such number, 561, which explains the name \"Carmichael number\".That 561 is a Carmichael number can be seen with Korselt\\'s criterion. Indeed,561=31117{\\\\displaystyle 561=3\\\\cdot 11\\\\cdot 17}is square-free and2560{\\\\displaystyle 2\\\\mid 560},10560{\\\\displaystyle 10\\\\mid 560}and16560{\\\\displaystyle 16\\\\mid 560}.The next six Carmichael numbers are (sequence A002997 in the OEIS):1105=51317(41104;121104;161104){\\\\displaystyle 1105=5\\\\cdot 13\\\\cdot 17\\\\qquad (4\\\\mid 1104;\\\\quad 12\\\\mid 1104;\\\\quad 16\\\\mid 1104)}1729=71319(61728;121728;181728){\\\\displaystyle 1729=7\\\\cdot 13\\\\cdot 19\\\\qquad (6\\\\mid 1728;\\\\quad 12\\\\mid 1728;\\\\quad 18\\\\mid 1728)}2465=51729(42464;162464;282464){\\\\displaystyle 2465=5\\\\cdot 17\\\\cdot 29\\\\qquad (4\\\\mid 2464;\\\\quad 16\\\\mid 2464;\\\\quad 28\\\\mid 2464)}2821=71331(62820;122820;302820){\\\\displaystyle 2821=7\\\\cdot 13\\\\cdot 31\\\\qquad (6\\\\mid 2820;\\\\quad 12\\\\mid 2820;\\\\quad 30\\\\mid 2820)}6601=72341(66600;226600;406600){\\\\displaystyle 6601=7\\\\cdot 23\\\\cdot 41\\\\qquad (6\\\\mid 6600;\\\\quad 22\\\\mid 6600;\\\\quad 40\\\\mid 6600)}8911=71967(68910;188910;668910).{\\\\displaystyle 8911=7\\\\cdot 19\\\\cdot 67\\\\qquad (6\\\\mid 8910;\\\\quad 18\\\\mid 8910;\\\\quad 66\\\\mid 8910).}These first seven Carmichael numbers, from 561 to 8911, were all found by the Czech mathematician Vclav imerka in 1885 (thus preceding not just Carmichael but also Korselt, although imerka did not find anything like Korselt\\'s criterion). His work, however, remained unnoticed.J. Chernick proved a theorem in 1939 which can be used to construct a subset of Carmichael numbers. The number(6k+1)(12k+1)(18k+1){\\\\displaystyle (6k+1)(12k+1)(18k+1)}is a Carmichael number if its three factors are all prime. Whether this formula produces an infinite quantity of Carmichael numbers is an open question (though it is implied by Dickson\\'s conjecture).Paul Erds heuristically argued there should be infinitely many Carmichael numbers. In 1994 W. R. (Red) Alford, Andrew Granville and Carl Pomerance  used a bound on Olson\\'s constant to show that there really do exist infinitely many Carmichael numbers. Specifically, they showed that for sufficiently largen{\\\\displaystyle n}, there are at leastn2/7{\\\\displaystyle n^{2/7}}Carmichael numbers between 1 andn.{\\\\displaystyle n.}Thomas Wright proved that ifa{\\\\displaystyle a}andm{\\\\displaystyle m}are relatively prime,then there are infinitely many Carmichael numbers in the arithmetic progressiona+km{\\\\displaystyle a+k\\\\cdot m},wherek=1,2,{\\\\displaystyle k=1,2,\\\\ldots }.Lh and Niebuhr in 1992 found some very large Carmichael numbers, including one with 1,101,518 factors and over 16 million digits.This has been improved to 10,333,229,505 prime factors and 295,486,761,787 digits, so the largest known Carmichael number is much greater than the largest known prime.PropertiesFactorizationsCarmichael numbers have at least three positive prime factors. The first Carmichael numbers withk=3,4,5,{\\\\displaystyle k=3,4,5,\\\\ldots }prime factors are (sequence A006931 in the OEIS):The first Carmichael numbers with 4 prime factors are (sequence A074379 in the OEIS):The second Carmichael number (1105) can be expressed as the sum of two squares in more ways than any smaller number. The third Carmichael number (1729) is the Hardy-Ramanujan Number: the smallest number that can be expressed as the sum of two cubes (of positive numbers) in two different ways.DistributionLetC(X){\\\\displaystyle C(X)}denote the number of Carmichael numbers less than or equal toX{\\\\displaystyle X}. The distribution of Carmichael numbers by powers of 10 (sequence A055553 in the OEIS):In 1953, Kndel proved the upper bound:C(X)<Xexp\\u2061(k1(log\\u2061Xlog\\u2061log\\u2061X)12){\\\\displaystyle C(X)<X\\\\exp \\\\left({-k_{1}\\\\left(\\\\log X\\\\log \\\\log X\\\\right)^{\\\\frac {1}{2}}}\\\\right)}for some constantk1{\\\\displaystyle k_{1}}.In 1956, Erds improved the bound toC(X)<Xexp\\u2061(k2log\\u2061Xlog\\u2061log\\u2061log\\u2061Xlog\\u2061log\\u2061X){\\\\displaystyle C(X)<X\\\\exp \\\\left({\\\\frac {-k_{2}\\\\log X\\\\log \\\\log \\\\log X}{\\\\log \\\\log X}}\\\\right)}for some constantk2{\\\\displaystyle k_{2}}. He further gave a heuristic argument suggesting that this upper bound should be close to the true growth rate ofC(X){\\\\displaystyle C(X)}.In the other direction, Alford, Granville and Pomerance proved in 1994 that for sufficiently large X,C(X)>X27.{\\\\displaystyle C(X)>X^{\\\\frac {2}{7}}.}In 2005, this bound was further improved by Harman toC(X)>X0.332{\\\\displaystyle C(X)>X^{0.332}}who subsequently improved the exponent to0.70390.4736=0.33336704>1/3{\\\\displaystyle 0.7039\\\\cdot 0.4736=0.33336704>1/3}.Regarding the asymptotic distribution of Carmichael numbers, there have been several conjectures. In 1956, Erds conjectured that there wereX1o(1){\\\\displaystyle X^{1-o(1)}}Carmichael numbers for X sufficiently large. In 1981, Pomerance sharpened Erds\\' heuristic arguments to conjecture that there are at leastXL(X)1+o(1){\\\\displaystyle X\\\\cdot L(X)^{-1+o(1)}}Carmichael numbers up toX{\\\\displaystyle X}, whereL(x)=exp\\u2061(log\\u2061xlog\\u2061log\\u2061log\\u2061xlog\\u2061log\\u2061x){\\\\displaystyle L(x)=\\\\exp {\\\\left({\\\\frac {\\\\log x\\\\log \\\\log \\\\log x}{\\\\log \\\\log x}}\\\\right)}}.However, inside current computational ranges (such as the counts of Carmichael numbers performed by Pinch up to 1021), these conjectures are not yet borne out by the data.GeneralizationsThe notion of Carmichael number generalizes to a Carmichael ideal in any number field K. For any nonzero prime idealp{\\\\displaystyle {\\\\mathfrak {p}}}inOK{\\\\displaystyle {\\\\mathcal {O}}_{K}}, we haveN(p)modp{\\\\displaystyle \\\\alpha ^{{\\\\rm {N}}({\\\\mathfrak {p}})}\\\\equiv \\\\alpha {\\\\bmod {\\\\mathfrak {p}}}}for all{\\\\displaystyle \\\\alpha }inOK{\\\\displaystyle {\\\\mathcal {O}}_{K}}, whereN(p){\\\\displaystyle {\\\\rm {N}}({\\\\mathfrak {p}})}is the norm of the idealp{\\\\displaystyle {\\\\mathfrak {p}}}. (This generalizes Fermat\\'s little theorem, thatmpmmodp{\\\\displaystyle m^{p}\\\\equiv m{\\\\bmod {p}}}for all integers m when p is prime.) Call a nonzero ideala{\\\\displaystyle {\\\\mathfrak {a}}}inOK{\\\\displaystyle {\\\\mathcal {O}}_{K}}Carmichael if it is not a prime ideal andN(a)moda{\\\\displaystyle \\\\alpha ^{{\\\\rm {N}}({\\\\mathfrak {a}})}\\\\equiv \\\\alpha {\\\\bmod {\\\\mathfrak {a}}}}for allOK{\\\\displaystyle \\\\alpha \\\\in {\\\\mathcal {O}}_{K}}, whereN(a){\\\\displaystyle {\\\\rm {N}}({\\\\mathfrak {a}})}is the norm of the ideala{\\\\displaystyle {\\\\mathfrak {a}}}.  When K isQ{\\\\displaystyle \\\\mathbf {Q} }, the ideala{\\\\displaystyle {\\\\mathfrak {a}}}is principal, and if we let a be its positive generator then the ideala=(a){\\\\displaystyle {\\\\mathfrak {a}}=(a)}is Carmichael exactly when a is a Carmichael number in the usual sense.When K is larger than the rationals it is easy to write down Carmichael ideals inOK{\\\\displaystyle {\\\\mathcal {O}}_{K}}: for any prime number p that splits completely in K, the principal idealpOK{\\\\displaystyle p{\\\\mathcal {O}}_{K}}is a Carmichael ideal. Since infinitely many prime numbers split completely in any number field, there are infinitely many Carmichael ideals inOK{\\\\displaystyle {\\\\mathcal {O}}_{K}}. For example, if p is any prime number that is 1 mod 4, the ideal (p) in the Gaussian integers Z[\\u200ai\\u200a] is a Carmichael ideal.Both prime and Carmichael numbers satisfy the following equality:gcd(x=1n1xn1,n)=1.{\\\\displaystyle \\\\gcd \\\\left(\\\\sum _{x=1}^{n-1}x^{n-1},n\\\\right)=1.}LucasCarmichael numberA positive composite integern{\\\\displaystyle n}is a LucasCarmichael number if and only ifn{\\\\displaystyle n}is square-free, and for all prime divisorsp{\\\\displaystyle p}ofn{\\\\displaystyle n}, it is true thatp+1n+1{\\\\displaystyle p+1\\\\mid n+1}. The first LucasCarmichael numbers are:399, 935, 2015, 2915, 4991, 5719, 7055, 8855, 12719, 18095, 20705, 20999, 22847, 29315, 31535, 46079, 51359, 60059, 63503, 67199, 73535, 76751, 80189, 81719, 88559, 90287, ... (sequence A006972 in the OEIS)QuasiCarmichael numberQuasiCarmichael numbers are squarefree composite numbers n with the property that for every prime factor p of n, p\\u2009+\\u2009b divides n\\u2009+\\u2009b positively with b being any integer besides 0. If b = 1, these are Carmichael numbers, and if b = 1, these are LucasCarmichael numbers. The first QuasiCarmichael numbers are:35, 77, 143, 165, 187, 209, 221, 231, 247, 273, 299, 323, 357, 391, 399, 437, 493, 527, 561, 589, 598, 713, 715, 899, 935, 943, 989, 1015, 1073, 1105, 1147, 1189, 1247, 1271, 1295, 1333, 1517, 1537, 1547, 1591, 1595, 1705, 1729, ... (sequence A257750 in the OEIS)Kndel numberAn n-Kndel number for a given positive integer n is a composite number m with the property that each i < m coprime to m satisfiesimn1(modm){\\\\displaystyle i^{m-n}\\\\equiv 1{\\\\pmod {m}}}. The n = 1 case are Carmichael numbers.Higher-order Carmichael numbersCarmichael numbers can be generalized using concepts of abstract algebra.The above definition states that a composite integer n is Carmichaelprecisely when the nth-power-raising function pn from the ring Zn of integers modulo n to itself is the identity function. The identity is the only Zn-algebra endomorphism on Zn so we can restate the definition as asking that pn be an algebra endomorphism of Zn.As above, pn satisfies the same property whenever n is prime.The nth-power-raising function pn is also defined on any Zn-algebra A. A theorem states that n is prime if and only if all such functions pn are algebra endomorphisms.In-between these two conditions lies the definition of Carmichael number of order m for any positive integer m as any composite number n such that pn is an endomorphism on every Zn-algebra that can be generated as Zn-module by m elements. Carmichael numbers of order 1 are just the ordinary Carmichael numbers.An order 2 Carmichael numberAccording to Howe, 17  31  41  43  89  97  167  331 is an order 2 Carmichael number. This product is equal to 443,372,888,629,441.PropertiesKorselt\\'s criterion can be generalized to higher-order Carmichael numbers, as shown by Howe.A heuristic argument, given in the same paper, appears to suggest that there are infinitely many Carmichael numbers of order m, for any m. However, not a single Carmichael number of order 3 or above is known.NotesReferencesCarmichael, R. D. (1910). \"Note on a new number theory function\". Bulletin of the American Mathematical Society. 16 (5): 232238. doi:10.1090/s0002-9904-1910-01892-9.Carmichael, R. D. (1912). \"On composite numbers P which satisfy the Fermat congruenceaP11modP{\\\\displaystyle a^{P-1}\\\\equiv 1{\\\\bmod {P}}}\". American Mathematical Monthly. 19 (2): 2227. doi:10.2307/2972687. JSTOR 2972687.Chernick, J. (1939). \"On Fermat\\'s simple theorem\" (PDF). Bull. Amer. Math. Soc. 45 (4): 269274. doi:10.1090/S0002-9904-1939-06953-X.Korselt, A. R. (1899). \"Problme chinois\". L\\'Intermdiaire des Mathmaticiens. 6: 142143.Lh, G.; Niebuhr, W. (1996). \"A new algorithm for constructing large Carmichael numbers\" (PDF). Math. Comp. 65 (214): 823836. Bibcode:1996MaCom..65..823L. doi:10.1090/S0025-5718-96-00692-8.Ribenboim, P. (1989). The Book of Prime Number Records. Springer. ISBN 978-0-387-97042-4.imerka, V. (1885). \"Zbytky z arithmetick posloupnosti (On the remainders of an arithmetic progression)\". asopis Pro Pstovn Matematiky a Fysiky. 14 (5): 221225. doi:10.21136/CPMF.1885.122245.External links\"Carmichael number\", Encyclopedia of Mathematics, EMS Press, 2001 [1994]Encyclopedia of MathematicsTable of Carmichael numbersTables of Carmichael numbers with many prime factorsTables of Carmichael numbers below1018{\\\\displaystyle 10^{18}}\"The Dullness of 1729\". MathPages.com.Weisstein, Eric W. \"Carmichael Number\". MathWorld.Final Answers Modular Arithmetic')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = []\n",
    "\n",
    "for fname in tqdm(glob('wiki_data/texts*.txt')):\n",
    "    with open(fname) as f:\n",
    "        document = \"\"\n",
    "        for line in f:\n",
    "            document = document + line.strip()\n",
    "        \n",
    "        documents.append(document)\n",
    "\n",
    "len(documents), documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZivmL96lbiv"
   },
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "c-wjC5unVQoQ"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\", disable=[\"parser\", \"ner\", \"tagger\"])\n",
    "\n",
    "def spacy_tokenizer(text):\n",
    "    return [t.lemma_ for t in nlp(text)]\n",
    "\n",
    "vec = TfidfVectorizer(tokenizer=spacy_tokenizer)\n",
    "trained_vectors = vec.fit_transform(documents).todense()\n",
    "texts = [[document, vector] for document, vector in zip(documents, trained_vectors)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVQKy5G7WJTh"
   },
   "source": [
    "Searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S-Udd4uQxtvz",
    "outputId": "e7b701d4-ade9-4d19-e761-6aeb1b69d0d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['= Dijkstra\\'s algorithm =Dijkstra\\'s algorithm ( DYKE-strz) is an algorithm for finding the shortest paths between nodes in a graph, which may represent, for example, road networks.  It was conceived by computer scientist Edsger W. Dijkstra in 1956 and published three years later.The algorithm exists in many variants. Dijkstra\\'s original algorithm found the shortest path between two given nodes, but a more common variant fixes a single node as the \"source\" node and finds shortest paths from the source to all other nodes in the graph, producing a shortest-path tree.For a given source node in the graph, the algorithm finds the shortest path between that node and every other.:\\u200a196206\\u200a It can also be used for finding the shortest paths from a single node to a single destination node by stopping the algorithm once the shortest path to the destination node has been determined. For example, if the nodes of the graph represent cities and edge path costs represent driving distances between pairs of cities connected by a direct road (for simplicity, ignore red lights, stop signs, toll roads and other obstructions), Dijkstra\\'s algorithm can be used to find the shortest route between one city and all other cities. A widely used application of shortest path algorithms is network routing protocols, most notably IS-IS (Intermediate System to Intermediate System) and Open Shortest Path First (OSPF). It is also employed as a subroutine in other algorithms such as Johnson\\'s.The Dijkstra algorithm uses labels that are positive integers or real numbers, which are totally ordered.  It can be generalized to use any labels that are partially ordered, provided the subsequent labels (a subsequent label is produced when traversing an edge) are monotonically non-decreasing.  This generalization is called the generic Dijkstra shortest-path algorithm.Dijkstra\\'s algorithm uses a data structure for storing and querying partial solutions sorted by distance from the start. While the original algorithm uses a min-priority queue and runs in time((|V|+|E|)log\\u2061|V|){\\\\displaystyle \\\\Theta ((|V|+|E|)\\\\log |V|)}(where|V|{\\\\displaystyle |V|}is the number of nodes and|E|{\\\\displaystyle |E|}is the number of edges), it can also be implemented in(|V|2){\\\\displaystyle \\\\Theta (|V|^{2})}using an array. The idea of this algorithm is also given in Leyzorek et al. 1957. Fredman & Tarjan 1984 propose using a Fibonacci heap min-priority queue to optimize the running time complexity to(|E|+|V|log\\u2061|V|){\\\\displaystyle \\\\Theta (|E|+|V|\\\\log |V|)}. This is asymptotically the fastest known single-source shortest-path algorithm for arbitrary directed graphs with unbounded non-negative weights. However, specialized cases (such as bounded/integer weights, directed acyclic graphs etc.) can indeed be improved further as detailed in Specialized variants. Additionally, if preprocessing is allowed algorithms such as contraction hierarchies can be up to seven orders of magnitude faster.In some fields, artificial intelligence in particular, Dijkstra\\'s algorithm or a variant of it is known as uniform cost search and formulated as an instance of the more general idea of best-first search.HistoryWhat is the shortest way to travel from Rotterdam to Groningen, in general: from given city to given city. It is the algorithm for the shortest path, which I designed in about twenty minutes. One morning I was shopping in Amsterdam with my young fiance, and tired, we sat down on the caf terrace to drink a cup of coffee and I was just thinking about whether I could do this, and I then designed the algorithm for the shortest path. As I said, it was a twenty-minute invention. In fact, it was published in \\'59, three years later. The publication is still readable, it is, in fact, quite nice. One of the reasons that it is so nice was that I designed it without pencil and paper. I learned later that one of the advantages of designing without pencil and paper is that you are almost forced to avoid all avoidable complexities. Eventually, that algorithm became to my great amazement, one of the cornerstones of my fame.Dijkstra thought about the shortest path problem when working at the Mathematical Center in Amsterdam in 1956 as a programmer to demonstrate the capabilities of a new computer called ARMAC. His objective was to choose both a problem and a solution (that would be produced by computer) that non-computing people could understand. He designed the shortest path algorithm and later implemented it for ARMAC for a slightly simplified transportation map of 64 cities in the Netherlands (64, so that 6 bits would be sufficient to encode the city number). A year later, he came across another problem from hardware engineers working on the institute\\'s next computer: minimize the amount of wire needed to connect the pins on the back panel of the machine. As a solution, he re-discovered the algorithm known as Prim\\'s minimal spanning tree algorithm (known earlier to Jarnk, and also rediscovered by Prim). Dijkstra published the algorithm in 1959, two years after Prim and 29 years after Jarnk.AlgorithmLet the node at which we are starting be called the initial node. Let the distance of node Y be the distance from the initial node to Y. Dijkstra\\'s algorithm will initially start with infinite distances and will try to improve them step by step.Mark all nodes unvisited. Create a set of all the unvisited nodes called the unvisited set.Assign to every node a tentative distance value: set it to zero for our initial node and to infinity for all other nodes. The tentative distance of a node v is the length of the shortest path discovered so far between the node v and the starting node. Since initially no path is known to any other vertex than the source itself (which is a path of length zero), all other tentative distances are initially set to infinity. Set the initial node as current.For the current node, consider all of its unvisited neighbors and calculate their tentative distances through the current node. Compare the newly calculated tentative distance to the current assigned value and assign the smaller one. For example, if the current node A is marked with a  distance of 6, and the edge connecting it with a neighbor B has length 2, then the distance to B through A will be 6 + 2 = 8. If B was previously marked with a distance greater than 8 then change it to 8. Otherwise, the current value will be kept.When we are done considering all of the unvisited neighbors of the current node, mark the current node as visited and remove it from the unvisited set. A visited node will never be checked again.If the destination node has been marked visited (when planning a route between two specific nodes) or if the smallest tentative distance among the nodes in the unvisited set is infinity (when planning a complete traversal; occurs when there is no connection between the initial node and remaining unvisited nodes), then stop. The algorithm has finished.Otherwise, select the unvisited node that is marked with the smallest tentative distance, set it as the new current node, and go back to step 3.When planning a route, it is actually not necessary to wait until the destination node is \"visited\" as above: the algorithm can stop once the destination node has the smallest tentative distance among all \"unvisited\" nodes (and thus could be selected as the next \"current\").DescriptionSuppose you would like to find the shortest path between two intersections on a city map: a starting point and a destination. Dijkstra\\'s algorithm initially marks the distance (from the starting point) to every other intersection on the map with infinity. This is done not to imply that there is an infinite distance, but to note that those intersections have not been visited yet. Some variants of this method leave the intersections\\' distances unlabeled. Now select the current intersection at each iteration.  For the first iteration, the current intersection will be the starting point, and the distance to it (the intersection\\'s label) will be zero. For subsequent iterations (after the first), the current intersection will be a closest unvisited intersection to the starting point (this will be easy to find).From the current intersection, update the distance to every unvisited intersection that is directly connected to it. This is done by determining the sum of the distance between an unvisited intersection and the value of the current intersection and then relabeling the unvisited intersection with this value (the sum) if it is less than the unvisited intersection\\'s current value. In effect, the intersection is relabeled if the path to it through the current intersection is shorter than the previously known paths.  To facilitate shortest path identification, in pencil, mark the road with an arrow pointing to the relabeled intersection if you label/relabel it, and erase all others pointing to it.  After you have updated the distances to each neighboring intersection, mark the current intersection as visited and select an unvisited intersection with minimal distance (from the starting point)  or the lowest labelas the current intersection. Intersections marked as visited are labeled with the shortest path from the starting point to it and will not be revisited or returned to.Continue this process of updating the neighboring intersections with the shortest distances, marking the current intersection as visited, and moving onto a closest unvisited intersection until you have marked the destination as visited. Once you have marked the destination as visited (as is the case with any visited intersection), you have determined the shortest path to it from the starting point and can trace your way back following the arrows in reverse. In the algorithm\\'s implementations, this is usually done (after the algorithm has reached the destination node) by following the nodes\\' parents from the destination node up to the starting node; that\\'s why we also keep track of each node\\'s parent.This algorithm makes no attempt of direct \"exploration\" towards the destination as one might expect. Rather, the sole consideration in determining the next \"current\" intersection is its distance from the starting point. This algorithm therefore expands outward from the starting point, interactively considering every node that is closer in terms of shortest path distance until it reaches the destination. When understood in this way, it is clear how the algorithm necessarily finds the shortest path. However, it may also reveal one of the algorithm\\'s weaknesses: its relative slowness in some topologies.PseudocodeIn the following pseudocode algorithm, dist is an array that contains the current distances from the source to other vertices, i.e. dist[u] is the current distance from the source to the vertex u. The prev array contains pointers to previous-hop nodes on the shortest path from source to the given vertex (equivalently, it is the next-hop on the path from the given vertex to the source). The code u  vertex in Q with min dist[u], searches for the vertex u in the vertex set Q that has the least dist[u] value. Graph.Edges(u, v) returns the length of the edge joining (i.e. the distance between) the two neighbor-nodes u and v. The variable alt on line 14 is the length of the path from the root node to the neighbor node v if it were to go through u. If this path is shorter than the current shortest path recorded for v, that current path is replaced with this alt path.1  function Dijkstra(Graph, source):23      for each vertex v in Graph.Vertices:4          dist[v]  INFINITY5          prev[v]  UNDEFINED6          add v to Q7      dist[source]  089      while Q is not empty:10          u  vertex in Q with min dist[u]1112          for each neighbor v of u still in Q:13              alt  dist[u] + Graph.Edges(u, v)14              if alt < dist[v]:15                  dist[v]  alt16                  prev[v]  u1718      return dist[], prev[]If we are only interested in a shortest path between vertices source and target, we can terminate the search after line 10 if u = target.Now we can read the shortest path from source to target by reverse iteration:1  S  empty sequence2  u  target3  if prev[u] is defined or u = source:          // Do something only if the vertex is reachable4      while u is defined:                       // Construct the shortest path with a stack S5          insert u at the beginning of S        // Push the vertex onto the stack6          u  prev[u]                           // Traverse from target to sourceNow sequence S is the list of vertices constituting one of the shortest paths from source to target, or the empty sequence if no path exists.A more general problem would be to find all the shortest paths between source and target (there might be several different ones of the same length). Then instead of storing only a single node in each entry of prev[] we would store all nodes satisfying the relaxation condition. For example, if both r and source connect to target and both of them lie on different shortest paths through target (because the edge cost is the same in both cases), then we would add both r and source to prev[target]. When the algorithm completes, prev[] data structure will actually describe a graph that is a subset of the original graph with some edges removed. Its key property will be that if the algorithm was run with some starting node, then every path from that node to any other node in the new graph will be the shortest path between those nodes in the original graph, and all paths of that length from the original graph will be present in the new graph. Then to actually find all these shortest paths between two given nodes we would use a path finding algorithm on the new graph, such as depth-first search.Using a priority queueA min-priority queue is an abstract data type that provides 3 basic operations: add_with_priority(), decrease_priority() and extract_min(). As mentioned earlier, using such a data structure can lead to faster computing times than using a basic queue. Notably, Fibonacci heap (Fredman & Tarjan 1984) or Brodal queue offer optimal implementations for those 3 operations. As the algorithm is slightly different, we mention it here, in pseudo-code as well :1  function Dijkstra(Graph, source):2      dist[source]  0                           // Initialization34      create vertex priority queue Q56      for each vertex v in Graph.Vertices:7          if v  source8              dist[v]  INFINITY                 // Unknown distance from source to v9              prev[v]  UNDEFINED                // Predecessor of v1011         Q.add_with_priority(v, dist[v])121314     while Q is not empty:                      // The main loop15         u  Q.extract_min()                    // Remove and return best vertex16         for each neighbor v of u:              // only v that are still in Q17             alt  dist[u] + Graph.Edges(u, v)18             if alt < dist[v]19                 dist[v]  alt20                 prev[v]  u21                 Q.decrease_priority(v, alt)2223     return dist, prevInstead of filling the priority queue with all nodes in the initialization phase, it is also possible to initialize it to contain only source; then, inside the if alt < dist[v] block, the decrease_priority() becomes an add_with_priority() operation if the node is not already in the queue.:\\u200a198\\u200aYet another alternative is to add nodes unconditionally to the priority queue and to instead check after extraction that no shorter connection was found yet. This can be done by additionally extracting the associated priority p from the queue and only processing further if p == dist[u] inside the while Q is not empty loop. These alternatives can use entirely array-based priority queues without decrease-key functionality, which have been found to achieve even faster computing times in practice. However, the difference in performance was found to be narrower for denser graphs.Proof of correctnessProof of Dijkstra\\'s algorithm is constructed by induction on the number of visited nodes.Invariant hypothesis: For each node v, dist[v] is the shortest distance from source to v when traveling via visited nodes only, or infinity if no such path exists. (Note: we do not assume dist[v] is the actual shortest distance for unvisited nodes.)The base case is when there is just one visited node, namely the initial node source, in which case the hypothesis is trivial.Otherwise, assume the hypothesis for n-1 visited nodes. In which case, we choose an edge vu where u has the least dist[u] of any unvisited nodes such that dist[u] = dist[v] + Graph.Edges[v,u]. dist[u] is considered to be the shortest distance from source to u because if there were a shorter path, and if w was the first unvisited node on that path then by the original hypothesis dist[w] > dist[u] which creates a contradiction. Similarly if there were a shorter path to u without using unvisited nodes, and if the last but one node on that path were w, then we would have had dist[u] = dist[w] + Graph.Edges[w,u], also a contradiction.After processing u it will still be true that for each unvisited node w, dist[w] will be the shortest distance from source to w using visited nodes only, because if there were a shorter path that doesn\\'t go by u we would have found it previously, and if there were a shorter path using u we would have updated it when processing u.After all nodes are visited, the shortest path from source to any node v consists only of visited nodes, therefore dist[v] is the shortest distance.Running timeBounds of the running time of Dijkstra\\'s algorithm on a graph with edges E and vertices V can be expressed as a function of the number of edges, denoted|E|{\\\\displaystyle |E|}, and the number of vertices, denoted|V|{\\\\displaystyle |V|}, using big-O notation. The complexity bound depends mainly on the data structure used to represent the set Q. In the following, upper bounds can be simplified because|E|{\\\\displaystyle |E|}isO(|V|2){\\\\displaystyle O(|V|^{2})}for any graph, but that simplification disregards the fact that in some problems, other upper bounds on|E|{\\\\displaystyle |E|}may hold.For any data structure for the vertex set Q, the running time is in(|E|Tdk+|V|Tem),{\\\\displaystyle \\\\Theta (|E|\\\\cdot T_{\\\\mathrm {dk} }+|V|\\\\cdot T_{\\\\mathrm {em} }),}whereTdk{\\\\displaystyle T_{\\\\mathrm {dk} }}andTem{\\\\displaystyle T_{\\\\mathrm {em} }}are the complexities of the decrease-key and extract-minimum operations in Q, respectively.The simplest version of Dijkstra\\'s algorithm stores the vertex set Q as an linked list or array, and edges as an adjacency list or matrix. In this case, extract-minimum is simply a linear search through all vertices in Q, so the running time is(|E|+|V|2)=(|V|2){\\\\displaystyle \\\\Theta (|E|+|V|^{2})=\\\\Theta (|V|^{2})}.For sparse graphs, that is, graphs with far fewer than|V|2{\\\\displaystyle |V|^{2}}edges, Dijkstra\\'s algorithm can be implemented more efficiently by storing the graph in the form of adjacency lists and using a self-balancing binary search tree, binary heap, pairing heap, or Fibonacci heap as a priority queue to implement extracting minimum efficiently. To perform decrease-key steps in a binary heap efficiently, it is necessary to use an auxiliary data structure that maps each vertex to its position in the heap, and to keep this structure up to date as the priority queue Q changes. With a self-balancing binary search tree or binary heap, the algorithm requires((|E|+|V|)log\\u2061|V|){\\\\displaystyle \\\\Theta ((|E|+|V|)\\\\log |V|)}time in the worst case (wherelog{\\\\displaystyle \\\\log }denotes the binary logarithmlog2{\\\\displaystyle \\\\log _{2}}); for connected graphs this time bound can be simplified to(|E|log\\u2061|V|){\\\\displaystyle \\\\Theta (|E|\\\\log |V|)}.  The Fibonacci heap improves this to(|E|+|V|log\\u2061|V|).{\\\\displaystyle \\\\Theta (|E|+|V|\\\\log |V|).}When using binary heaps, the average case time complexity is lower than the worst-case: assuming edge costs are drawn independently from a common probability distribution, the expected number of decrease-key operations is bounded by(|V|log\\u2061(|E|/|V|)){\\\\displaystyle \\\\Theta (|V|\\\\log(|E|/|V|))}, giving a total running time of:\\u200a199200O(|E|+|V|log\\u2061|E||V|log\\u2061|V|).{\\\\displaystyle O\\\\left(|E|+|V|\\\\log {\\\\frac {|E|}{|V|}}\\\\log |V|\\\\right).}Practical optimizations and infinite graphsIn common presentations of Dijkstra\\'s algorithm, initially all nodes are entered into the priority queue. This is, however, not necessary: the algorithm can start with a priority queue that contains only one item, and insert new items as they are discovered (instead of doing a decrease-key, check whether the key is in the queue; if it is, decrease its key, otherwise insert it).:\\u200a198\\u200a This variant has the same worst-case bounds as the common variant, but maintains a smaller priority queue in practice, speeding up the queue operations.Moreover, not inserting all nodes in a graph makes it possible to extend the algorithm to find the shortest path from a single source to the closest of a set of target nodes on infinite graphs or those too large to represent in memory. The resulting algorithm is called uniform-cost search (UCS) in the artificial intelligence literature and can be expressed in pseudocode asprocedure uniform_cost_search(start) isnode  startfrontier  priority queue containing node onlyexplored  empty setdoif frontier is empty thenreturn failurenode  frontier.pop()if node is a goal state thenreturn solution(node)explored.add(node)for each of node\\'s neighbors n doif n is not in explored and not in frontier thenfrontier.add(n)else if n is in frontier with higher costreplace existing node with nThe complexity of this algorithm can be expressed in an alternative way for very large graphs: when C* is the length of the shortest path from the start node to any node satisfying the \"goal\" predicate, each edge has cost at least , and the number of neighbors per node is bounded by b, then the algorithm\\'s worst-case time and space complexity are both in O(b1+C*  ).Further optimizations of Dijkstra\\'s algorithm for the single-target case include bidirectional variants, goal-directed variants such as the A* algorithm (see  Related problems and algorithms), graph pruning to determine which nodes are likely to form the middle segment of shortest paths (reach-based routing), and hierarchical decompositions of the input graph that reduce st routing to connecting s and t to their respective \"transit nodes\" followed by shortest-path computation between these transit nodes using a \"highway\".Combinations of such techniques may be needed for optimal practical performance on specific problems.Specialized variantsWhen arc weights are small integers (bounded by a parameterC{\\\\displaystyle C}), specialized queues which take advantage of this fact can be used to speed up Dijkstra\\'s algorithm. The first algorithm of this type was Dial\\'s algorithm (Dial 1969) for graphs with positive integer edge weights, which uses a bucket queue to obtain a running timeO(|E|+|V|C){\\\\displaystyle O(|E|+|V|C)}. The use of a Van Emde Boas tree as the priority queue brings the complexity toO(|E|log\\u2061log\\u2061C){\\\\displaystyle O(|E|\\\\log \\\\log C)}(Ahuja et al. 1990). Another interesting variant based on a combination of a new radix heap and the well-known Fibonacci heap runs in timeO(|E|+|V|log\\u2061C){\\\\displaystyle O(|E|+|V|{\\\\sqrt {\\\\log C}})}(Ahuja et al. 1990). Finally, the best algorithms in this special case are as follows. The algorithm given by (Thorup 2000) runs inO(|E|log\\u2061log\\u2061|V|){\\\\displaystyle O(|E|\\\\log \\\\log |V|)}time and the algorithm given by (Raman 1997) runs inO(|E|+|V|min{(log\\u2061|V|)1/3+,(log\\u2061C)1/4+}){\\\\displaystyle O(|E|+|V|\\\\min\\\\{(\\\\log |V|)^{1/3+\\\\varepsilon },(\\\\log C)^{1/4+\\\\varepsilon }\\\\})}time.Related problems and algorithmsThe functionality of Dijkstra\\'s original algorithm can be extended with a variety of modifications. For example, sometimes it is desirable to present solutions which are less than mathematically optimal. To obtain a ranked list of less-than-optimal solutions, the optimal solution is first calculated. A single edge appearing in the optimal solution is removed from the graph, and the optimum solution to this new graph is calculated. Each edge of the original solution is suppressed in turn and a new shortest-path calculated. The secondary solutions are then ranked and presented after the first optimal solution.Dijkstra\\'s algorithm is usually the working principle behind link-state routing protocols, OSPF and IS-IS being the most common ones.Unlike Dijkstra\\'s algorithm, the BellmanFord algorithm can be used on graphs with negative edge weights, as long as the graph contains no negative cycle reachable from the source vertex s. The presence of such cycles means there is no shortest path, since the total weight becomes lower each time the cycle is traversed. (This statement assumes that a \"path\" is allowed to repeat vertices. In graph theory that is normally not allowed.  In theoretical computer science it often is allowed.) It is possible to adapt Dijkstra\\'s algorithm to handle negative weight edges by combining it with the Bellman-Ford algorithm (to remove negative edges and detect negative cycles), such an algorithm is called Johnson\\'s algorithm.The A* algorithm is a generalization of Dijkstra\\'s algorithm that cuts down on the size of the subgraph that must be explored, if additional information is available that provides a lower bound on the \"distance\" to the target. This approach can be viewed from the perspective of linear programming: there is a natural linear program for computing shortest paths, and solutions to its dual linear program are feasible if and only if they form a consistent heuristic (speaking roughly, since the sign conventions differ from place to place in the literature). This feasible dual / consistent heuristic defines a non-negative reduced cost and A* is essentially running Dijkstra\\'s algorithm with these reduced costs. If the dual satisfies the weaker condition of admissibility, then A* is instead more akin to the BellmanFord algorithm.The process that underlies Dijkstra\\'s algorithm is similar to the greedy process used in Prim\\'s algorithm.  Prim\\'s purpose is to find a minimum spanning tree that connects all nodes in the graph; Dijkstra is concerned with only two nodes. Prim\\'s does not evaluate the total weight of the path from the starting node, only the individual edges.Breadth-first search can be viewed as a special-case of Dijkstra\\'s algorithm on unweighted graphs, where the priority queue degenerates into a FIFO queue.The fast marching method can be viewed as a continuous version of Dijkstra\\'s algorithm which computes the geodesic distance on a triangle mesh.Dynamic programming perspectiveFrom a dynamic programming point of view, Dijkstra\\'s algorithm is a successive approximation scheme that solves the dynamic programming functional equation for the shortest path problem by the Reaching method.In fact, Dijkstra\\'s explanation of the logic behind the algorithm, namelyProblem 2. Find the path of minimum total length between two given nodesP{\\\\displaystyle P}andQ{\\\\displaystyle Q}.We use the fact that, ifR{\\\\displaystyle R}is a node on the minimal path fromP{\\\\displaystyle P}toQ{\\\\displaystyle Q}, knowledge of the latter implies the knowledge of the minimal path fromP{\\\\displaystyle P}toR{\\\\displaystyle R}.is a paraphrasing of Bellman\\'s famous Principle of Optimality in the context of the shortest path problem.ApplicationsLeast-cost paths are calculated for instance to establish tracks of electricity lines or oil pipelines. The algorithm has also been used to calculate optimal long-distance footpaths in Ethiopia and contrast them with the situation on the ground.See alsoA* search algorithmBellmanFord algorithmEuclidean shortest pathFloydWarshall algorithmJohnson\\'s algorithmLongest path problemParallel all-pairs shortest path algorithmNotesReferencesCormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L.; Stein, Clifford (2001). \"Section 24.3: Dijkstra\\'s algorithm\". Introduction to Algorithms (Second ed.). MIT Press and McGrawHill. pp. 595601. ISBN 0-262-03293-7.Dial, Robert B. (1969). \"Algorithm 360: Shortest-path forest with topological ordering [H]\". Communications of the ACM. 12 (11): 632633. doi:10.1145/363269.363610. S2CID 6754003.Fredman, Michael Lawrence; Tarjan, Robert E. (1984). Fibonacci heaps and their uses in improved network optimization algorithms. 25th Annual Symposium on Foundations of Computer Science. IEEE. pp. 338346. doi:10.1109/SFCS.1984.715934.Fredman, Michael Lawrence; Tarjan, Robert E. (1987). \"Fibonacci heaps and their uses in improved network optimization algorithms\". Journal of the Association for Computing Machinery. 34 (3): 596615. doi:10.1145/28869.28874. S2CID 7904683.Zhan, F. Benjamin; Noon, Charles E. (February 1998). \"Shortest Path Algorithms: An Evaluation Using Real Road Networks\". Transportation Science. 32 (1): 6573. doi:10.1287/trsc.32.1.65. S2CID 14986297.Leyzorek, M.; Gray, R. S.; Johnson, A. A.; Ladew, W. C.; Meaker, Jr., S. R.; Petry, R. M.; Seitz, R. N. (1957). Investigation of Model Techniques  First Annual Report  6 June 1956  1 July 1957  A Study of Model Techniques for Communication Systems. Cleveland, Ohio: Case Institute of Technology.Knuth, D.E. (1977). \"A Generalization of Dijkstra\\'s Algorithm\". Information Processing Letters. 6 (1): 15. doi:10.1016/0020-0190(77)90002-3.Ahuja, Ravindra K.; Mehlhorn, Kurt; Orlin, James B.; Tarjan, Robert E. (April 1990). \"Faster Algorithms for the Shortest Path Problem\" (PDF). Journal of the ACM. 37 (2): 213223. doi:10.1145/77600.77615. hdl:1721.1/47994. S2CID 5499589.Raman, Rajeev (1997). \"Recent results on the single-source shortest paths problem\". SIGACT News. 28 (2): 8187. doi:10.1145/261342.261352. S2CID 18031586.Thorup, Mikkel (2000). \"On RAM priority Queues\". SIAM Journal on Computing. 30 (1): 86109. doi:10.1137/S0097539795288246. S2CID 5221089.Thorup, Mikkel (1999). \"Undirected single-source shortest paths with positive integer weights in linear time\". Journal of the ACM. 46 (3): 362394. doi:10.1145/316542.316548. S2CID 207654795.External linksOral history interview with Edsger W. Dijkstra, Charles Babbage Institute, University of Minnesota, MinneapolisImplementation of Dijkstra\\'s algorithm using TDD, Robert Cecil Martin, The Clean Code Blog',\n",
       " \"= Johnson's algorithm =Johnson's algorithm is a way to find the shortest paths between all pairs of vertices in an edge-weighted directed graph. It allows some of the edge weights to be negative numbers, but no negative-weight cycles may exist. It works by using the BellmanFord algorithm to compute a transformation of the input graph that removes all negative weights, allowing Dijkstra's algorithm to be used on the transformed graph. It is named after Donald B. Johnson, who first published the technique in 1977.A similar reweighting technique is also used in Suurballe's algorithm for finding two disjoint paths of minimum total length between the same two vertices in a graph with non-negative edge weights.Algorithm descriptionJohnson's algorithm consists of the following steps:First, a new node q is added to the graph, connected by zero-weight edges to each of the other nodes.Second, the BellmanFord algorithm is used, starting from the new vertex q, to find for each vertex v the minimum weight h(v) of a path from q to v. If this step detects a negative cycle, the algorithm is terminated.Next the edges of the original graph are reweighted using the values computed by the BellmanFord algorithm: an edge from u to v, having lengthw(u,v){\\\\displaystyle w(u,v)}, is given the new length w(u,v) + h(u)  h(v).Finally, q is removed, and Dijkstra's algorithm is used to find the shortest paths from each node s to every other vertex in the reweighted graph. The distance in the original graph is then computed for each distance D(u , v), by adding h(v)  h(u) to the distance returned by Dijkstra's algorithm.ExampleThe first three stages of Johnson's algorithm are depicted in the illustration below.The graph on the left of the illustration has two negative edges, but no negative cycles. The center graph shows the new vertex q, a shortest path tree as computed by the BellmanFord algorithm with q as starting vertex, and the values h(v) computed at each other node as the length of the shortest path from q to that node. Note that these values are all non-positive, because q has a length-zero edge to each vertex and the shortest path can be no longer than that edge. On the right is shown the reweighted graph, formed by replacing each edge weightw(u,v){\\\\displaystyle w(u,v)}by w(u,v) + h(u)  h(v). In this reweighted graph, all edge weights are non-negative, but the shortest path between any two nodes uses the same sequence of edges as the shortest path between the same two nodes in the original graph. The algorithm concludes by applying Dijkstra's algorithm to each of the four starting nodes in the reweighted graph.CorrectnessIn the reweighted graph, all paths between a pair s and t of nodes have the same quantity h(s)  h(t) added to them. The previous statement can be proven as follows: Let p be anst{\\\\displaystyle s-t}path. Its weight W in the reweighted graph is given by the following expression:(w(s,p1)+h(s)h(p1))+(w(p1,p2)+h(p1)h(p2))+...+(w(pn,t)+h(pn)h(t)).{\\\\displaystyle {\\\\bigl (}w(s,p_{1})+h(s)-h(p_{1}){\\\\bigr )}+{\\\\bigl (}w(p_{1},p_{2})+h(p_{1})-h(p_{2}){\\\\bigr )}+...+{\\\\bigl (}w(p_{n},t)+h(p_{n})-h(t){\\\\bigr )}.}Every+h(pi){\\\\displaystyle +h(p_{i})}is cancelled byh(pi){\\\\displaystyle -h(p_{i})}in the previous bracketed expression; therefore, we are left with the following expression for W:(w(s,p1)+w(p1,p2)+...+w(pn,t))+h(s)h(t){\\\\displaystyle {\\\\bigl (}w(s,p_{1})+w(p_{1},p_{2})+...+w(p_{n},t){\\\\bigr )}+h(s)-h(t)}The bracketed expression is the weight of p in the original weighting.Since the reweighting adds the same amount to the weight of everyst{\\\\displaystyle s-t}path, a path is a shortest path in the original weighting if and only if it is a shortest path after reweighting. The weight of edges that belong to a shortest path from q to any node is zero, and therefore the lengths of the shortest paths from q to every node become zero in the reweighted graph; however, they still remain shortest paths. Therefore, there can be no negative edges: if edge uv had a negative weight after the reweighting, then the zero-length path from q to u together with this edge would form a negative-length path from q to v, contradicting the fact that all vertices have zero distance from q. The non-existence of negative edges ensures the optimality of the paths found by Dijkstra's algorithm. The distances in the original graph may be calculated from the distances calculated by Dijkstra's algorithm in the reweighted graph by reversing the reweighting transformation.AnalysisThe time complexity of this algorithm, using Fibonacci heaps in the implementation of Dijkstra's algorithm, isO(|V|2log\\u2061|V|+|V||E|){\\\\displaystyle O(|V|^{2}\\\\log |V|+|V||E|)}: the algorithm usesO(|V||E|){\\\\displaystyle O(|V||E|)}time for the BellmanFord stage of the algorithm, andO(|V|log\\u2061|V|+|E|){\\\\displaystyle O(|V|\\\\log |V|+|E|)}for each of the|V|{\\\\displaystyle |V|}instantiations of Dijkstra's algorithm. Thus, when the graph is sparse, the total time can be faster than the FloydWarshall algorithm, which solves the same problem in timeO(|V|3){\\\\displaystyle O(|V|^{3})}.ReferencesExternal linksBoost: All Pairs Shortest Paths\",\n",
       " \"= DijkstraScholten algorithm =The DijkstraScholten algorithm (named after Edsger W. Dijkstra and Carel S. Scholten) is an algorithm for detecting termination in a distributed system. The algorithm was proposed by Dijkstra and Scholten in 1980.First, consider the case of a simple process graph which is a tree. A distributed computation which is tree-structured is not uncommon. Such a process graph may arise when the computation is strictly a divide-and-conquer type. A node starts the computation and divides the problem in two (or more, usually a multiple of 2) roughly equal parts and distribute those parts to other processors. This process continues recursively until the problems are of sufficiently small size to solve in a single processor.AlgorithmThe DijkstraScholten algorithm is a tree-based algorithm which can be described by the following:The initiator of a computation is the root of the tree.Upon receiving a computational message:If the receiving process is currently not in the computation: the process joins the tree by becoming a child of the sender of the message. (No acknowledgment message is sent at this point.)If the receiving process is already in the computation: the process immediately sends an acknowledgment message to the sender of the message.When a process has no more children and has become idle, the process detaches itself from the tree by sending an acknowledgment message to its tree parent.Termination occurs when the initiator has no children and has become idle.DijkstraScholten algorithm for a treeFor a tree, it is easy to detect termination. When a leaf process determines that it has terminated, it sends a signal to its parent. In general, a process waits for all its children to send signals and then it sends a signal to its parent.The program terminates when the root receives signals from all its children.DijkstraScholten algorithm for directed acyclic graphsThe algorithm for a tree can be extended to acyclic directed graphs. We add an additional integer attribute Deficit to each edge.On an incoming edge, Deficit will denote the difference between the number of messages received and the number of signals sent in reply.When a node wishes to terminate, it waits until it has received signals from outgoing edges reducing their deficits to zero.Then it sends enough signals to ensure that the deficit is zero on each incoming edge.Since the graph is acyclic, some nodes will have no outgoing edges and these nodes will be the first to terminate after sending enough signals to their incoming edges. After that the nodes at higher levels will terminate level by level.DijkstraScholten algorithm for cyclic directed graphsIf cycles are allowed, the previous algorithm does not work. This is because, there may not be any node with zero outgoing edges. So, potentially there is no node which can terminate without consulting other nodes.The DijkstraScholten algorithm solves this problem by implicitly creating a spanning tree of the graph. A spanning-tree is a tree which includes each node of the underlying graph once and the edge-set is a subset of the original set of edges.The tree will be directed (i.e., the channels will be directed) with the source node (which initiates the computation) as the root.The spanning-tree is created in the following way. A variable First_Edge is added to each node. When a node receives a message for the first time, it initializes First_Edge with the edge through which it received the message. First_Edge is never changed afterwards. Note that, the spanning tree is not unique and it depends on the order of messages in the system.Termination is handled by each node in three steps :Send signals on all incoming edges except the first edge. (Each node will send signals which reduces the deficit on each incoming edge to zero.)Wait for signals from all outgoing edges. (The number of signals received on each outgoing edge should reduce each of their deficits to zero.)Send signals on First_Edge. (Once steps 1 and 2 are complete, a node informs its parent in the spanning tree about its intention of terminating.)See alsoHuang's algorithm== References ==\",\n",
       " '= Suurballe\\'s algorithm =In theoretical computer science and network routing, Suurballe\\'s algorithm is an algorithm for finding two disjoint paths in a nonnegatively-weighted directed graph, so that both paths connect the same pair of vertices and have minimum total length. The algorithm was conceived by John W. Suurballe and published in 1974. The main idea of Suurballe\\'s algorithm is to use Dijkstra\\'s algorithm to find one path, to modify the weights of the graph edges, and then to run Dijkstra\\'s algorithm a second time. The output of the algorithm is formed by combining these two paths, discarding edges that are traversed in opposite directions by the paths, and using the remaining edges to form the two paths to return as the output.The modification to the weights is similar to the weight modification in Johnson\\'s algorithm, and preserves the non-negativity of the weights while allowing the second instance of Dijkstra\\'s algorithm to find the correct second path.The problem of finding two disjoint paths of minimum weight can be seen as a special case of a minimum cost flow problem, where in this case there are two units of \"flow\" and nodes have unit \"capacity\". Suurballe\\'s algorithm, also, can be seen as a special case of a minimum cost flow algorithm that repeatedly pushes the maximum possible amount of flow along a shortest augmenting path.The first path found by Suurballe\\'s algorithm is the shortest augmenting path for the initial (zero) flow, and the second path found by Suurballe\\'s algorithm is the shortest augmenting path for the residual graph left after pushing one unit of flow along the first path.DefinitionsLet G be a weighted directed graph with vertex set  V and edge set  E (figure A); let  s be a designated source vertex in  G, and let  t be a designated destination vertex. Let each edge  (u,v) in  E, from vertex  u to vertex  v, have a non-negative cost  w(u,v).Define d(s,u) to be the cost of the shortest path to vertex u from vertex s in the shortest path tree rooted at s (figure C).Note: Node and Vertex are often used interchangeably.AlgorithmSuurballe\\'s algorithm performs the following steps:Find the shortest path tree T rooted at node s by running Dijkstra\\'s algorithm (figure C). This tree contains for every vertex u, a shortest path from s to u. Let P1 be the shortest cost path from s to t (figure B). The edges in T are called tree edges and the remaining edges (the edges missing from figure C) are called non-tree edges.Modify the cost of each edge in the graph by replacing the cost w(u,v) of every edge (u,v) by w(u,v) = w(u,v)  d(s,v) + d(s,u). According to the resulting modified cost function, all tree edges have a cost of 0, and non-tree edges have a non-negative cost. For example:  If u=B, v=E, then w(u,v) = w(B,E)  d(A,E) + d(A,B) = 2  3 + 1 = 0  If u=E, v=B, then w(u,v) = w(E,B)  d(A,B) + d(A,E) = 2  1 + 3 = 4Create a residual graph Gt formed from G by removing the edges of G on path P1 that are directed into s and then reverse the direction of the zero length edges along path P1 (figure D).Find the shortest path P2 in the residual graph Gt by running Dijkstra\\'s algorithm (figure E).Discard the reversed edges of P2 from both paths. The remaining edges of P1 and P2 form a subgraph with two outgoing edges at s, two incoming edges at t, and one incoming and one outgoing edge at each remaining vertex. Therefore, this subgraph consists of two edge-disjoint paths from s to t and possibly some additional (zero-length) cycles. Return the two disjoint paths from the subgraph.ExampleThe following example shows how Suurballe\\'s algorithm finds the shortest pair of disjoint paths from A to F.Figure A illustrates a weighted graph G.Figure B calculates the shortest path P1 from A to F (ABDF).Figure C illustrates the shortest path tree T rooted at A, and the computed distances from A to every vertex (u).Figure D shows the residual graph Gt with the updated cost of each edge and the edges of path P1 reversed.Figure E calculates path P2 in the residual graph Gt (ACDBEF).Figure F illustrates both path P1 and path P2.Figure G finds the shortest pair of disjoint paths by combining the edges of paths P1 and P2 and then discarding the common reversed edges between both paths (BD). As a result, we get the two shortest pair of disjoint paths (ABEF) and (ACDF).CorrectnessThe weight of any path from s to t in the modified system of weights equals the weight in the original graph, minus d(s,t). Therefore, the shortest two disjoint paths under the modified weights are the same paths as the shortest two paths in the original graph, although they have different weights.Suurballe\\'s algorithm may be seen as a special case of the successive shortest paths method for finding a minimum cost flow with total flow amount two from s to t. The modification to the weights does not affect the sequence of paths found by this method, only their weights. Therefore, the correctness of the algorithm follows from the correctness of the successive shortest paths method.Analysis and running timeThis algorithm requires two iterations of Dijkstra\\'s algorithm. Using Fibonacci heaps, both iterations can be performed in timeO(|E|+|V|log\\u2061|V|){\\\\displaystyle O(|E|+|V|\\\\log |V|)}where|V|{\\\\displaystyle |V|}and|E|{\\\\displaystyle |E|}are the number of vertices and edges respectively. Therefore, the same time bound applies to Suurballe\\'s algorithm.VariationsThe version of Suurballe\\'s algorithm as described above finds paths that have disjoint edges, but that may share vertices. It is possible to use the same algorithm to find vertex-disjoint paths, by replacing each vertex by a pair of adjacent vertices, one with all of the incoming adjacencies  u-in of the original vertex, and one with all of the outgoing adjacencies  u-out. Two edge-disjoint paths in this modified graph necessarily correspond to two vertex-disjoint paths in the original graph, and vice versa, so applying Suurballe\\'s algorithm to the modified graph results in the construction of two vertex-disjoint paths in the original graph. Suurballe\\'s original 1974 algorithm was for the vertex-disjoint version of the problem, and was extended in 1984 by Suurballe and Tarjan to the edge-disjoint version.By using a modified version of Dijkstra\\'s algorithm that simultaneously computes the distances to each vertex t in the graphs Gt, it is also possible to find the total lengths of the shortest pairs of paths from a given source vertex s to every other vertex in the graph, in an amount of time that is proportional to a single instance of Dijkstra\\'s algorithm.Note: The pair of adjacent vertices resulting from the split are connected by a zero cost uni-directional edge from the incoming to outgoing vertex. The source vertex becomes s-out and the destination vertex becomes t-in.See alsoEdge disjoint shortest pair algorithm== References ==',\n",
       " '= Yen\\'s algorithm =Yen\\'s algorithm computes single-source K-shortest loopless paths for a graph with non-negative edge cost.  The algorithm was published by Jin Y. Yen in 1971 and employs any shortest path algorithm to find the best path, then proceeds to find K  1 deviations of the best path.AlgorithmTerminology and notationDescriptionThe algorithm can be broken down into two parts, determining the first k-shortest path,A1{\\\\displaystyle A^{1}}, and then determining all other k-shortest paths. It is assumed that the containerA{\\\\displaystyle A}will hold the k-shortest path, whereas the containerB{\\\\displaystyle B}, will hold the potential k-shortest paths. To determineA1{\\\\displaystyle A^{1}}, the shortest path from the source to the sink, any efficient shortest path algorithm can be used.To find theAk{\\\\displaystyle A^{k}}, wherek{\\\\displaystyle k}ranges from2{\\\\displaystyle 2}toK{\\\\displaystyle K}, the algorithm assumes that all paths fromA1{\\\\displaystyle A^{1}}toAk1{\\\\displaystyle A^{k-1}}have previously been found. Thek{\\\\displaystyle k}iteration can be divided into two processes, finding all the deviationsAki{\\\\displaystyle {A^{k}}_{i}}and choosing a minimum length path to becomeAk{\\\\displaystyle A^{k}}. Note that in this iteration,i{\\\\displaystyle i}ranges from1{\\\\displaystyle 1}toQkk{\\\\displaystyle {Q^{k}}_{k}}.The first process can be further subdivided into three operations, choosing theRki{\\\\displaystyle {R^{k}}_{i}}, findingSki{\\\\displaystyle {S^{k}}_{i}}, and then addingAki{\\\\displaystyle {A^{k}}_{i}}to the containerB{\\\\displaystyle B}. The root path,Rki{\\\\displaystyle {R^{k}}_{i}}, is chosen by finding the subpath inAk1{\\\\displaystyle A^{k-1}}that follows the firsti{\\\\displaystyle i}nodes ofAj{\\\\displaystyle A^{j}}, wherej{\\\\displaystyle j}ranges from1{\\\\displaystyle 1}tok1{\\\\displaystyle k-1}. Then, if a path is found, the cost of edgedi(i+1){\\\\displaystyle d_{i(i+1)}}ofAj{\\\\displaystyle A^{j}}is set to infinity. Next, the spur path,Ski{\\\\displaystyle {S^{k}}_{i}}, is found by computing the shortest path from the spur node, nodei{\\\\displaystyle i}, to the sink. The removal of previous used edges from(i){\\\\displaystyle (i)}to(i+1){\\\\displaystyle (i+1)}ensures that the spur path is different.Aki=Rki+Ski{\\\\displaystyle {A^{k}}_{i}={R^{k}}_{i}+{S^{k}}_{i}}, the addition of the root path and the spur path, is added toB{\\\\displaystyle B}. Next, the edges that were removed, i.e. had their cost set to infinity, are restored to their initial values.The second process determines a suitable path forAk{\\\\displaystyle A^{k}}by finding the path in containerB{\\\\displaystyle B}with the lowest cost. This path is removed from containerB{\\\\displaystyle B}and inserted into containerA{\\\\displaystyle A}and the algorithm continues to the next iteration.PseudocodeThe algorithm assumes that the Dijkstra algorithm is used to find the shortest path between two nodes, but any shortest path algorithm can be used in its place.function YenKSP(Graph, source, sink, K):// Determine the shortest path from the source to the sink.A[0] = Dijkstra(Graph, source, sink);// Initialize the set to store the potential kth shortest path.B = [];for k from 1 to K:// The spur node ranges from the first node to the next to last node in the previous k-shortest path.for i from 0 to size(A[k  1])  2:// Spur node is retrieved from the previous k-shortest path, k  1.spurNode = A[k-1].node(i);// The sequence of nodes from the source to the spur node of the previous k-shortest path.rootPath = A[k-1].nodes(0, i);for each path p in A:if rootPath == p.nodes(0, i):// Remove the links that are part of the previous shortest paths which share the same root path.remove p.edge(i,i + 1) from Graph;for each node rootPathNode in rootPath except spurNode:remove rootPathNode from Graph;// Calculate the spur path from the spur node to the sink.// Consider also checking if any spurPath foundspurPath = Dijkstra(Graph, spurNode, sink);// Entire path is made up of the root path and spur path.totalPath = rootPath + spurPath;// Add the potential k-shortest path to the heap.if (totalPath not in B):B.append(totalPath);// Add back the edges and nodes that were removed from the graph.restore edges to Graph;restore nodes in rootPath to Graph;if B is empty:// This handles the case of there being no spur paths, or no spur paths left.// This could happen if the spur paths have already been exhausted (added to A),// or there are no spur paths at all - such as when both the source and sink vertices// lie along a \"dead end\".break;// Sort the potential k-shortest paths by cost.B.sort();// Add the lowest cost path becomes the k-shortest path.A[k] = B[0];// In fact we should rather use shift since we are removing the first elementB.pop();return A;ExampleThe example uses Yen\\'s K-Shortest Path Algorithm to compute three paths from(C){\\\\displaystyle (C)}to(H){\\\\displaystyle (H)}. Dijkstra\\'s algorithm is used to calculate the best path from(C){\\\\displaystyle (C)}to(H){\\\\displaystyle (H)}, which is(C)(E)(F)(H){\\\\displaystyle (C)-(E)-(F)-(H)}with cost 5. This path is appended to containerA{\\\\displaystyle A}and becomes the first k-shortest path,A1{\\\\displaystyle A^{1}}.Node(C){\\\\displaystyle (C)}ofA1{\\\\displaystyle A^{1}}becomes the spur node with a root path of itself,R21=(C){\\\\displaystyle {R^{2}}_{1}=(C)}. The edge,(C)(E){\\\\displaystyle (C)-(E)}, is removed because it coincides with the root path and a path in containerA{\\\\displaystyle A}. Dijkstra\\'s algorithm is used to compute the spur pathS21{\\\\displaystyle {S^{2}}_{1}}, which is(C)(D)(F)(H){\\\\displaystyle (C)-(D)-(F)-(H)}, with a cost of 8.A21=R21+S21=(C)(D)(F)(H){\\\\displaystyle {A^{2}}_{1}={R^{2}}_{1}+{S^{2}}_{1}=(C)-(D)-(F)-(H)}is added to containerB{\\\\displaystyle B}as a potential k-shortest path.Node(E){\\\\displaystyle (E)}ofA1{\\\\displaystyle A^{1}}becomes the spur node withR22=(C)(E){\\\\displaystyle {R^{2}}_{2}=(C)-(E)}. The edge,(E)(F){\\\\displaystyle (E)-(F)}, is removed because it coincides with the root path and a path in containerA{\\\\displaystyle A}. Dijkstra\\'s algorithm is used to compute the spur pathS22{\\\\displaystyle {S^{2}}_{2}}, which is(E)(G)(H){\\\\displaystyle (E)-(G)-(H)}, with a cost of 7.A22=R22+S22=(C)(E)(G)(H){\\\\displaystyle {A^{2}}_{2}={R^{2}}_{2}+{S^{2}}_{2}=(C)-(E)-(G)-(H)}is added to containerB{\\\\displaystyle B}as a potential k-shortest path.Node(F){\\\\displaystyle (F)}ofA1{\\\\displaystyle A^{1}}becomes the spur node with a root path,R23=(C)(E)(F){\\\\displaystyle {R^{2}}_{3}=(C)-(E)-(F)}. The edge,(F)(H){\\\\displaystyle (F)-(H)}, is removed because it coincides with the root path and a path in containerA{\\\\displaystyle A}. Dijkstra\\'s algorithm is used to compute the spur pathS23{\\\\displaystyle {S^{2}}_{3}}, which is(F)(G)(H){\\\\displaystyle (F)-(G)-(H)}, with a cost of 8.A23=R23+S23=(C)(E)(F)(G)(H){\\\\displaystyle {A^{2}}_{3}={R^{2}}_{3}+{S^{2}}_{3}=(C)-(E)-(F)-(G)-(H)}is added to containerB{\\\\displaystyle B}as a potential k-shortest path.Of the three paths in container B,A22{\\\\displaystyle {A^{2}}_{2}}is chosen to becomeA2{\\\\displaystyle A^{2}}because it has the lowest cost of 7. This process is continued to the 3rd k-shortest path. However, within this 3rd iteration, note that some spur paths do not exist. And the path that is chosen to becomeA3{\\\\displaystyle A^{3}}is(C)(D)(F)(H){\\\\displaystyle (C)-(D)-(F)-(H)}.FeaturesSpace complexityTo store the edges of the graph, the shortest path listA{\\\\displaystyle A}, and the potential shortest path listB{\\\\displaystyle B},N2+KN{\\\\displaystyle N^{2}+KN}memory addresses are required. At worse case, the every node in the graph has an edge to every other node in the graph, thusN2{\\\\displaystyle N^{2}}addresses are needed. OnlyKN{\\\\displaystyle KN}addresses are need for both listA{\\\\displaystyle A}andB{\\\\displaystyle B}because at most onlyK{\\\\displaystyle K}paths will be stored, where it is possible for each path to haveN{\\\\displaystyle N}nodes.Time complexityThe time complexity of Yen\\'s algorithm is dependent on the shortest path algorithm used in the computation of the spur paths, so the Dijkstra algorithm is assumed. Dijkstra\\'s algorithm has a worse case time complexity ofO(N2){\\\\displaystyle O(N^{2})}, but using a Fibonacci heap it becomesO(M+Nlog\\u2061N){\\\\displaystyle O(M+N\\\\log N)}, whereM{\\\\displaystyle M}is the amount of edges in the graph. Since Yen\\'s algorithm makesKl{\\\\displaystyle Kl}calls to the Dijkstra in computing the spur paths, wherel{\\\\displaystyle l}is the length of spur paths. In a condensed graph, the expected value ofl{\\\\displaystyle l}isO(log\\u2061N){\\\\displaystyle O(\\\\log N)}, while the worst case isN{\\\\displaystyle N}., the time complexity becomesO(KN(M+Nlog\\u2061N)){\\\\displaystyle O(KN(M+N\\\\log N))}.ImprovementsYen\\'s algorithm can be improved by using a heap to storeB{\\\\displaystyle B}, the set of potential k-shortest paths. Using a heap instead of a list will improve the performance of the algorithm, but not the complexity.  One method to slightly decrease complexity is to skip the nodes where there are non-existent spur paths. This case is produced when all the spur paths from a spur node have been used in the previousAk{\\\\displaystyle A^{k}}. Also, if containerB{\\\\displaystyle B}hasKk{\\\\displaystyle K-k}paths of minimum length, in reference to those in containerA{\\\\displaystyle A}, then they can be extract and inserted into containerA{\\\\displaystyle A}since no shorter paths will be found.Lawler\\'s modificationEugene Lawler proposed a modification to Yen\\'s algorithm in which duplicates path are not calculated as opposed to the original algorithm where they are calculated and then discarded when they are found to be duplicates. These duplicates paths result from calculating spur paths of nodes in the root ofAk{\\\\displaystyle A^{k}}. For instance,Ak{\\\\displaystyle A^{k}}deviates fromAk1{\\\\displaystyle A^{k-1}}at some node(i){\\\\displaystyle (i)}. Any spur path,Skj{\\\\displaystyle {S^{k}}_{j}}wherej=0,,i{\\\\displaystyle j=0,\\\\ldots ,i}, that is calculated will be a duplicate because they have already been calculated during thek1{\\\\displaystyle k-1}iteration. Therefore, only spur paths for nodes that were on the spur path ofAk1{\\\\displaystyle A^{k-1}}must be calculated, i.e. onlySkh{\\\\displaystyle {S^{k}}_{h}}whereh{\\\\displaystyle h}ranges from(i+1)k1{\\\\displaystyle (i+1)^{k-1}}to(Qk)k1{\\\\displaystyle (Q_{k})^{k-1}}. To perform this operation forAk{\\\\displaystyle A^{k}}, a record is needed to identify the node whereAk1{\\\\displaystyle A^{k-1}}branched fromAk2{\\\\displaystyle A^{k-2}}.See alsoYen\\'s improvement to the BellmanFord algorithmReferencesExternal linksOpen Source Python Implementation on GitHubOpen Source C++ ImplementationOpen Source C++ Implementation using Boost Graph Library',\n",
       " '= Path-based strong component algorithm =In graph theory, the strongly connected components of a directed graph may be found using an algorithm that uses depth-first search in combination with two stacks, one to keep track of the vertices in the current component and the second to keep track of the current search path. Versions of this algorithm have been proposed by Purdom (1970), Munro (1971), Dijkstra (1976), Cheriyan & Mehlhorn (1996), and Gabow (2000); of these, Dijkstra\\'s version was the first to achieve linear time.DescriptionThe algorithm performs a depth-first search of the given graph G, maintaining as it does two stacks S and P (in addition to the normal call stack for a recursive function).Stack S contains all the vertices that have not yet been assigned to a strongly connected component, in the order in which the depth-first search reaches the vertices.Stack P contains vertices that have not yet been determined to belong to different strongly connected components from each other. It also uses a counter C of the number of vertices reached so far, which it uses to compute the preorder numbers of the vertices.When the depth-first search reaches a vertex v, the algorithm performs the following steps:Set the preorder number of v to C, and increment C.Push v onto S and also onto P.For each edge from v to a neighboring vertex w:If the preorder number of w has not yet been assigned (the edge is a tree edge), recursively search w;Otherwise, if w has not yet been assigned to a strongly connected component (the edge is a forward/back/cross edge):Repeatedly pop vertices from P until the top element of P has a preorder number less than or equal to the preorder number of w.If v is the top element of P:Pop vertices from S until v has been popped, and assign the popped vertices to a new component.Pop v from P.The overall algorithm consists of a loop through the vertices of the graph, calling this recursive search on each vertex that does not yet have a preorder number assigned to it.Related algorithmsLike this algorithm, Tarjan\\'s strongly connected components algorithm also uses depth first search together with a stack to keep track of vertices that have not yet been assigned to a component, and moves these vertices into a new component when it finishes expanding the final vertex of its component. However, in place of the stack P, Tarjan\\'s algorithm uses  a vertex-indexed array of preorder numbers, assigned in the order that vertices are first visited in the depth-first search. The preorder array is used to keep track of when to form a new component.NotesReferencesCheriyan, J.; Mehlhorn, K. (1996), \"Algorithms for dense graphs and networks on the random access computer\", Algorithmica, 15 (6): 521549, doi:10.1007/BF01940880, S2CID 8930091.Dijkstra, Edsger (1976), A Discipline of Programming, NJ: Prentice Hall, Ch. 25.Gabow, Harold N. (2000), \"Path-based depth-first search for strong and biconnected components\", Information Processing Letters, 74 (34): 107114, doi:10.1016/S0020-0190(00)00051-X, MR 1761551.Munro, Ian (1971), \"Efficient determination of the transitive closure of a directed graph\", Information Processing Letters, 1 (2): 5658, doi:10.1016/0020-0190(71)90006-8.Purdom, P., Jr. (1970), \"A transitive closure algorithm\", BIT, 10: 7694, doi:10.1007/bf01940892, S2CID 20818200.Sedgewick, R. (2004), \"19.8 Strong Components in Digraphs\", Algorithms in Java, Part 5  Graph Algorithms (3rd ed.), Cambridge MA: Addison-Wesley, pp. 205216.',\n",
       " '= A* search algorithm =A* (pronounced \"A-star\") is a graph traversal and path search algorithm, which is often used in many fields of computer science due to its completeness, optimality, and optimal efficiency. One major practical drawback is itsO(bd){\\\\displaystyle O(b^{d})}space complexity, as it stores all generated nodes in memory. Thus, in practical travel-routing systems, it is generally outperformed by algorithms which can pre-process the graph to attain better performance, as well as memory-bounded approaches; however, A* is still the best solution in many cases.Peter Hart, Nils Nilsson and Bertram Raphael of Stanford Research Institute (now SRI International) first published the algorithm in 1968. It can be seen as an extension of Dijkstra\\'s algorithm. A* achieves better performance by using heuristics to guide its search.HistoryA* was created as part of the Shakey project, which had the aim of building a mobile robot that could plan its own actions. Nils Nilsson originally proposed using the Graph Traverser algorithm for Shakey\\'s path planning. Graph Traverser is guided by a heuristic function h(n), the estimated distance from node n to the goal node: it entirely ignores g(n), the distance from the start node to n. Bertram Raphael suggested using the sum, g(n) + h(n). Peter Hart invented the concepts we now call admissibility and consistency of heuristic functions. A* was originally designed for finding least-cost paths when the cost of a path is the sum of its  costs, but it has been shown that A* can be used to find optimal paths for any problem satisfying the conditions of a cost algebra.The original 1968 A* paper contained a theorem stating that no A*-like algorithm could expand fewer nodes than A* if the heuristic function is consistent and A*\\'s tie-breaking rule is suitably chosen. A correction was published a few years later claiming that consistency was not required, but this was shown to be false in Dechter and Pearl\\'s definitive study of A*\\'s optimality (now called optimal efficiency), which gave an example of A* with a heuristic that was admissible but not consistent expanding arbitrarily more nodes than an alternative A*-like algorithm.DescriptionA* is an informed search algorithm, or a best-first search, meaning that it is formulated in terms of weighted graphs: starting from a specific starting node of a graph, it aims to find a path to the given goal node having the smallest cost (least distance travelled, shortest time, etc.).  It does this by maintaining a tree of paths originating at the start node and extending those paths one edge at a time until its termination criterion is satisfied.At each iteration of its main loop, A* needs to determine which of its paths to extend. It does so based on the cost of the path and an estimate of the cost required to extend the path all the way to the goal. Specifically, A* selects the path that minimizesf(n)=g(n)+h(n){\\\\displaystyle f(n)=g(n)+h(n)}where n is the next node on the path, g(n) is the cost of the path from the start node to n, and h(n) is a heuristic function that estimates the cost of the cheapest path from n to the goal. A* terminates when the path it chooses to extend is a path from start to goal or if there are no paths eligible to be extended. The heuristic function is problem-specific. If the heuristic function is admissible, meaning that it never overestimates the actual cost to get to the goal, A* is guaranteed to return a least-cost path from start to goal.Typical implementations of A* use a priority queue to perform the repeated selection of minimum (estimated) cost nodes to expand. This priority queue is known as the open set or fringe. At each step of the algorithm, the node with the lowest f(x) value is removed from the queue, the f and g values of its neighbors are updated accordingly, and these neighbors are added to the queue. The algorithm continues until a removed node (thus the node with the lowest f value out of all fringe nodes) is a goal node. The f value of that goal is then also the cost of the shortest path, since h at the goal is zero in an admissible heuristic.The algorithm described so far gives us only the length of the shortest path. To find the actual sequence of steps, the algorithm can be easily revised so that each node on the path keeps track of its predecessor. After this algorithm is run, the ending node will point to its predecessor, and so on, until some node\\'s predecessor is the start node.As an example, when searching for the shortest route on a map, h(x) might represent the straight-line distance to the goal, since that is physically the smallest possible distance between any two points. For a grid map from a video game, using the Manhattan distance or the octile distance becomes better depending on the set of movements available (4-way or 8-way).If the heuristic h satisfies the additional condition h(x)  d(x, y) + h(y) for every edge (x, y) of the graph (where d denotes the length of that edge), then h is called monotone, or consistent. With a consistent heuristic, A* is guaranteed to find an optimal path without processing any node more than once and A* is equivalent to running Dijkstra\\'s algorithm with the reduced cost d\\'(x, y) = d(x, y) + h(y)  h(x).PseudocodeThe following pseudocode describes the algorithm:Remark: In this pseudocode, if a node is reached by one path, removed from openSet, and subsequently reached by a cheaper path, it will be added to openSet again. This is essential to guarantee that the path returned is optimal if the heuristic function is admissible but not  consistent.   If the heuristic is consistent, when a node is removed from openSet the path to it is guaranteed to be optimal so the test tentative_gScore < gScore[neighbor] will always fail if the node is reached again.ExampleAn example of an A* algorithm in action where nodes are cities connected with roads and h(x) is the     straight-line distance to target point:Key: green: start; blue: goal; orange: visitedThe A* algorithm also has real-world applications. In this example, edges are railroads and h(x) is the great-circle distance (the shortest possible distance on a sphere) to the target. The algorithm is searching for a path between Washington, D.C. and Los Angeles.Implementation detailsThere are a number of simple optimizations or implementation details that can significantly affect the performance of an A* implementation.  The first detail to note is that the way the priority queue handles ties can have a significant effect on performance in some situations.  If ties are broken so the queue behaves in a LIFO manner, A* will behave like depth-first search among equal cost paths (avoiding exploring more than one equally optimal solution).When a path is required at the end of the search, it is common to keep with each node a reference to that node\\'s parent.  At the end of the search these references can be used to recover the optimal path.  If these references are being kept then it can be important that the same node doesn\\'t appear in the priority queue more than once (each entry corresponding to a different path to the node, and each with a different cost).  A standard approach here is to check if a node about to be added already appears in the priority queue.  If it does, then the priority and parent pointers are changed to correspond to the lower cost path. A standard binary heap based priority queue does not directly support the operation of searching for one of its elements, but it can be augmented with a hash table that maps elements to their position in the heap, allowing this decrease-priority operation to be performed in logarithmic time. Alternatively, a Fibonacci heap can perform the same decrease-priority operations in constant amortized time.Special casesDijkstra\\'s algorithm, as another example of a uniform-cost search algorithm, can be viewed as a special case of A* whereh(x)=0{\\\\displaystyle h(x)=0}for all x. General depth-first search can be implemented using A* by considering that there is a global counter C initialized with a very large value. Every time we process a node we assign C to all of its newly discovered neighbors. After each single assignment, we decrease the counter C by one. Thus the earlier a node is discovered, the higher itsh(x){\\\\displaystyle h(x)}value. Both Dijkstra\\'s algorithm and depth-first search can be implemented more efficiently without including anh(x){\\\\displaystyle h(x)}value at each node.PropertiesTermination and CompletenessOn finite graphs with non-negative edge weights A* is guaranteed to terminate and is complete, i.e. it will always find a solution (a path from start to goal) if one exists. On infinite graphs with a finite branching factor and edge costs that are bounded away from zero (d(x,y)>>0{\\\\textstyle d(x,y)>\\\\varepsilon >0}for some fixed{\\\\displaystyle \\\\varepsilon }), A* is guaranteed to terminate only if there exists a solution.AdmissibilityA search algorithm is said to be admissible if it is guaranteed to return an optimal solution. If the heuristic function used by A* is admissible, then A* is admissible. An intuitive proof of this  is as follows:When A* terminates its search, it has found a path from start to goal whose actual cost is lower than the estimated cost of any path from start to goal through any open node (the node\\'sf{\\\\displaystyle f}value). When the heuristic is admissible, those estimates are optimistic (not quitesee the next paragraph), so A* can safely ignore those nodes because they cannot possibly lead to a cheaper solution than the one it already has. In other words, A* will never overlook the possibility of a lower-cost path from start to goal and so it will continue to search until no such possibilities exist.The actual proof is a bit more involved because thef{\\\\displaystyle f}values of open nodes are not guaranteed to be optimistic even if the heuristic is admissible. This is because theg{\\\\displaystyle g}values of open nodes are not guaranteed to be optimal, so the sumg+h{\\\\displaystyle g+h}is not guaranteed to be optimistic.Optimality and ConsistencyAlgorithm A is optimally efficient with respect to a set of alternative algorithms Alts on a set of problems P if for every problem P in P and every algorithm A in Alts, the set of nodes expanded by A in solving P is a subset (possibly equal) of the set of nodes expanded by A in solving P. The definitive study of the optimal efficiency of A* is due to Rina Dechter and Judea Pearl.They considered a variety of  definitions of Alts and P  in combination with A*\\'s heuristic being merely admissible or being both consistent and admissible.  The most interesting positive result they proved is that A*, with a consistent heuristic, is optimally efficient with respect to all admissible A*-like search algorithms on all non-pathological search problems.  Roughly speaking, their notion of non-pathological problem is what we now mean by up to tie-breaking.  This result does not hold if A*\\'s heuristic is admissible but not consistent. In that case, Dechter and Pearl showed there exist admissible A*-like algorithms that can expand arbitrarily fewer nodes than A* on some non-pathological problems.Optimal efficiency is about the set of nodes expanded, not the number of node expansions (the number of iterations of A*\\'s main loop).  When the heuristic being used is admissible but not consistent, it is possible for a node to be expanded by A* many times, an exponential number of times in the worst case.In such circumstances Dijkstra\\'s algorithm could outperform A* by a large margin. However, more recent research found that this pathological case only occurs in certain contrived situations where the edge weight of the search graph is exponential in the size of the graph, and that certain inconsistent (but admissible) heuristics can lead to a reduced number of node expansions in A* searches.Bounded relaxationWhile the admissibility criterion guarantees an optimal solution path, it also means that A* must examine all equally meritorious paths to find the optimal path. To compute approximate shortest paths, it is possible to speed up the search at the expense of optimality by relaxing the admissibility criterion. Oftentimes we want to bound this relaxation, so that we can guarantee that the solution path is no worse than (1 + ) times the optimal solution path. This new guarantee is referred to as -admissible.There are a number of -admissible algorithms:Weighted A*/Static Weighting\\'s. If ha(n) is an admissible heuristic function, in the weighted version of the A* search one uses hw(n) =  ha(n),  > 1 as the heuristic function, and perform the A* search as usual (which eventually happens faster than using ha since fewer nodes are expanded). The path hence found by the search algorithm can have a cost of at most  times that of the least cost path in the graph.Dynamic Weighting uses the cost functionf(n)=g(n)+(1+w(n))h(n){\\\\displaystyle f(n)=g(n)+(1+\\\\varepsilon w(n))h(n)}, wherew(n)={1d(n)Nd(n)N0otherwise{\\\\displaystyle w(n)={\\\\begin{cases}1-{\\\\frac {d(n)}{N}}&d(n)\\\\leq N\\\\\\\\0&{\\\\text{otherwise}}\\\\end{cases}}}, and whered(n){\\\\displaystyle d(n)}is the depth of the search and N is the anticipated length of the solution path.Sampled Dynamic Weighting uses sampling of nodes to better estimate and debias the heuristic error.A{\\\\displaystyle A_{\\\\varepsilon }^{*}}. uses two heuristic functions. The first is the FOCAL list, which is used to select candidate nodes, and the second hF is used to select the most promising node from the FOCAL list.A selects nodes with the functionAf(n)+BhF(n){\\\\displaystyle Af(n)+Bh_{F}(n)}, where A and B are constants. If no nodes can be selected, the algorithm will backtrack with the functionCf(n)+DhF(n){\\\\displaystyle Cf(n)+Dh_{F}(n)}, where C and D are constants.AlphA* attempts to promote depth-first exploitation by preferring recently expanded nodes. AlphA* uses the cost functionf(n)=(1+w(n))f(n){\\\\displaystyle f_{\\\\alpha }(n)=(1+w_{\\\\alpha }(n))f(n)}, wherew(n)={g((n))g(n~)otherwise{\\\\displaystyle w_{\\\\alpha }(n)={\\\\begin{cases}\\\\lambda &g(\\\\pi (n))\\\\leq g({\\\\tilde {n}})\\\\\\\\\\\\Lambda &{\\\\text{otherwise}}\\\\end{cases}}}, where  and  are constants with{\\\\displaystyle \\\\lambda \\\\leq \\\\Lambda }, (n) is the parent of n, and  is the most recently expanded node.ComplexityThe time complexity of A* depends on the heuristic. In the worst case of an unbounded search space, the number of nodes expanded is exponential in the depth of the solution (the shortest path) d: O(bd), where b is the branching factor (the average number of successors per state). This assumes that a goal state exists at all, and is reachable from the start state; if it is not, and the state space is infinite, the algorithm will not terminate.The heuristic function has a major effect on the practical performance of A* search, since a good heuristic allows A* to prune away many of the bd nodes that an uninformed search would expand. Its quality can be expressed in terms of the effective branching factor b*, which can be determined empirically for a problem instance by measuring the number of nodes generated by expansion, N, and the depth of the solution, then solvingN+1=1+b+(b)2++(b)d.{\\\\displaystyle N+1=1+b^{*}+(b^{*})^{2}+\\\\dots +(b^{*})^{d}.}Good heuristics are those with low effective branching factor (the optimal being b* = 1).The time complexity is polynomial when the search space is a tree, there is a single goal state, and the heuristic function h meets the following condition:|h(x)h(x)|=O(log\\u2061h(x)){\\\\displaystyle |h(x)-h^{*}(x)|=O(\\\\log h^{*}(x))}where h* is the optimal heuristic, the exact cost to get from x to the goal. In other words, the error of h will not grow faster than the logarithm of the \"perfect heuristic\" h* that returns the true distance from x to the goal.The space complexity of A* is roughly the same as that of all other graph search algorithms, as it keeps all generated nodes in memory. In practice, this turns out to be the biggest drawback of A* search, leading to the development of memory-bounded heuristic searches, such as Iterative deepening A*, memory bounded A*, and SMA*.ApplicationsA* is often used for the common pathfinding problem in applications such as video games, but was originally designed as a general graph traversal algorithm.It finds applications in diverse problems, including the problem of parsing using stochastic grammars in NLP.Other cases include an Informational search with online learning.Relations to other algorithmsWhat sets A* apart from a greedy best-first search algorithm is that it takes the cost/distance already traveled, g(n), into account.Some common variants of Dijkstra\\'s algorithm can be viewed as a special case of A* where the heuristich(n)=0{\\\\displaystyle h(n)=0}for all nodes; in turn, both Dijkstra and A* are special cases of dynamic programming.A* itself is a special case of a generalization of branch and bound.VariantsAnytime A*Block A*D*Field D*FringeFringe Saving A* (FSA*)Generalized Adaptive A* (GAA*)Incremental heuristic searchReduced A*Iterative deepening A* (IDA*)Jump point searchLifelong Planning A* (LPA*)New Bidirectional A* (NBA*)Simplified Memory bounded A* (SMA*)Theta*A* can also be adapted to a bidirectional search algorithm. Special care needs to be taken for the stopping criterion.See alsoBreadth-first searchDepth-first searchAny-angle path planning, search for paths that are not limited to move along graph edges but rather can take on any angleNotesReferencesFurther readingNilsson, N. J. (1980). Principles of Artificial Intelligence. Palo Alto, California: Tioga Publishing Company. ISBN 978-0-935382-01-3.External linksClear visual A* explanation, with advice and thoughts on path-findingVariation on A* called Hierarchical Path-Finding A* (HPA*)Brian Grinstead. \"A* Search Algorithm in JavaScript (Updated)\". Archived from the original on 15 February 2020. Retrieved 8 February 2021.',\n",
       " '= BellmanFord algorithm =The BellmanFord algorithm is an algorithm that computes shortest paths from a single source vertex to all of the other vertices in a weighted digraph.It is slower than Dijkstra\\'s algorithm for the same problem, but more versatile, as it is capable of handling graphs in which some of the edge weights are negative numbers.The algorithm was first proposed by Alfonso Shimbel (1955), but is instead named after Richard Bellman and Lester Ford Jr., who published it in 1958 and 1956, respectively. Edward F. Moore also published a variation of the algorithm in 1959, and for this reason it is also sometimes called the BellmanFordMoore algorithm.Negative edge weights are found in various applications of graphs, hence the usefulness of this algorithm.If a graph contains a \"negative cycle\" (i.e. a cycle whose edges sum to a negative value) that is reachable from the source, then there is no cheapest path: any path that has a point on the negative cycle can be made cheaper by one more walk around the negative cycle. In such a case, the BellmanFord algorithm can detect and report the negative cycle.AlgorithmLike Dijkstra\\'s algorithm, BellmanFord proceeds by relaxation, in which approximations to the correct distance are replaced by better ones until they eventually reach the solution. In both algorithms, the approximate distance to each vertex is always an overestimate of the true distance, and is replaced by the minimum of its old value and the length of a newly found path.However, Dijkstra\\'s algorithm uses a priority queue to greedily select the closest vertex that has not yet been processed, and performs this relaxation process on all of its outgoing edges; by contrast, the BellmanFord algorithm simply relaxes all the edges, and does this|V|1{\\\\displaystyle |V|-1}times, where|V|{\\\\displaystyle |V|}is the number of vertices in the graph. In each of these repetitions, the number of vertices with correctly calculated distances grows, from which it follows that eventually all vertices will have their correct distances. This method allows the BellmanFord algorithm to be applied to a wider class of inputs than Dijkstra. The intermediate answers depend on the order of edges relaxed, but the final answer remains the same.BellmanFord runs inO(|V||E|){\\\\displaystyle O(|V|\\\\cdot |E|)}time, where|V|{\\\\displaystyle |V|}and|E|{\\\\displaystyle |E|}are the number of vertices and edges respectively.function BellmanFord(list vertices, list edges, vertex source) is// This implementation takes in a graph, represented as// lists of vertices (represented as integers [0..n-1]) and edges,// and fills two arrays (distance and predecessor) holding// the shortest path from the source to each vertexdistance := list of size npredecessor := list of size n// Step 1: initialize graphfor each vertex v in vertices dodistance[v] := inf             // Initialize the distance to all vertices to infinitypredecessor[v] := null         // And having a null predecessordistance[source] := 0              // The distance from the source to itself is, of course, zero// Step 2: relax edges repeatedlyrepeat |V|1 times:for each edge (u, v) with weight w in edges doif distance[u] + w < distance[v] thendistance[v] := distance[u] + wpredecessor[v] := u// Step 3: check for negative-weight cyclesfor each edge (u, v) with weight w in edges doif distance[u] + w < distance[v] thenerror \"Graph contains a negative-weight cycle\"return distance, predecessorSimply put, the algorithm initializes the distance to the source to 0 and all other nodes to infinity. Then for all edges, if the distance to the destination can be shortened by taking the edge, the distance is updated to the new lower value. At each iteration i that the edges are scanned, the algorithm finds all shortest paths of at most length i edges (and possibly some paths longer than i edges). Since the longest possible path without a cycle can be|V|1{\\\\displaystyle |V|-1}edges, the edges must be scanned|V|1{\\\\displaystyle |V|-1}times to ensure the shortest path has been found for all nodes. A final scan of all the edges is performed and if any distance is updated, then a path of length|V|{\\\\displaystyle |V|}edges has been found which can only occur if at least one negative cycle exists in the graph.Proof of correctnessThe correctness of the algorithm can be shown by induction:Lemma. After i repetitions of for loop,if Distance(u) is not infinity, it is equal to the length of some path from s to u; andif there is a path from s to u with at most i edges, then Distance(u) is at most the length of the shortest path from s to u with at most i edges.Proof. For the base case of induction, consider i=0 and the moment before for loop is executed for the first time. Then, for the source vertex, source.distance = 0, which is correct. For other vertices u, u.distance = infinity, which is also correct because there is no path from source to u with 0 edges.For the inductive case, we first prove the first part. Consider a moment when a vertex\\'s distance is updated byv.distance := u.distance + uv.weight. By inductive assumption, u.distance is the length of some path from source to u. Then u.distance + uv.weight is the length of the path from source to v that follows the path from  source to u and then goes to v.For the second part, consider a shortest path P (there may be more than one) from source to v with at most i edges. Let u be the last vertex before v on this path. Then, the part of the path from source to u is a shortest path from source to u with at most i-1 edges, since if it were not, then there must be some strictly shorter path from source to u with at most i-1 edges, and we could then append the edge uv to this path to obtain a path with at most i edges that is strictly shorter than Pa contradiction. By inductive assumption, u.distance after i1 iterations is at most the length of this path from source to u. Therefore, uv.weight + u.distance is at most the length of P. In the ith iteration, v.distance gets compared with uv.weight + u.distance, and is set equal to it if uv.weight + u.distance is smaller. Therefore, after i iterations, v.distance is at most the length of P, i.e., the length of the shortest path from source to v that uses at most i edges.If there are no negative-weight cycles, then every shortest path visits each vertex at most once, so at step 3 no further improvements can be made. Conversely, suppose no improvement can be made. Then for any cycle with vertices v[0], ..., v[k1],v[i].distance <= v[i-1 (mod k)].distance + v[i-1 (mod k)]v[i].weightSumming around the cycle, the v[i].distance and v[i1 (mod k)].distance terms cancel, leaving0 <= sum from 1 to k of v[i-1 (mod k)]v[i].weightI.e., every cycle has nonnegative weight.Finding negative cyclesWhen the algorithm is used to find shortest paths, the existence of negative cycles is a problem, preventing the algorithm from finding a correct answer. However, since it terminates upon finding a negative cycle, the BellmanFord algorithm can be used for applications in which this is the target to be sought  for example in cycle-cancelling techniques in network flow analysis.Applications in routingA distributed variant of the BellmanFord algorithm is used in distance-vector routing protocols, for example the Routing Information Protocol (RIP). The algorithm is distributed because it involves a number of nodes (routers) within an Autonomous system (AS), a collection of IP networks typically owned by an ISP.It consists of the following steps:Each node calculates the distances between itself and all other nodes within the AS and stores this information as a table.Each node sends its table to all neighboring nodes.When a node receives distance tables from its neighbors, it calculates the shortest routes to all other nodes and updates its own table to reflect any changes.The main disadvantages of the BellmanFord algorithm in this setting are as follows:It does not scale well.Changes in network topology are not reflected quickly since updates are spread node-by-node.Count to infinity if link or node failures render a node unreachable from some set of other nodes, those nodes may spend forever gradually increasing their estimates of the distance to it, and in the meantime there may be routing loops.ImprovementsThe BellmanFord algorithm may be improved in practice (although not in the worst case) by the observation that, if an iteration of the main loop of the algorithm terminates without making any changes, the algorithm can be immediately terminated, as subsequent iterations will not make any more changes. With this early termination condition, the main loop may in some cases use many fewer than |V|  1 iterations, even though the worst case of the algorithm remains unchanged. The following improvements all maintain theO(|V||E|){\\\\displaystyle O(|V|\\\\cdot |E|)}worst-case time complexity.A variation of the Bellman-Ford algorithm known as Shortest Path Faster Algorithm, first described by Moore (1959), reduces the number of relaxation steps that need to be performed within each iteration of the algorithm. If a vertex v has a distance value that has not changed since the last time the edges out of v were relaxed, then there is no need to relax the edges out of v a second time. In this way, as the number of vertices with correct distance values grows, the number whose outgoing edges that need to be relaxed in each iteration shrinks, leading to a constant-factor savings in time for dense graphs.Yen (1970) described another improvement to the BellmanFord algorithm. His improvement first assigns some arbitrary linear order on all vertices and then partitions the set of all edges into two subsets. The first subset, Ef, contains all edges (vi, vj) such that i < j; the second, Eb, contains edges (vi, vj) such that i > j. Each vertex is visited in the order v1, v2, ..., v|V|, relaxing each outgoing edge from that vertex in Ef. Each vertex is then visited in the order v|V|, v|V|1, ..., v1, relaxing each outgoing edge from that vertex in Eb. Each iteration of the main loop of the algorithm, after the first one, adds at least two edges to the set of edges whose relaxed distances match the correct shortest path distances: one from Ef and one from Eb. This modification reduces the worst-case number of iterations of the main loop of the algorithm from |V|  1 to|V|/2{\\\\displaystyle |V|/2}.Another improvement, by Bannister & Eppstein (2012), replaces the arbitrary linear order of the vertices used in Yen\\'s second improvement by a random permutation. This change makes the worst case for Yen\\'s improvement (in which the edges of a shortest path strictly alternate between the two subsets Ef and Eb) very unlikely to happen. With a randomly permuted vertex ordering, the expected number of iterations needed in the main loop is at most|V|/3{\\\\displaystyle |V|/3}.NotesReferencesOriginal sourcesShimbel, A. (1955). Structure in communication nets. Proceedings of the Symposium on Information Networks. New York, New York: Polytechnic Press of the Polytechnic Institute of Brooklyn. pp. 199203.Bellman, Richard (1958). \"On a routing problem\". Quarterly of Applied Mathematics. 16: 8790. doi:10.1090/qam/102435. MR 0102435.Ford, Lester R. Jr. (August 14, 1956). Network Flow Theory. Paper P-923. Santa Monica, California: RAND Corporation.Moore, Edward F. (1959). The shortest path through a maze. Proc. Internat. Sympos. Switching Theory 1957, Part II. Cambridge, Massachusetts: Harvard Univ. Press. pp. 285292. MR 0114710.Yen, Jin Y. (1970). \"An algorithm for finding shortest routes from all source nodes to a given destination in general networks\". Quarterly of Applied Mathematics. 27 (4): 526530. doi:10.1090/qam/253822. MR 0253822.Bannister, M. J.; Eppstein, D. (2012). Randomized speedup of the BellmanFord algorithm. Analytic Algorithmics and Combinatorics (ANALCO12), Kyoto, Japan. pp. 4147. arXiv:1111.5414. doi:10.1137/1.9781611973020.6.Secondary sourcesBang-Jensen, Jrgen; Gutin, Gregory (2000). \"Section 2.3.4: The Bellman-Ford-Moore algorithm\". Digraphs: Theory, Algorithms and Applications (First ed.). ISBN 978-1-84800-997-4.Schrijver, Alexander (2005). \"On the history of combinatorial optimization (till 1960)\" (PDF). Handbook of Discrete Optimization. Elsevier: 168.Cormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L. Introduction to Algorithms. MIT Press and McGraw-Hill., Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7. Section 24.1: The BellmanFord algorithm, pp. 588592. Problem 24-1, pp. 614615. Third Edition. MIT Press, 2009. ISBN 978-0-262-53305-8. Section 24.1: The BellmanFord algorithm, pp. 651655.Heineman, George T.; Pollice, Gary; Selkow, Stanley (2008). \"Chapter 6: Graph Algorithms\". Algorithms in a Nutshell. O\\'Reilly Media. pp. 160164. ISBN 978-0-596-51624-6.Kleinberg, Jon; Tardos, va (2006). Algorithm Design. New York: Pearson Education, Inc.Sedgewick, Robert (2002). \"Section 21.7: Negative Edge Weights\". Algorithms in Java (3rd ed.). ISBN 0-201-36121-3. Archived from the original on 2008-05-31. Retrieved 2007-05-28.',\n",
       " '= K shortest path routing =The k shortest path routing problem is a generalization of the shortest path routing problem in a given network. It asks not only about a shortest path but also about next k1 shortest paths (which may be longer than the shortest path). A variation of the problem is the loopless k shortest paths.Finding k shortest paths is possible by extending Dijkstra algorithm or Bellman-Ford algorithm and extend them to find more than one path.HistorySince 1957 many papers were published on the k shortest path routing problem. Most of the fundamental works were done between 1960s and 2001. Since then, most of the research has been on the problem\\'s applications and its variants. In 2010, Michael Gnther et al. published a book on Symbolic calculation of k-shortest paths and related measures with the stochastic process algebra tool CASPA.AlgorithmThe Dijkstra algorithm can be generalized to find the k shortest paths.VariationsThere are two main variations of the k shortest path routing problem.  In one variation, paths are allowed to visit the same node more than once, thus creating loops.  In another variation, paths are required to be simple and loopless.  The loopy version is solvable using Eppstein\\'s algorithm and the loopless variation is solvable by Yen\\'s algorithm.Loopy variantIn this variant, the problem is simplified by not requiring paths to be loopless.  A solution was given by B. L. Fox in 1975 in which the k-shortest paths are determined in O(m + kn log n) asymptotic time complexity (using big O notation.  In 1998, David Eppstein reported an approach that maintains an asymptotic complexity of O(m + n log n + k) by computing an implicit representation of the paths, each of which can be output in O(n) extra time. In 2015, Akuba et al. devised an indexing method as a significantly faster alternative for Eppstein\\'s algorithm, in which a data structure called an index is constructed from a graph and then top-k distances between arbitrary pairs of vertices can be rapidly obtained.Loopless variantIn the loopless variant, the paths are forbidden to contain loops which adds an additional level of complexity. It can be solved using Yen\\'s algorithm to find the lengths of all shortest paths from a fixed node to all other nodes in an n-node non negative-distance network, a technique requiring only 2n2 additions and n2 comparison, fewer than other available shortest path algorithms need.  The running time complexity is pseudo-polynomial, being O(kn(m + n log n)) (where m and n represent the number of edges and vertices, respectively). In 2007, John Hershberger and Subhash Suri proposed a replacement paths algorithm, a more efficient implementation of Lawler\\'s  and Yen\\'s algorithm with O(n) improvement in time.Some examples and descriptionExample #1The following example makes use of Yens model to find k shortest paths between communicating end nodes. That is, it finds a shortest path, second shortest path, etc. up to the Kth shortest path. More details can be found here.The code provided in this example attempts to solve the k shortest path routing problem for a 15-nodes network containing a combination of unidirectional and bidirectional links:Example #2Another example is the use of k shortest paths algorithm to track multiple objects. The technique implements a multiple object tracker based on the k shortest paths routing algorithm. A set of probabilistic occupancy maps is used as input. An object detector provides the input.The complete details can be found at \"Computer Vision Laboratory  CVLAB\".Example #3Another use of k shortest paths algorithms is to design a transit network that enhances passengers\\' experience in public transportation systems. Such an example of a transit network can be constructed by putting traveling time under consideration. In addition to traveling time, other conditions may be taken depending upon economical and geographical limitations. Despite variations in parameters, the k shortest path algorithms finds the most optimal solutions that satisfies almost all user needs. Such applications of k shortest path algorithms are becoming common, recently Xu, He, Song, and Chaudry (2012) studied the k shortest path problems in transit network systems.ApplicationsThe k shortest path routing is a good alternative for:Geographic path planningNetwork routing, especially in optical mesh network where there are additional constraints that cannot be solved by using ordinary shortest path algorithms.Hypothesis generation in computational linguisticsSequence alignment and metabolic pathway finding in bioinformaticsMultiple object tracking as described aboveRoad Networks: road junctions are the nodes (vertices) and each  edge (link) of the graph is associated with a road segment between two junctions.Related problemsThe breadth-first search algorithm is used when the search is only limited to two operations.The FloydWarshall algorithm solves all pairs shortest paths.Johnson\\'s algorithm solves all pairs\\' shortest paths, and may be faster than FloydWarshall on sparse graphs.Perturbation theory finds (at worst) the locally shortest path.Cherkassky et al. provide more algorithms and associated evaluations.See alsoConstrained shortest path routingNotesExternal linksImplementation of Yen\\'s algorithmImplementation of Yen\\'s and fastest k shortest simple paths algorithmshttp://www.technical-recipes.com/2012/the-k-shortest-paths-algorithm-in-c/#more-2432Multiple objects tracking technique using K-shortest path algorithm: http://cvlab.epfl.ch/software/ksp/Computer Vision Laboratory: http://cvlab.epfl.ch/software/ksp/',\n",
       " \"= Widest path problem =In graph algorithms, the widest path problem is the problem of finding a path between two designated vertices in a weighted graph, maximizing the weight of the minimum-weight edge in the path. The widest path problem is also known as the maximum capacity path problem. It is possible to adapt most shortest path algorithms to compute widest paths, by modifying them to use the bottleneck distance instead of path length. However, in many cases even faster algorithms are possible.For instance, in a graph that represents connections between routers in the Internet, where the weight of an edge represents the bandwidth of a connection between two routers, the widest path problem is the problem of finding an end-to-end path between two Internet nodes that has the maximum possible bandwidth.  The smallest edge weight on this path is known as the capacity or bandwidth of the path. As well as its applications in network routing, the widest path problem is also an important component of the Schulze method for deciding the winner of a multiway election, and has been applied to digital compositing, metabolic pathway analysis, and the computation of maximum flows.A closely related problem, the minimax path problem or bottleneck shortest path problem asks for the path that minimizes the maximum weight of any of its edges. It has applications that include transportation planning. Any algorithm for the widest path problem can be transformed into an algorithm for the minimax path problem, or vice versa, by reversing the sense of all the weight comparisons performed by the algorithm, or equivalently by replacing every edge weight by its negation.Undirected graphsIn an undirected graph, a widest path may be found as the path between the two vertices in the maximum spanning tree of the graph, and a minimax path may be found as the path between the two vertices in the minimum spanning tree.In any graph, directed or undirected, there is a straightforward algorithm for finding a widest path once the weight of its minimum-weight edge is known: simply delete all smaller edges and search for any path among the remaining edges using breadth first search or depth first search. Based on this test, there also exists a linear time algorithm for finding a widest s-t path in an undirected graph, that does not use the maximum spanning tree. The main idea of the algorithm is to apply the linear-time path-finding algorithm to the median edge weight in the graph, and then either to delete all smaller edges or contract all larger edges according to whether a path does or does not exist, and recurse in the resulting smaller graph.Fernndez, Garfinkel & Arbiol (1998) use undirected bottleneck shortest paths in order to form composite aerial photographs that combine multiple images of overlapping areas. In the subproblem to which the widest path problem applies, two images have already been transformed into a common coordinate system; the remaining task is to select a seam, a curve that passes through the region of overlap and divides one of the two images from the other. Pixels on one side of the seam will be copied from one of the images, and pixels on the other side of the seam will be copied from the other image. Unlike other compositing methods that average pixels from both images, this produces a valid photographic image of every part of the region being photographed. They weight the edges of a grid graph by a numeric estimate of how visually apparent a seam across that edge would be, and find a bottleneck shortest path for these weights. Using this path as the seam, rather than a more conventional shortest path, causes their system to find a seam that is difficult to discern at all of its points, rather than allowing it to trade off greater visibility in one part of the image for lesser visibility elsewhere.A solution to the minimax path problem between the two opposite corners of a grid graph can be used to find the weak Frchet distance between two polygonal chains. Here, each grid graph vertex represents a pair of line segments, one from each chain, and the weight of an edge represents the Frchet distance needed to pass from one pair of segments to another.If all edge weights of an undirected graph are positive, then the minimax distances between pairs of points (the maximum edge weights of minimax paths) form an ultrametric; conversely every finite ultrametric space comes from minimax distances in this way. A data structure constructed from the minimum spanning tree allows the minimax distance between any pair of vertices to be queried in constant time per query, using lowest common ancestor queries in a Cartesian tree. The root of the Cartesian tree represents the heaviest minimum spanning tree edge, and the children of the root are Cartesian trees recursively constructed from the subtrees of the minimum spanning tree formed by removing the heaviest edge. The leaves of the Cartesian tree represent the vertices of the input graph, and the minimax distance between two vertices equals the weight of the Cartesian tree node that is their lowest common ancestor. Once the minimum spanning tree edges have been sorted, this Cartesian tree can be constructed in linear time.Directed graphsIn directed graphs, the maximum spanning tree solution cannot be used. Instead, several different algorithms are known; the choice of which algorithm to use depends on whether a start or destination vertex for the path is fixed, or whether paths for many start or destination vertices must be found simultaneously.All pairsThe all-pairs widest path problem has applications in the Schulze method for choosing a winner in multiway elections in which voters rank the candidates in preference order. The Schulze method constructs a complete directed graph in which the vertices represent the candidates and every two vertices are connected by an edge. Each edge is directed from the winner to the loser of a pairwise contest between the two candidates it connects, and is labeled with the margin of victory of that contest. Then the method computes widest paths between all pairs of vertices, and the winner is the candidate whose vertex has wider paths to each opponent than vice versa. The results of an election using this method are consistent with the Condorcet method  a candidate who wins all pairwise contests automatically wins the whole election  but it generally allows a winner to be selected, even in situations where the Concorcet method itself fails. The Schulze method has been used by several organizations including the Wikimedia Foundation.To compute the widest path widths for all pairs of nodes in a dense directed graph, such as the ones that arise in the voting application, the asymptotically fastest known approach takes time O(n(3+)/2) where  is the exponent for fast matrix multiplication. Using the best known algorithms for matrix multiplication, this time bound becomes  O(n2.688). Instead, the reference implementation for the Schulze method uses a modified version of the simpler FloydWarshall algorithm, which takes O(n3) time. For sparse graphs, it may be more efficient to repeatedly apply a single-source widest path algorithm.Single sourceIf the edges are sorted by their weights, then a modified version of Dijkstra's algorithm can compute the bottlenecks between a designated start vertex and every other vertex in the graph, in linear time. The key idea behind the speedup over a conventional version of Dijkstra's algorithm is that the sequence of bottleneck distances to each vertex, in the order that the vertices are considered by this algorithm, is a monotonic subsequence of the sorted sequence of edge weights; therefore, the priority queue of Dijkstra's algorithm can be implemented as a bucket queue: an array indexed by the numbers from 1 to m (the number of edges in the graph), where array cell i contains the vertices whose bottleneck distance is the weight of the edge with position i in the sorted order. This method allows the widest path problem to be solved as quickly as sorting; for instance, if the edge weights are represented as integers, then the time bounds for integer sorting a list of m integers would apply also to this problem.Single source and single destinationBerman & Handler (1987) suggest that service vehicles and emergency vehicles should use minimax paths when returning from a service call to their base. In this application, the time to return is less important than the response time if another service call occurs while the vehicle is in the process of returning. By using a minimax path, where the weight of an edge is the maximum travel time from a point on the edge to the farthest possible service call, one can plan a route that minimizes the maximum possible delay between receipt of a service call and arrival of a responding vehicle. Ullah, Lee & Hassoun (2009) use maximin paths to model the dominant reaction chains in metabolic networks; in their model, the weight of an edge is the free energy of the metabolic reaction represented by the edge.Another application of widest paths arises in the FordFulkerson algorithm for the maximum flow problem. Repeatedly augmenting a flow along a maximum capacity path in the residual network of the flow leads to a small bound, O(m log U), on the number of augmentations needed to find a maximum flow; here, the edge capacities are assumed to be integers that are at most U. However, this analysis does not depend on finding a path that has the exact maximum of capacity; any path whose capacity is within a constant factor of the maximum suffices. Combining this approximation idea with the shortest path augmentation method of the EdmondsKarp algorithm leads to a maximum flow algorithm with running time O(mn log U).It is possible to find maximum-capacity paths and minimax paths with a single source and single destination very efficiently even in models of computation that allow only comparisons of the input graph's edge weights and not arithmetic on them. The algorithm maintains a set S of edges that are known to contain the bottleneck edge of the optimal path; initially, S is just the set of all m edges of the graph. At each iteration of the algorithm, it splits S into an ordered sequence of subsets S1, S2, ... of approximately equal size; the number of subsets in this partition is chosen in such a way that all of the split points between subsets can be found by repeated median-finding in time O(m). The algorithm then reweights each edge of the graph by the index of the subset containing the edge, and uses the modified Dijkstra algorithm on the reweighted graph; based on the results of this computation, it can determine in linear time which of the subsets contains the bottleneck edge weight. It then replaces S by the subset Si that it has determined to contain the bottleneck weight, and starts the next iteration with this new set S. The number of subsets into which S can be split increases exponentially with each step, so the number of iterations is proportional to the iterated logarithm function, O(log*n), and the total time is O(m log*n). In a model of computation where each edge weight is a machine integer, the use of repeated bisection in this algorithm can be replaced by a list-splitting technique of Han & Thorup (2002), allowing S to be split into O(m) smaller sets Si in a single step and leading to a linear overall time bound.Euclidean point setsA variant of the minimax path problem has also been considered for sets of points in the Euclidean plane. As in the undirected graph problem, this Euclidean minimax path problem can be solved efficiently by finding a Euclidean minimum spanning tree: every path in the tree is a minimax path. However, the problem becomes more complicated when a path is desired that not only minimizes the hop length but also, among paths with the same hop length, minimizes or approximately minimizes the total length of the path. The solution can be approximated using geometric spanners.In number theory, the unsolved Gaussian moat problem asks whether or not minimax paths in the Gaussian prime numbers have bounded or unbounded minimax length. That is, does there exist a constant B such that, for every pair of points p and q in the infinite Euclidean point set defined by the Gaussian primes, the minimax path in the Gaussian primes between p and q has minimax edge length at most B?== References ==\"]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def do_search(text):\n",
    "    new_vector = vec.transform([text]).todense()\n",
    "    texts_tmp = [[cosine(vector, new_vector), document] for document, vector in texts]\n",
    "    texts_tmp.sort()\n",
    "    return [document for vector, document in texts_tmp[:10]]\n",
    "\n",
    "do_search('Dijkstra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZsX0XY_pnXOx",
    "outputId": "f7511a26-00a2-42ea-febd-8e6bdff516bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trie\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['= Burstsort =Burstsort and its variants are cache-efficient algorithms for sorting strings.  They are variants of the traditional radix sort but faster for large data sets of common strings, first published in 2003, with some optimizing versions published in later years.Burstsort algorithms use a trie to store prefixes of strings, with growable arrays of pointers as end nodes containing sorted, unique, suffixes (referred to as buckets). Some variants copy the string tails into the buckets. As the buckets grow beyond a predetermined threshold, the buckets are \"burst\" into tries, giving the sort its name. A more recent variant uses a bucket index with smaller sub-buckets to reduce memory usage. Most implementations delegate to multikey quicksort, an extension of three-way radix quicksort, to sort the contents of the buckets. By dividing the input into buckets with common prefixes, the sorting can be done in a cache-efficient manner.Burstsort was introduced as a sort that is similar to MSD radix sort, but is faster due to being aware of caching and related radixes being stored closer to each other due to specifics of trie structure. It exploits specifics of strings that are usually encountered in real world. And although asymptotically it is the same as radix sort, with time complexity of O(wn) (w  word length and n  number of strings to be sorted), but due to better memory distribution it tends to be twice as fast on big data sets of strings.  It has been billed as the \"fastest known algorithm to sort large sets of strings\".ReferencesA burstsort derivative (C-burstsort), faster than burstsort: Sinha, Ranjan; Zobel, Justin; Ring, David (January 2006). \"Cache-Efficient String Sorting Using Copying\" (PDF). Journal of Experimental Algorithmics. 11 (1.2): 1.2. CiteSeerX 10.1.1.85.3498. doi:10.1145/1187436.1187439. S2CID 3184411. Archived from the original (PDF) on 2007-10-01. Retrieved 2007-05-31.The data type used in burstsort: Heinz, Steffen; Zobel, Justin; Williams, Hugh E. (April 2002). \"Burst Tries: A Fast, Efficient Data Structure for String Keys\" (PDF). ACM Transactions on Information Systems. 20 (2): 192223. CiteSeerX 10.1.1.18.3499. doi:10.1145/506309.506312. S2CID 14122377. Archived from the original (PDF) on 2013-12-05. Retrieved 2007-09-25.Sinha, Ranjan; Zobel, Justin (2003). \"Efficient Trie-Based Sorting of Large Sets of Strings\" (PDF). Proceedings of the 26th Australasian Computer Science Conference. Vol. 16. pp. 1118. CiteSeerX 10.1.1.12.2757. ISBN 978-0-909-92594-9. Archived from the original (PDF) on 2012-02-08. Retrieved 2007-09-25.Sinha, Ranjan; Wirth, Anthony (March 2010). \"Engineering Burstsort: Towards Fast In-Place String Sorting\" (PDF). ACM Journal of Experimental Algorithmics. 15 (2.5): 124. doi:10.1145/1671970.1671978. S2CID 16410080.External linksA burstsort implementation in Java: burstsort4jJudy arrays are a type of copy burstsort: C implementation',\n",
       " '= AhoCorasick algorithm =In computer science, the AhoCorasick algorithm is a string-searching algorithm invented by Alfred V. Aho and Margaret J. Corasick in 1975. It is a kind of dictionary-matching algorithm that locates elements of a finite set of strings (the \"dictionary\") within an input text. It matches all strings simultaneously. The complexity of the algorithm is linear in the length of the strings plus the length of the searched text plus the number of output matches. Note that because all matches are found, there can be a quadratic number of matches if every substring matches (e.g. dictionary = a, aa, aaa, aaaa and input string is aaaa).Informally, the algorithm constructs a finite-state machine that resembles a trie with additional links between the various internal nodes.  These extra internal links allow fast transitions between failed string matches (e.g. a search for cat in a trie that does not contain cat, but contains cart, and thus would fail at the node prefixed by ca), to other branches of the trie that share a common prefix (e.g., in the previous case, a branch for attribute might be the best lateral transition).  This allows the automaton to transition between string matches without the need for backtracking.When the string dictionary is known in advance (e.g. a computer virus database), the construction of the automaton can be performed once off-line and the compiled automaton stored for later use.  In this case, its run time is linear in the length of the input plus the number of matched entries.The AhoCorasick string-matching algorithm formed the basis of the original Unix command fgrep.ExampleIn this example, we will consider a dictionary consisting of the following words: {a, ab, bab, bc, bca, c, caa}.The graph below is the AhoCorasick data structure constructed from the specified dictionary, with each row in the table representing a node in the trie, with the column path indicating the (unique) sequence of characters from the root to the node.The data structure has one node for every prefix of every string in the dictionary.  So if (bca) is in the dictionary, then there will be nodes for (bca), (bc), (b), and (). If a node is in the dictionary then it is a blue node. Otherwise it is a grey node.There is a black directed \"child\" arc from each node to a node whose name is found by appending one character.  So there is a black arc from (bc) to (bca).There is a blue directed \"suffix\" arc from each node to the node that is the longest possible strict suffix of it in the graph.  For example, for node (caa), its strict suffixes are (aa) and (a) and ().  The longest of these that exists in the graph is (a).  So there is a blue arc from (caa) to (a).  The blue arcs can be computed in linear time by performing a breadth-first search [potential suffix node will always be at lower level] starting from the root. The target for the blue arc of a visited node can be found by following its parent\\'s blue arc to its longest suffix node and searching for a child of the suffix node whose character matches that of the visited node. If the character does not exist as a child, we can find the next longest suffix (following the blue arc again) and then search for the character. We can do this until we either find the character (as child of a node) or we reach the root (which will always be a suffix of every string).There is a green \"dictionary suffix\" arc from each node to the next node in the dictionary that can be reached by following blue arcs.  For example, there is a green arc from (bca) to (a) because (a) is the first node in the dictionary (i.e. a blue node) that is reached when following the blue arcs to (ca) and then on to (a).  The green arcs can be computed in linear time by repeatedly traversing blue arcs until a blue node is found, and memoizing this information.At each step, the current node is extended by finding its child,and if that doesn\\'t exist, finding its suffix\\'s child, and ifthat doesn\\'t work, finding its suffix\\'s suffix\\'s child, and so on, finallyending in the root node if nothing\\'s seen before.When the algorithm reaches a node, it outputs all the dictionaryentries that end at the current character position in the input text.  This is doneby printing every node reached by following the dictionary suffix links, startingfrom that node, and continuing until it reaches a node with no dictionary suffix link.In addition, the node itself is printed, if it is a dictionary entry.Execution on input string abccab yields the following steps:Dynamic search listThe original Aho-Corasick algorithm assumes that the set of search strings is fixed. It does not directly apply to applications in which new search strings are added during application of the algorithm. An example is an interactive indexing program, in which the user goes through the text and highlights new words or phrases to index as he or she sees them. Bertrand Meyer introduced an incremental version of the algorithm in which the search string set can be incrementally extended during the search, retaining the algorithmic complexity of the original.See alsoCommentz-Walter algorithmReferencesExternal linksAho-Corasick in NIST\\'s Dictionary of Algorithms and Data Structures (2019-07-15)Aho-Corasick visualisationAho-Corasick Implementation (C++) on GitHubA Golang implementation of the Aho-Corasick string matching algorithm on GitHubA fast implementation of Aho-Corasick in Rust on GitHub',\n",
       " '= Suffix tree =In computer science, a suffix tree (also called PAT tree or, in an earlier form, position tree) is a compressed trie containing all the suffixes of the given text as their keys and positions in the text as their values. Suffix trees allow particularly fast implementations of many important string operations.The construction of such a tree for the stringS{\\\\displaystyle S}takes time and space linear in the length ofS{\\\\displaystyle S}. Once constructed, several operations can be performed quickly, for instance locating a substring inS{\\\\displaystyle S}, locating a substring if a certain number of mistakes are allowed, locating matches for a regular expression pattern etc. Suffix trees also provide one of the first linear-time solutions for the longest common substring problem. These speedups come at a cost: storing a string\\'s suffix tree typically requires significantly more space than storing the string itself.DefinitionThe suffix tree for the stringS{\\\\displaystyle S}of lengthn{\\\\displaystyle n}is defined as a tree such that:The tree has exactly n leaves numbered from1{\\\\displaystyle 1}ton{\\\\displaystyle n}.Except for the root, every internal node has at least two children.Each edge is labelled with a non-empty substring ofS{\\\\displaystyle S}.No two edges starting out of a node can have string-labels beginning with the same character.The string obtained by concatenating all the string-labels found on the path from the root to leafi{\\\\displaystyle i}spells out suffixS[i..n]{\\\\displaystyle S[i..n]}, fori{\\\\displaystyle i}from1{\\\\displaystyle 1}ton{\\\\displaystyle n}.Since such a tree does not exist for all strings,S{\\\\displaystyle S}is padded with a terminal symbol not seen in the string (usually denoted $). This ensures that no suffix is a prefix of another, and that there will ben{\\\\displaystyle n}leaf nodes, one for each of then{\\\\displaystyle n}suffixes ofS{\\\\displaystyle S}. Since all internal non-root nodes are branching, there can be at most n   1 such nodes, and n + (n  1) + 1 = 2n nodes in total (n leaves, n  1 internal non-root nodes, 1 root).Suffix links are a key feature for older linear-time construction algorithms, although most newer algorithms, which are based on Farach\\'s algorithm, dispense with suffix links. In a complete suffix tree, all internal non-root nodes have a suffix link to another internal node. If the path from the root to a node spells the string{\\\\displaystyle \\\\chi \\\\alpha }, where{\\\\displaystyle \\\\chi }is a single character and{\\\\displaystyle \\\\alpha }is a string (possibly empty), it has a suffix link to the internal node representing{\\\\displaystyle \\\\alpha }. See for example the suffix link from the node for ANA to the node for NA in the figure above. Suffix links are also used in some algorithms running on the tree.A generalized suffix tree is a suffix tree made for a set of strings instead of a single string. It represents all suffixes from this set of strings. Each string must be terminated by a different termination symbol.HistoryThe concept was first introduced by Weiner (1973).Rather than the suffix S[i..n], Weiner stored in his trie the prefix identifier for each position, that is, the shortest string starting at i and occurring only once in S. His Algorithm D takes an uncompressed trie for S[k+1..n] and extends it into a trie for S[k..n]. This way, starting from the trivial trie for S[n..n], a trie for S[1..n] can be built by n-1 successive calls to Algorithm D; however, the overall run time is O(n2). Weiner\\'s Algorithm B maintains several auxiliary data structures, to achieve an over all run time linear in the size of the constructed trie. The latter can still be O(n2) nodes, e.g. for S = anbnanbn$. Weiner\\'s Algorithm C finally uses compressed tries to achieve linear overall storage size and run time.Donald Knuth subsequently characterized the latter as \"Algorithm of the Year 1973\".The text book Aho, Hopcroft & Ullman (1974, Sect.9.5) reproduced Weiner\\'s results in a simplified and more elegant form, introducing the term position tree.McCreight (1976) was the first to build a (compressed) trie of all suffixes of S. Although the suffix starting at i is usually longer than the prefix identifier, their path representations in a compressed trie do not differ in size. On the other hand, McCreight could dispense with most of Weiner\\'s auxiliary data structures; only suffix links remained.Ukkonen (1995) further simplified the construction. He provided the first online-construction of suffix trees, now known as Ukkonen\\'s algorithm, with running time that matched the then fastest algorithms.These algorithms are all linear-time for a constant-size alphabet, and have worst-case running time ofO(nlog\\u2061n){\\\\displaystyle O(n\\\\log n)}in general.Farach (1997) gave the first suffix tree construction algorithm that is optimal for all alphabets.  In particular, this is the first linear-time algorithmfor strings drawn from an alphabet of integers in a polynomial range.  Farach\\'s algorithm has become the basis for new algorithms for constructing both suffix trees and suffix arrays, for example, in external memory, compressed, succinct, etc.FunctionalityA suffix tree for a stringS{\\\\displaystyle S}of lengthn{\\\\displaystyle n}can be built in(n){\\\\displaystyle \\\\Theta (n)}time, if the letters come from an alphabet of integers in a polynomial range (in particular, this is true for constant-sized alphabets).For larger alphabets, the running time is dominated by first sorting the letters to bring them into a range of sizeO(n){\\\\displaystyle O(n)}; in general, this takesO(nlog\\u2061n){\\\\displaystyle O(n\\\\log n)}time.The costs below are given under the assumption that the alphabet is constant.Assume that a suffix tree has been built for the stringS{\\\\displaystyle S}of lengthn{\\\\displaystyle n}, or that a generalised suffix tree has been built for the set of stringsD={S1,S2,,SK}{\\\\displaystyle D=\\\\{S_{1},S_{2},\\\\dots ,S_{K}\\\\}}of total lengthn=n1+n2++nK{\\\\displaystyle n=n_{1}+n_{2}+\\\\cdots +n_{K}}.You can:Search for strings:Check if a stringP{\\\\displaystyle P}of lengthm{\\\\displaystyle m}is a substring inO(m){\\\\displaystyle O(m)}time.Find the first occurrence of the patternsP1,,Pq{\\\\displaystyle P_{1},\\\\dots ,P_{q}}of total lengthm{\\\\displaystyle m}as substrings inO(m){\\\\displaystyle O(m)}time.Find allz{\\\\displaystyle z}occurrences of the patternsP1,,Pq{\\\\displaystyle P_{1},\\\\dots ,P_{q}}of total lengthm{\\\\displaystyle m}as substrings inO(m+z){\\\\displaystyle O(m+z)}time.Search for a regular expression P in time expected sublinear inn{\\\\displaystyle n}.Find for each suffix of a patternP{\\\\displaystyle P}, the length of the longest match between a prefix ofP[im]{\\\\displaystyle P[i\\\\dots m]}and a substring inD{\\\\displaystyle D}in(m){\\\\displaystyle \\\\Theta (m)}time. This is termed the matching statistics forP{\\\\displaystyle P}.Find properties of the strings:Find the longest common substrings of the stringSi{\\\\displaystyle S_{i}}andSj{\\\\displaystyle S_{j}}in(ni+nj){\\\\displaystyle \\\\Theta (n_{i}+n_{j})}time.Find all maximal pairs, maximal repeats or supermaximal repeats in(n+z){\\\\displaystyle \\\\Theta (n+z)}time.Find the LempelZiv decomposition in(n){\\\\displaystyle \\\\Theta (n)}time.Find the longest repeated substrings in(n){\\\\displaystyle \\\\Theta (n)}time.Find the most frequently occurring substrings of a minimum length in(n){\\\\displaystyle \\\\Theta (n)}time.Find the shortest strings from{\\\\displaystyle \\\\Sigma }that do not occur inD{\\\\displaystyle D}, inO(n+z){\\\\displaystyle O(n+z)}time, if there arez{\\\\displaystyle z}such strings.Find the shortest substrings occurring only once in(n){\\\\displaystyle \\\\Theta (n)}time.Find, for eachi{\\\\displaystyle i}, the shortest substrings ofSi{\\\\displaystyle S_{i}}not occurring elsewhere inD{\\\\displaystyle D}in(n){\\\\displaystyle \\\\Theta (n)}time.The suffix tree can be prepared for constant time lowest common ancestor retrieval between nodes in(n){\\\\displaystyle \\\\Theta (n)}time. One can then also:Find the longest common prefix between the suffixesSi[p..ni]{\\\\displaystyle S_{i}[p..n_{i}]}andSj[q..nj]{\\\\displaystyle S_{j}[q..n_{j}]}in(1){\\\\displaystyle \\\\Theta (1)}.Search for a pattern P of length m with at most k mismatches inO(kn+z){\\\\displaystyle O(kn+z)}time, where z is the number of hits.Find allz{\\\\displaystyle z}maximal palindromes in(n){\\\\displaystyle \\\\Theta (n)}, or(gn){\\\\displaystyle \\\\Theta (gn)}time if gaps of lengthg{\\\\displaystyle g}are allowed, or(kn){\\\\displaystyle \\\\Theta (kn)}ifk{\\\\displaystyle k}mismatches are allowed.Find allz{\\\\displaystyle z}tandem repeats inO(nlog\\u2061n+z){\\\\displaystyle O(n\\\\log n+z)}, and k-mismatch tandem repeats inO(knlog\\u2061(n/k)+z){\\\\displaystyle O(kn\\\\log(n/k)+z)}.Find the longest common substrings to at leastk{\\\\displaystyle k}strings inD{\\\\displaystyle D}fork=2,,K{\\\\displaystyle k=2,\\\\dots ,K}in(n){\\\\displaystyle \\\\Theta (n)}time.Find the longest palindromic substring of a given string (using the generalized suffix tree of the string and its reverse) in linear time.ApplicationsSuffix trees can be used to solve a large number of string problems that occur in text-editing, free-text search, computational biology and other application areas. Primary applications include:String search, in O(m) complexity, where m is the length of the sub-string (but with initial O(n) time required to build the suffix tree for the string)Finding the longest repeated substringFinding the longest common substringFinding the longest palindrome in a stringSuffix trees are often used in bioinformatics applications, searching for patterns in DNA or protein sequences (which can be viewed as long strings of characters). The ability to search efficiently with mismatches might be considered their greatest strength. Suffix trees are also used in data compression; they can be used to find repeated data, and can be used for the sorting stage of the BurrowsWheeler transform. Variants of the LZW compression schemes use suffix trees (LZSS). A suffix tree is also used in suffix tree clustering, a data clustering algorithm used in some search engines.ImplementationIf each node and edge can be represented in(1){\\\\displaystyle \\\\Theta (1)}space, the entire tree can be represented in(n){\\\\displaystyle \\\\Theta (n)}space. The total length of all the strings on all of the edges in the tree isO(n2){\\\\displaystyle O(n^{2})}, but each edge can be stored as the position and length of a substring of S, giving a total space usage of(n){\\\\displaystyle \\\\Theta (n)}computer words. The worst-case space usage of a suffix tree is seen with a fibonacci word, giving the full2n{\\\\displaystyle 2n}nodes.An important choice when making a suffix tree implementation is the parent-child relationships between nodes. The most common is using linked lists called sibling lists. Each node has a pointer to its first child, and to the next node in the child list it is a part of. Other implementations with efficient running time properties use hash maps, sorted or unsorted arrays (with array doubling), or balanced search trees. We are interested in:The cost of finding the child on a given character.The cost of inserting a child.The cost of enlisting all children of a node (divided by the number of children in the table below).Let  be the size of the alphabet. Then you have the following costs:LookupInsertionTraversalSibling lists / unsorted arraysO()(1)(1)Bitwise sibling treesO(log\\u2061)(1)(1)Hash maps(1)(1)O()Balanced search treeO(log\\u2061)O(log\\u2061)O(1)Sorted arraysO(log\\u2061)O()O(1)Hash maps + sibling listsO(1)O(1)O(1){\\\\displaystyle {\\\\begin{array}{r|lll}&{\\\\text{Lookup}}&{\\\\text{Insertion}}&{\\\\text{Traversal}}\\\\\\\\\\\\hline {\\\\text{Sibling lists / unsorted arrays}}&O(\\\\sigma )&\\\\Theta (1)&\\\\Theta (1)\\\\\\\\{\\\\text{Bitwise sibling trees}}&O(\\\\log \\\\sigma )&\\\\Theta (1)&\\\\Theta (1)\\\\\\\\{\\\\text{Hash maps}}&\\\\Theta (1)&\\\\Theta (1)&O(\\\\sigma )\\\\\\\\{\\\\text{Balanced search tree}}&O(\\\\log \\\\sigma )&O(\\\\log \\\\sigma )&O(1)\\\\\\\\{\\\\text{Sorted arrays}}&O(\\\\log \\\\sigma )&O(\\\\sigma )&O(1)\\\\\\\\{\\\\text{Hash maps + sibling lists}}&O(1)&O(1)&O(1)\\\\end{array}}}The insertion cost is amortised, and that the costs for hashing are given for perfect hashing.The large amount of information in each edge and node makes the suffix tree very expensive, consuming about 10 to 20 times the memory size of the source text in good implementations. The suffix array reduces this requirement to a factor of 8 (for array including LCP values built within 32-bit address space and 8-bit characters.) This factor depends on the properties and may reach 2 with usage of 4-byte wide characters (needed to contain any symbol in some UNIX-like systems, see wchar_t) on 32-bit systems. Researchers have continued to find smaller indexing structures.Parallel constructionVarious parallel algorithms to speed up suffix tree construction have been proposed.Recently, a practical parallel algorithm for suffix tree construction withO(n){\\\\displaystyle O(n)}work (sequential time) andO(log2\\u2061n){\\\\displaystyle O(\\\\log ^{2}n)}span has been developed. The algorithm achieves good parallel scalability on shared-memory multicore machines and can index the human genome  approximately 3GB  in under 3 minutes using a 40-core machine.External constructionThough linear, the memory usage of a suffix tree is significantly higherthan the actual size of the sequence collection.  For a large text,construction may require external memory approaches.There are theoretical results for constructing suffix trees in externalmemory.The algorithm by Farach-Colton, Ferragina & Muthukrishnan (2000)is theoretically optimal, with an I/O complexity equal to that of sorting.However the overall intricacy of this algorithm has prevented, so far, itspractical implementation.On the other hand, there have been practical works for constructingdisk-based suffix treeswhich scale to (few) GB/hours.The state of the art methods are TDD,TRELLIS,DiGeST,andB2ST.TDD and TRELLIS scale up to the entire human genome resulting in a disk-based suffix tree of a size in the tens of gigabytes. However, these methods cannot handle efficiently collections of sequences exceeding 3GB.  DiGeST performs significantly better and is able to handle collections of sequences in the order of 6GB in about 6 hours..All these methods can efficiently build suffix trees for the case when thetree does not fit in main memory,but the input does.The most recent method, B2ST, scales to handleinputs that do not fit in main memory. ERA  is a recent parallel suffix tree construction method that is significantly faster. ERA can index the entire human genome in 19 minutes on an 8-core desktop computer with 16GB RAM. On a simple Linux cluster with 16 nodes (4GB RAM per node), ERA can index the entire human genome in less than 9 minutes.See alsoSuffix arraySuffix automatonGeneralised suffix treeTrieNotesReferencesAho, Alfred V.; Hopcroft, John E.; Ullman, Jeffrey D. (1974), The Design and Analysis of Computer Algorithms, Reading/MA: Addison-Wesley, ISBN 0-201-00029-6.Apostolico, A.; Iliopoulos, C.; Landau, G. M.; Schieber, B.; Vishkin, U. (1988), \"Parallel construction of a suffix tree with applications\", Algorithmica, 3 (14): 347365, doi:10.1007/bf01762122, S2CID 5024136.Baeza-Yates, Ricardo A.; Gonnet, Gaston H. (1996), \"Fast text searching for regular expressions or automaton searching on tries\", Journal of the ACM, 43 (6): 915936, doi:10.1145/235809.235810, S2CID 1420298.Barsky, Marina; Stege, Ulrike; Thomo, Alex; Upton, Chris (2008), \"A new method for indexing genomes using on-disk suffix trees\", CIKM \\'08: Proceedings of the 17th ACM Conference on Information and Knowledge Management, New York, NY, USA: ACM, pp. 649658.Barsky, Marina; Stege, Ulrike; Thomo, Alex; Upton, Chris (2009), \"Suffix trees for very large genomic sequences\", CIKM \\'09: Proceedings of the 18th ACM Conference on Information and Knowledge Management, New York, NY, USA: ACM.Farach, Martin (1997), \"Optimal Suffix Tree Construction with Large Alphabets\" (PDF), 38th IEEE Symposium on Foundations of Computer Science (FOCS \\'97), pp. 137143.Farach, Martin; Muthukrishnan, S. (1996), \"Optimal Logarithmic Time Randomized Suffix Tree Construction\", International Colloquium on Automata Languages and Programming.Farach-Colton, Martin; Ferragina, Paolo; Muthukrishnan, S. (2000), \"On the sorting-complexity of suffix tree construction.\", Journal of the ACM, 47 (6): 9871011, doi:10.1145/355541.355547, S2CID 8164822.Giegerich, R.; Kurtz, S. (1997), \"From Ukkonen to McCreight and Weiner: A Unifying View of Linear-Time Suffix Tree Construction\" (PDF), Algorithmica, 19 (3): 331353, doi:10.1007/PL00009177, S2CID 18039097, archived from the original (PDF) on 2016-03-03, retrieved 2012-07-13.Gusfield, Dan (1999), Algorithms on Strings, Trees and Sequences: Computer Science and Computational Biology, Cambridge University Press, ISBN 0-521-58519-8.Hariharan, Ramesh (1994), \"Optimal Parallel Suffix Tree Construction\", ACM Symposium on Theory of Computing.Iliopoulos, Costas; Rytter, Wojciech (2004), \"On Parallel Transformations of Suffix Arrays into Suffix Trees\", 15th Australasian Workshop on Combinatorial Algorithms.Mansour, Essam; Allam, Amin; Skiadopoulos, Spiros; Kalnis, Panos (2011), \"ERA: Efficient Serial and Parallel Suffix Tree Construction for Very Long Strings\" (PDF), Proceedings of the VLDB Endowment, 5 (1): 4960, arXiv:1109.6884, Bibcode:2011arXiv1109.6884M, doi:10.14778/2047485.2047490, S2CID 7582116.McCreight, Edward M. (1976), \"A Space-Economical Suffix Tree Construction Algorithm\", Journal of the ACM, 23 (2): 262272, CiteSeerX 10.1.1.130.8022, doi:10.1145/321941.321946, S2CID 9250303.Phoophakdee, Benjarath; Zaki, Mohammed J. (2007), \"Genome-scale disk-based suffix tree indexing\", SIGMOD \\'07: Proceedings of the ACM SIGMOD International Conference on Management of Data, New York, NY, USA: ACM, pp. 833844.Sahinalp, Cenk; Vishkin, Uzi (1994), \"Symmetry breaking for suffix tree construction\", ACM Symposium on Theory of ComputingSmyth, William (2003), Computing Patterns in Strings, Addison-Wesley.Shun, Julian; Blelloch, Guy E. (2014), \"A Simple Parallel Cartesian Tree Algorithm and its Application to Parallel Suffix Tree Construction\", ACM Transactions on Parallel Computing, 1: 120, doi:10.1145/2661653, S2CID 1912378.Tata, Sandeep; Hankins, Richard A.; Patel, Jignesh M. (2003), \"Practical Suffix Tree Construction\", VLDB \\'03: Proceedings of the 30th International Conference on Very Large Data Bases, Morgan Kaufmann, pp. 3647.Ukkonen, E. (1995), \"On-line construction of suffix trees\" (PDF), Algorithmica, 14 (3): 249260, doi:10.1007/BF01206331, S2CID 6027556.Weiner, P. (1973), \"Linear pattern matching algorithms\" (PDF), 14th Annual IEEE Symposium on Switching and Automata Theory, pp. 111, doi:10.1109/SWAT.1973.13.Zamir, Oren; Etzioni, Oren (1998), \"Web document clustering: a feasibility demonstration\", SIGIR \\'98: Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, New York, NY, USA: ACM, pp. 4654.External linksSuffix Trees by Sartaj SahniNIST\\'s Dictionary of Algorithms and Data Structures: Suffix TreeUniversal Data Compression Based on the Burrows-Wheeler Transformation: Theory and Practice, application of suffix trees in the BWTTheory and Practice of Succinct Data Structures, C++ implementation of a compressed suffix treeUkkonen\\'s Suffix Tree Implementation in C Part 1 Part 2 Part 3 Part 4 Part 5 Part 6Online Demo: Ukkonen\\'s Suffix Tree Visualization',\n",
       " '= Levenshtein automaton =In computer science, a Levenshtein automaton for a string w and a number n is a finite-state automaton that can recognize the set of all strings whose Levenshtein distance from w is at most n. That is, a string x is in the formal language recognized by the Levenshtein automaton if and only if x can be transformed into w by at most n single-character insertions, deletions, and substitutions.ApplicationsLevenshtein automata may be used for spelling correction, by finding words in a given dictionary that are close to a misspelled word. In this application, once a word is identified as being misspelled, its Levenshtein automaton may be constructed, and then applied to all of the words in the dictionary to determine which ones are close to the misspelled word. If the dictionary is stored in compressed form as a trie, the time for this algorithm (after the automaton has been constructed) is proportional to the number of nodes in the trie, significantly faster than using dynamic programming to compute the Levenshtein distance separately for each dictionary word.It is also possible to find words in a regular language, rather than a finite dictionary, that are close to a given target word, by computing the Levenshtein automaton for the word, and then using a Cartesian product construction to combine it with an automaton for the regular language, giving an automaton for the intersection language. Alternatively, rather than using the product construction, both the Levenshtein automaton and the automaton for the given regular language may be traversed simultaneously using a backtracking algorithm.Levenshtein automata are used in Lucene for full-text searches that can return relevant documents even if the query is misspelled.ConstructionFor any fixed constant n, the Levenshtein automaton for w and n may be constructed in time O(|w|).Mitankin studies a variant of this construction called the universal Levenshtein automaton, determined only by a numeric parameter n, that can recognize pairs of words (encoded in a certain way by bitvectors) that are within Levenshtein distance n of each other. Touzet proposed an effective algorithm to build this automaton.Yet a third finite automaton construction of Levenshtein (or DamerauLevenshtein) distance are the Levenshtein transducers of Hassan et al., who show finite state transducers implementing edit distance one, then compose these to implement edit distances up to some constant.See alsoagrep, tool (implemented several times) for approximate regular expression matchingTRE, library for regular expression matching that is tolerant to Levenshtein-style edits== References ==',\n",
       " '= Suffix automaton =In computer science, a suffix automaton is an efficient data structure for representing the substring index of a given string which allows the storage, processing, and retrieval of compressed information about all its substrings. The suffix automaton of a stringS{\\\\displaystyle S}is the smallest directed acyclic graph with a dedicated initial vertex and a set of \"final\" vertices, such that paths from the initial vertex to final vertices represent the suffixes of the string.In terms of automata theory, a suffix automaton is the minimal partial deterministic finite automaton that recognizes the set of suffixes of a given stringS=s1s2sn{\\\\displaystyle S=s_{1}s_{2}\\\\dots s_{n}}. The state graph of a suffix automaton is called a directed acyclic word graph (DAWG), a term that is also sometimes used for any deterministic acyclic finite state automaton.Suffix automata were introduced in 1983 by a group of scientists from the University of Denver and the University of Colorado Boulder. They suggested a linear time online algorithm for its construction and showed that the suffix automaton of a stringS{\\\\displaystyle S}having length at least two characters has at most2|S|1{\\\\textstyle 2|S|-1}states and at most3|S|4{\\\\textstyle 3|S|-4}transitions. Further works have shown a close connection between suffix automata and suffix trees, and have outlined several generalizations of suffix automata, such as compacted suffix automaton obtained by compression of nodes with a single outgoing arc.Suffix automata provide efficient solutions to problems such as substring search and computation of the largest common substring of two and more strings.HistoryThe concept of suffix automaton was introduced in 1983 by a group of scientists from University of Denver and University of Colorado Boulder consisting of Anselm Blumer, Janet Blumer, Andrzej Ehrenfeucht, David Haussler and Ross McConnell, although similar concepts had earlier been studied alongside suffix trees in the works of Peter Weiner, Vaughan Pratt and Anatol Slissenko. In their initial work, Blumer et al. showed a suffix automaton built for the stringS{\\\\displaystyle S}of length greater than1{\\\\displaystyle 1}has at most2|S|1{\\\\displaystyle 2|S|-1}states and at most3|S|4{\\\\displaystyle 3|S|-4}transitions, and suggested a linear algorithm for automaton construction.In 1983, Mu-Tian Chen and Joel Seiferas independently showed that Weiner\\'s 1973 suffix-tree construction algorithm while building a suffix tree of the stringS{\\\\displaystyle S}constructs a suffix automaton of the reversed stringSR{\\\\textstyle S^{R}}as an auxiliary structure. In 1987, Blumer et al. applied the compressing technique used in suffix trees to a suffix automaton and invented the compacted suffix automaton, which is also called the compacted directed acyclic word graph (CDAWG). In 1997, Maxime Crochemore and Renaud Vrin developed a linear algorithm for direct CDAWG construction. In 2001, Shunsuke Inenaga et al. developed an algorithm for construction of CDAWG for a set of words given by a trie.DefinitionsUsually when speaking about suffix automata and related concepts, some notions from formal language theory and automata theory are used, in particular:\"Alphabet\" is a finite set{\\\\displaystyle \\\\Sigma }that is used to construct words. Its elements are called \"characters\";\"Word\" is a finite sequence of characters=12n{\\\\displaystyle \\\\omega =\\\\omega _{1}\\\\omega _{2}\\\\dots \\\\omega _{n}}. \"Length\" of the word{\\\\displaystyle \\\\omega }is denoted as||=n{\\\\displaystyle |\\\\omega |=n};\"Formal language\" is a set of words over given alphabet;\"Language of all words\" is denoted as{\\\\displaystyle \\\\Sigma ^{*}}(where the \"*\" character stands for Kleene star), \"empty word\" (the word of zero length) is denoted by the character{\\\\displaystyle \\\\varepsilon };\"Concatenation of words\"=12n{\\\\displaystyle \\\\alpha =\\\\alpha _{1}\\\\alpha _{2}\\\\dots \\\\alpha _{n}}and=12m{\\\\displaystyle \\\\beta =\\\\beta _{1}\\\\beta _{2}\\\\dots \\\\beta _{m}}is denoted as{\\\\displaystyle \\\\alpha \\\\cdot \\\\beta }or{\\\\displaystyle \\\\alpha \\\\beta }and corresponds to the word obtained by writing{\\\\displaystyle \\\\beta }to the right of{\\\\displaystyle \\\\alpha }, that is,=12n12m{\\\\displaystyle \\\\alpha \\\\beta =\\\\alpha _{1}\\\\alpha _{2}\\\\dots \\\\alpha _{n}\\\\beta _{1}\\\\beta _{2}\\\\dots \\\\beta _{m}};\"Concatenation of languages\"A{\\\\displaystyle A}andB{\\\\displaystyle B}is denoted asAB{\\\\displaystyle A\\\\cdot B}orAB{\\\\displaystyle AB}and corresponds to the set of pairwise concatenationsAB={:A,B}{\\\\displaystyle AB=\\\\{\\\\alpha \\\\beta :\\\\alpha \\\\in A,\\\\beta \\\\in B\\\\}};If the word{\\\\displaystyle \\\\omega \\\\in \\\\Sigma ^{*}}may be represented as={\\\\displaystyle \\\\omega =\\\\alpha \\\\gamma \\\\beta }, where,,{\\\\displaystyle \\\\alpha ,\\\\beta ,\\\\gamma \\\\in \\\\Sigma ^{*}}, then words{\\\\displaystyle \\\\alpha },{\\\\displaystyle \\\\beta }and{\\\\displaystyle \\\\gamma }are called \"prefix\", \"suffix\" and \"subword\" (substring) of the word{\\\\displaystyle \\\\omega }correspondingly;IfTlTl+1Tr=S{\\\\displaystyle T_{l}T_{l+1}\\\\dots T_{r}=S}thenS{\\\\displaystyle S}is said to \"occur\" inT{\\\\displaystyle T}as a subword. Herel{\\\\displaystyle l}andr{\\\\displaystyle r}are called left and right positions of occurrence ofS{\\\\displaystyle S}inT{\\\\displaystyle T}correspondingly.Automaton structureFormally, deterministic finite automaton is determined by 5-tupleA=(,Q,q0,F,){\\\\displaystyle {\\\\mathcal {A}}=(\\\\Sigma ,Q,q_{0},F,\\\\delta )}, where:{\\\\displaystyle \\\\Sigma }is an \"alphabet\" that is used to construct words,Q{\\\\displaystyle Q}is a set of automaton \"states\",q0Q{\\\\displaystyle q_{0}\\\\in Q}is an \"initial\" state of automaton,FQ{\\\\displaystyle F\\\\subset Q}is a set of \"final\" states of automaton,:QQ{\\\\displaystyle \\\\delta :Q\\\\times \\\\Sigma \\\\mapsto Q}is a partial \"transition\" function of automaton, such that(q,){\\\\displaystyle \\\\delta (q,\\\\sigma )}forqQ{\\\\displaystyle q\\\\in Q}and{\\\\displaystyle \\\\sigma \\\\in \\\\Sigma }is either undefined or defines a transition fromq{\\\\displaystyle q}over character{\\\\displaystyle \\\\sigma }.Most commonly, deterministic finite automaton is represented as a directed graph (\"diagram\") such that:Set of graph vertices corresponds to the state of statesQ{\\\\displaystyle Q},Graph has a specific marked vertex corresponding to initial stateq0{\\\\displaystyle q_{0}},Graph has several marked vertices corresponding to the set of final statesF{\\\\displaystyle F},Set of graph arcs corresponds to the set of transitions{\\\\displaystyle \\\\delta },Specifically, every transition(q1,)=q2{\\\\textstyle \\\\delta (q_{1},\\\\sigma )=q_{2}}is represented by an arc fromq1{\\\\displaystyle q_{1}}toq2{\\\\displaystyle q_{2}}marked with the character{\\\\displaystyle \\\\sigma }. This transition also may be denoted asq1q2{\\\\textstyle q_{1}{\\\\begin{smallmatrix}{\\\\sigma }\\\\\\\\[-5pt]{\\\\longrightarrow }\\\\end{smallmatrix}}q_{2}}.In terms of its diagram, the automaton recognizes the word=12m{\\\\displaystyle \\\\omega =\\\\omega _{1}\\\\omega _{2}\\\\dots \\\\omega _{m}}only if there is a path from the initial vertexq0{\\\\displaystyle q_{0}}to some final vertexqF{\\\\displaystyle q\\\\in F}such that concatenation of characters on this path forms{\\\\displaystyle \\\\omega }. The set of words recognized by an automaton forms a language that is set to be recognized by the automaton. In these terms, the language recognized by a suffix automaton ofS{\\\\displaystyle S}is the language of its (possibly empty) suffixes.Automaton states\"Right context\" of the word{\\\\displaystyle \\\\omega }with respect to languageL{\\\\displaystyle L}is a set[]R={:L}{\\\\displaystyle [\\\\omega ]_{R}=\\\\{\\\\alpha :\\\\omega \\\\alpha \\\\in L\\\\}}that is a set of words{\\\\displaystyle \\\\alpha }such that their concatenation with{\\\\displaystyle \\\\omega }forms a word fromL{\\\\displaystyle L}. Right contexts induce a natural equivalence relation[]R=[]R{\\\\displaystyle [\\\\alpha ]_{R}=[\\\\beta ]_{R}}on the set of all words. If languageL{\\\\displaystyle L}is recognized by some deterministic finite automaton, there exists unique up to isomorphism automaton that recognizes the same language and has the minimum possible number of states. Such an automaton is called a minimal automaton for the given languageL{\\\\displaystyle L}. MyhillNerode theorem allows it to define it explicitly in terms of right contexts:In these terms, a \"suffix automaton\" is the minimal deterministic finite automaton recognizing the language of suffixes of the wordS=s1s2sn{\\\\displaystyle S=s_{1}s_{2}\\\\dots s_{n}}. The right context of the word{\\\\displaystyle \\\\omega }with respect to this language consists of words{\\\\displaystyle \\\\alpha }, such that{\\\\displaystyle \\\\omega \\\\alpha }is a suffix ofS{\\\\displaystyle S}. It allows to formulate the following lemma defining a bijection between the right context of the word and the set of right positions of its occurrences inS{\\\\displaystyle S}:For example, for the wordS=abacaba{\\\\displaystyle S=abacaba}and its subword=ab{\\\\displaystyle \\\\omega =ab}, it holdsendpos(ab)={2,6}{\\\\displaystyle endpos(ab)=\\\\{2,6\\\\}}and[ab]R={a,acaba}{\\\\displaystyle [ab]_{R}=\\\\{a,acaba\\\\}}. Informally,[ab]R{\\\\displaystyle [ab]_{R}}is formed by words that follow occurrences ofab{\\\\displaystyle ab}to the end ofS{\\\\displaystyle S}andendpos(ab){\\\\displaystyle endpos(ab)}is formed by right positions of those occurrences. In this example, the elementx=2endpos(ab){\\\\displaystyle x=2\\\\in endpos(ab)}corresponds with the words3s4s5s6s7=acaba[ab]R{\\\\displaystyle s_{3}s_{4}s_{5}s_{6}s_{7}=acaba\\\\in [ab]_{R}}while the worda[ab]R{\\\\displaystyle a\\\\in [ab]_{R}}corresponds with the element7|a|=6endpos(ab){\\\\displaystyle 7-|a|=6\\\\in endpos(ab)}.It implies several structure properties of suffix automaton states. Let||||{\\\\displaystyle |\\\\alpha |\\\\leq |\\\\beta |}, then:If[]R{\\\\displaystyle [\\\\alpha ]_{R}}and[]R{\\\\displaystyle [\\\\beta ]_{R}}have at least one common elementx{\\\\displaystyle x}, thenendpos(){\\\\displaystyle endpos(\\\\alpha )}andendpos(){\\\\displaystyle endpos(\\\\beta )}have a common element as well. It implies{\\\\displaystyle \\\\alpha }is a suffix of{\\\\displaystyle \\\\beta }and thereforeendpos()endpos(){\\\\displaystyle endpos(\\\\beta )\\\\subset endpos(\\\\alpha )}and[]R[]R{\\\\displaystyle [\\\\beta ]_{R}\\\\subset [\\\\alpha ]_{R}}. In aforementioned example,a[ab]R[cab]R{\\\\displaystyle a\\\\in [ab]_{R}\\\\cap [cab]_{R}}, soab{\\\\displaystyle ab}is a suffix ofcab{\\\\displaystyle cab}and thus[cab]R={a}{a,acaba}=[ab]R{\\\\displaystyle [cab]_{R}=\\\\{a\\\\}\\\\subset \\\\{a,acaba\\\\}=[ab]_{R}}andendpos(cab)={6}{2,6}=endpos(ab){\\\\displaystyle endpos(cab)=\\\\{6\\\\}\\\\subset \\\\{2,6\\\\}=endpos(ab)};If[]R=[]R{\\\\displaystyle [\\\\alpha ]_{R}=[\\\\beta ]_{R}}, thenendpos()=endpos(){\\\\displaystyle endpos(\\\\alpha )=endpos(\\\\beta )}, thus{\\\\displaystyle \\\\alpha }occurs inS{\\\\displaystyle S}only as a suffix of{\\\\displaystyle \\\\beta }. For example, for=b{\\\\displaystyle \\\\alpha =b}and=ab{\\\\displaystyle \\\\beta =ab}it holds that[b]R=[ab]R={a,acaba}{\\\\displaystyle [b]_{R}=[ab]_{R}=\\\\{a,acaba\\\\}}andendpos(b)=endpos(ab)={2,6}{\\\\displaystyle endpos(b)=endpos(ab)=\\\\{2,6\\\\}};If[]R=[]R{\\\\displaystyle [\\\\alpha ]_{R}=[\\\\beta ]_{R}}and{\\\\displaystyle \\\\gamma }is a suffix of{\\\\displaystyle \\\\beta }such that||||||{\\\\displaystyle |\\\\alpha |\\\\leq |\\\\gamma |\\\\leq |\\\\beta |}, then[]R=[]R=[]R{\\\\displaystyle [\\\\alpha ]_{R}=[\\\\gamma ]_{R}=[\\\\beta ]_{R}}. In the example above[c]R=[bac]R={aba}{\\\\displaystyle [c]_{R}=[bac]_{R}=\\\\{aba\\\\}}and it holds for \"intermediate\" suffix=ac{\\\\displaystyle \\\\gamma =ac}that[ac]R={aba}{\\\\displaystyle [ac]_{R}=\\\\{aba\\\\}}.Any stateq=[]R{\\\\displaystyle q=[\\\\alpha ]_{R}}of the suffix automaton recognizes some continuous chain of nested suffixes of the longest word recognized by this state.\"Left extension\"{\\\\displaystyle {\\\\overset {\\\\scriptstyle {\\\\leftarrow }}{\\\\gamma }}}of the string{\\\\displaystyle \\\\gamma }is the longest string{\\\\displaystyle \\\\omega }that has the same right context as{\\\\displaystyle \\\\gamma }. Length||{\\\\displaystyle |{\\\\overset {\\\\scriptstyle {\\\\leftarrow }}{\\\\gamma }}|}of the longest string recognized byq=[]R{\\\\displaystyle q=[\\\\gamma ]_{R}}is denoted bylen(q){\\\\displaystyle len(q)}. It holds:\"Suffix link\"link(q){\\\\displaystyle link(q)}of the stateq=[]R{\\\\displaystyle q=[\\\\alpha ]_{R}}is the pointer to the statep{\\\\displaystyle p}that contains the largest suffix of{\\\\displaystyle \\\\alpha }that is not recognized byq{\\\\displaystyle q}.In this terms it can be saidq=[]R{\\\\displaystyle q=[\\\\alpha ]_{R}}recognizes exactly all suffixes of{\\\\displaystyle {\\\\overset {\\\\scriptstyle {\\\\leftarrow }}{\\\\alpha }}}that is longer thanlen(link(q)){\\\\displaystyle len(link(q))}and not longer thanlen(q){\\\\displaystyle len(q)}. It also holds:Connection with suffix treesA \"prefix tree\" (or \"trie\") is a rooted directed tree in which arcs are marked by characters in such a way no vertexv{\\\\displaystyle v}of such tree has two out-going arcs marked with the same character. Some vertices in trie are marked as final. Trie is said to recognize a set of words defined by paths from its root to final vertices. In this way prefix trees are a special kind of deterministic finite automata if you perceive its root as an initial vertex. The \"suffix trie\" of the wordS{\\\\displaystyle S}is a prefix tree recognizing a set of its suffixes. \"A suffix tree\" is a tree obtained from a suffix trie via the compaction procedure, during which consequent edges are merged if the degree of the vertex between them is equal to two.By its definition, a suffix automaton can be obtained via minimization of the suffix trie. It may be shown that a compacted suffix automaton is obtained by both minimization of the suffix tree (if one assumes each string on the edge of the suffix tree is a solid character from the alphabet) and compaction of the suffix automaton. Besides this connection between the suffix tree and the suffix automaton of the same string there is as well a connection between the suffix automaton of the stringS=s1s2sn{\\\\displaystyle S=s_{1}s_{2}\\\\dots s_{n}}and the suffix tree of the reversed stringSR=snsn1s1{\\\\displaystyle S^{R}=s_{n}s_{n-1}\\\\dots s_{1}}.Similarly to right contexts one may introduce \"left contexts\"[]L={:L}{\\\\displaystyle [\\\\omega ]_{L}=\\\\{\\\\beta \\\\in \\\\Sigma ^{*}:\\\\beta \\\\omega \\\\in L\\\\}}, \"right extensions\"{\\\\displaystyle {\\\\overset {\\\\scriptstyle {\\\\rightarrow }}{\\\\omega ~}}}corresponding to the longest string having same left context as{\\\\displaystyle \\\\omega }and the equivalence relation[]L=[]L{\\\\displaystyle [\\\\alpha ]_{L}=[\\\\beta ]_{L}}. If one considers right extensions with respect to the languageL{\\\\displaystyle L}of \"prefixes\" of the stringS{\\\\displaystyle S}it may be obtained:, which implies the suffix link tree of the stringS{\\\\displaystyle S}and the suffix tree of the stringSR{\\\\displaystyle S^{R}}are isomorphic:Similarly to the case of left extensions, the following lemma holds for right extensions:SizeA suffix automaton of the stringS{\\\\displaystyle S}of lengthn>1{\\\\displaystyle n>1}has at most2n1{\\\\displaystyle 2n-1}states and at most3n4{\\\\displaystyle 3n-4}transitions. These bounds are reached on stringsabbbb=abn1{\\\\displaystyle abb\\\\dots bb=ab^{n-1}}andabbbc=abn2c{\\\\displaystyle abb\\\\dots bc=ab^{n-2}c}correspondingly. This may be formulated in a stricter way as|||Q|+n2{\\\\displaystyle |\\\\delta |\\\\leq |Q|+n-2}where||{\\\\displaystyle |\\\\delta |}and|Q|{\\\\displaystyle |Q|}are the numbers of transitions and states in automaton correspondingly.ConstructionInitially the automaton only consists of a single state corresponding to the empty word, then characters of the string are added one by one and the automaton is rebuilt on each step incrementally.State updatesAfter a new character is appended to the string, some equivalence classes are altered. Let[]R{\\\\displaystyle [\\\\alpha ]_{R_{\\\\omega }}}be the right context of{\\\\displaystyle \\\\alpha }with respect to the language of{\\\\displaystyle \\\\omega }suffixes. Then the transition from[]R{\\\\displaystyle [\\\\alpha ]_{R_{\\\\omega }}}to[]Rx{\\\\displaystyle [\\\\alpha ]_{R_{\\\\omega x}}}afterx{\\\\displaystyle x}is appended to{\\\\displaystyle \\\\omega }is defined by lemma:After addingx{\\\\displaystyle x}to the current word{\\\\displaystyle \\\\omega }the right context of{\\\\displaystyle \\\\alpha }may change significantly only if{\\\\displaystyle \\\\alpha }is a suffix ofx{\\\\displaystyle \\\\omega x}. It implies equivalence relationRx{\\\\displaystyle \\\\equiv _{R_{\\\\omega x}}}is a refinement ofR{\\\\displaystyle \\\\equiv _{R_{\\\\omega }}}. In other words, if[]Rx=[]Rx{\\\\displaystyle [\\\\alpha ]_{R_{\\\\omega x}}=[\\\\beta ]_{R_{\\\\omega x}}}, then[]R=[]R{\\\\displaystyle [\\\\alpha ]_{R_{\\\\omega }}=[\\\\beta ]_{R_{\\\\omega }}}. After the addition of a new character at most two equivalence classes ofR{\\\\displaystyle \\\\equiv _{R_{\\\\omega }}}will be split and each of them may split in at most two new classes. First, equivalence class corresponding to empty right context is always split into two equivalence classes, one of them corresponding tox{\\\\displaystyle \\\\omega x}itself and having{}{\\\\displaystyle \\\\{\\\\varepsilon \\\\}}as a right context. This new equivalence class contains exactlyx{\\\\displaystyle \\\\omega x}and all its suffixes that did not occur in{\\\\displaystyle \\\\omega }, as the right context of such words was empty before and contains only empty word now.Given the correspondence between states of the suffix automaton and vertices of the suffix tree, it is possible to find out the second state that may possibly split after a new character is appended. The transition from{\\\\displaystyle \\\\omega }tox{\\\\displaystyle \\\\omega x}corresponds to the transition fromR{\\\\displaystyle \\\\omega ^{R}}toxR{\\\\displaystyle x\\\\omega ^{R}}in the reversed string. In terms of suffix trees it corresponds to the insertion of the new longest suffixxR{\\\\displaystyle x\\\\omega ^{R}}into the suffix tree ofR{\\\\displaystyle \\\\omega ^{R}}. At most two new vertices may be formed after this insertion: one of them corresponding toxR{\\\\displaystyle x\\\\omega ^{R}}, while the other one corresponds to its direct ancestor if there was a branching. Returning to suffix automata, it means the first new state recognizesx{\\\\displaystyle \\\\omega x}and the second one (if there is a second new state) is its suffix link. It may be stated as a lemma:It implies that if={\\\\displaystyle \\\\alpha =\\\\beta }(for example, whenx{\\\\displaystyle x}didn\\'t occur in{\\\\displaystyle \\\\omega }at all and=={\\\\displaystyle \\\\alpha =\\\\beta =\\\\varepsilon }), then only the equivalence class corresponding to the empty right context is split.Besides suffix links it is also needed to define final states of the automaton. It follows from structure properties that all suffixes of a word{\\\\displaystyle \\\\alpha }recognized byq=[]R{\\\\displaystyle q=[\\\\alpha ]_{R}}are recognized by some vertex on suffix path(q,link(q),link2(q),){\\\\displaystyle (q,link(q),link^{2}(q),\\\\dots )}ofq{\\\\displaystyle q}. Namely, suffixes with length greater thanlen(link(q)){\\\\displaystyle len(link(q))}lie inq{\\\\displaystyle q}, suffixes with length greater thanlen(link(link(q)){\\\\displaystyle len(link(link(q))}but not greater thanlen(link(q)){\\\\displaystyle len(link(q))}lie inlink(q){\\\\displaystyle link(q)}and so on. Thus if the state recognizing{\\\\displaystyle \\\\omega }is denoted bylast{\\\\displaystyle last}, then all final states (that is, recognizing suffixes of{\\\\displaystyle \\\\omega }) form up the sequence(last,link(last),link2(last),){\\\\displaystyle (last,link(last),link^{2}(last),\\\\dots )}.Transitions and suffix links updatesAfter the characterx{\\\\displaystyle x}is appended to{\\\\displaystyle \\\\omega }possible new states of suffix automaton are[x]Rx{\\\\displaystyle [\\\\omega x]_{R_{\\\\omega x}}}and[]Rx{\\\\displaystyle [\\\\alpha ]_{R_{\\\\omega x}}}. Suffix link from[x]Rx{\\\\displaystyle [\\\\omega x]_{R_{\\\\omega x}}}goes to[]Rx{\\\\displaystyle [\\\\alpha ]_{R_{\\\\omega x}}}and from[]Rx{\\\\displaystyle [\\\\alpha ]_{R_{\\\\omega x}}}it goes tolink([]R){\\\\displaystyle link([\\\\alpha ]_{R_{\\\\omega }})}. Words from[x]Rx{\\\\displaystyle [\\\\omega x]_{R_{\\\\omega x}}}occur inx{\\\\displaystyle \\\\omega x}only as its suffixes therefore there should be no transitions at all from[x]Rx{\\\\displaystyle [\\\\omega x]_{R_{\\\\omega x}}}while transitions to it should go from suffixes of{\\\\displaystyle \\\\omega }having length at least{\\\\displaystyle \\\\alpha }and be marked with the characterx{\\\\displaystyle x}. State[]Rx{\\\\displaystyle [\\\\alpha ]_{R_{\\\\omega x}}}is formed by subset of[]R{\\\\displaystyle [\\\\alpha ]_{R_{\\\\omega }}}, thus transitions from[]Rx{\\\\displaystyle [\\\\alpha ]_{R_{\\\\omega x}}}should be same as from[]R{\\\\displaystyle [\\\\alpha ]_{R_{\\\\omega }}}. Meanwhile, transitions leading to[]Rx{\\\\displaystyle [\\\\alpha ]_{R_{\\\\omega x}}}should go from suffixes of{\\\\displaystyle \\\\omega }having length less than||{\\\\displaystyle |\\\\alpha |}and at leastlen(link([]R)){\\\\displaystyle len(link([\\\\alpha ]_{R_{\\\\omega }}))}, as such transitions have led to[]R{\\\\displaystyle [\\\\alpha ]_{R_{\\\\omega }}}before and corresponded to seceded part of this state. States corresponding to these suffixes may be determined via traversal of suffix link path for[]R{\\\\displaystyle [\\\\omega ]_{R_{\\\\omega }}}.Construction algorithmTheoretical results above lead to the following algorithm that takes characterx{\\\\displaystyle x}and rebuilds the suffix automaton of{\\\\displaystyle \\\\omega }into the suffix automaton ofx{\\\\displaystyle \\\\omega x}:The state corresponding to the word{\\\\displaystyle \\\\omega }is kept aslast{\\\\displaystyle last};Afterx{\\\\displaystyle x}is appended, previous value oflast{\\\\displaystyle last}is stored in the variablep{\\\\displaystyle p}andlast{\\\\displaystyle last}itself is reassigned to the new state corresponding tox{\\\\displaystyle \\\\omega x};States corresponding to suffixes of{\\\\displaystyle \\\\omega }are updated with transitions tolast{\\\\displaystyle last}. To do this one should go throughp,link(p),link2(p),{\\\\displaystyle p,link(p),link^{2}(p),\\\\dots }, until there is a state that already has a transition byx{\\\\displaystyle x};Once the aforementioned loop is over, there are 3 cases:If none of states on the suffix path had a transition byx{\\\\displaystyle x}, thenx{\\\\displaystyle x}never occurred in{\\\\displaystyle \\\\omega }before and the suffix link fromlast{\\\\displaystyle last}should lead toq0{\\\\displaystyle q_{0}};If the transition byx{\\\\displaystyle x}is found and leads from the statep{\\\\displaystyle p}to the stateq{\\\\displaystyle q}, such thatlen(p)+1=len(q){\\\\displaystyle len(p)+1=len(q)}, thenq{\\\\displaystyle q}does not have to be split and it is a suffix link oflast{\\\\displaystyle last};If the transition is found butlen(q)>len(p)+1{\\\\displaystyle len(q)>len(p)+1}, then words fromq{\\\\displaystyle q}having length at mostlen(p)+1{\\\\displaystyle len(p)+1}should be segregated into new \"clone\" statecl{\\\\displaystyle cl};If the previous step was concluded with the creation ofcl{\\\\displaystyle cl}, transitions from it and its suffix link should copy those ofq{\\\\displaystyle q}, at the same timecl{\\\\displaystyle cl}is assigned to be common suffix link of bothq{\\\\displaystyle q}andlast{\\\\displaystyle last};Transitions that have led toq{\\\\displaystyle q}before but corresponded to words of the length at mostlen(p)+1{\\\\displaystyle len(p)+1}are redirected tocl{\\\\displaystyle cl}. To do this, one continues going through the suffix path ofp{\\\\displaystyle p}until the state is found such that transition byx{\\\\displaystyle x}from it doesn\\'t lead toq{\\\\displaystyle q}.The whole procedure is described by the following pseudo-code:function add_letter(x):define p = lastassign last = new_state()assign len(last) = len(p) + 1while (p, x) is undefined:assign (p, x) = last, p = link(p)define q = (p, x)if q = last:assign link(last) = q0else if len(q) = len(p) + 1:assign link(last) = qelse:define cl = new_state()assign len(cl) = len(p) + 1assign (cl) = (q), link(cl) = link(q)assign link(last) = link(q) = clwhile (p, x) = q:assign (p, x) = cl, p = link(p)Hereq0{\\\\displaystyle q_{0}}is the initial state of the automaton andnew_state(){\\\\displaystyle new\\\\_state()}is a function creating new state for it. It is assumedlast{\\\\displaystyle last},len{\\\\displaystyle len},link{\\\\displaystyle link}and{\\\\displaystyle \\\\delta }are stored as global variables.ComplexityComplexity of the algorithm may vary depending on the underlying structure used to store transitions of the automaton. It may be implemented inO(nlog\\u2061||){\\\\displaystyle O(n\\\\log |\\\\Sigma |)}withO(n){\\\\displaystyle O(n)}memory overhead or inO(n){\\\\displaystyle O(n)}withO(n||){\\\\displaystyle O(n|\\\\Sigma |)}memory overhead if one assumes that memory allocation is done inO(1){\\\\displaystyle O(1)}. To obtain such complexity, one has to use the methods of amortized analysis. The value oflen(p){\\\\displaystyle len(p)}strictly reduces with each iteration of the cycle while it may only increase by as much as one after the first iteration of the cycle on the next add_letter call. Overall value oflen(p){\\\\displaystyle len(p)}never exceedsn{\\\\displaystyle n}and it is only increased by one between iterations of appending new letters that suggest total complexity is at most linear as well. The linearity of the second cycle is shown in a similar way.GeneralizationsThe suffix automaton is closely related to other suffix structures and substring indices. Given a suffix automaton of a specific string one may construct its suffix tree via compacting and recursive traversal in linear time. Similar transforms are possible in both directions to switch between the suffix automaton ofS{\\\\displaystyle S}and the suffix tree of reversed stringSR{\\\\displaystyle S^{R}}. Other than this several generalizations were developed to construct an automaton for the set of strings given by trie, compacted suffix automation (CDAWG), to maintain the structure of the automaton on the sliding window, and to construct it in a bidirectional way, supporting the insertion of a characters to both the beginning and the end of the string.Compacted suffix automatonAs was already mentioned above, a compacted suffix automaton is obtained via both compaction of a regular suffix automaton (by removing states which are non-final and have exactly one out-going arc) and the minimization of a suffix tree. Similarly to the regular suffix automaton, states of compacted suffix automaton may be defined in explicit manner. A two-way extension{\\\\displaystyle {\\\\overset {\\\\scriptstyle {\\\\longleftrightarrow }}{\\\\gamma }}}of a word{\\\\displaystyle \\\\gamma }is the longest word={\\\\displaystyle \\\\omega =\\\\beta \\\\gamma \\\\alpha }, such that every occurrence of{\\\\displaystyle \\\\gamma }inS{\\\\displaystyle S}is preceded by{\\\\displaystyle \\\\beta }and succeeded by{\\\\displaystyle \\\\alpha }. In terms of left and right extensions it means that two-way extension is the left extension of the right extension or, which is equivalent, the right extension of the left extension, that is=={\\\\textstyle {\\\\overset {\\\\scriptstyle \\\\longleftrightarrow }{\\\\gamma }}={\\\\overset {\\\\scriptstyle \\\\leftarrow }{\\\\overset {\\\\rightarrow }{\\\\gamma }}}={\\\\overset {\\\\rightarrow }{\\\\overset {\\\\scriptstyle \\\\leftarrow }{\\\\gamma }}}}. In terms of two-way extensions compacted automaton is defined as follows:Two-way extensions induce an equivalence relation={\\\\textstyle {\\\\overset {\\\\scriptstyle \\\\longleftrightarrow }{\\\\alpha }}={\\\\overset {\\\\scriptstyle \\\\longleftrightarrow }{\\\\beta }}}which defines the set of words recognized by the same state of compacted automaton. This equivalence relation is a transitive closure of the relation defined by(=)(=){\\\\textstyle ({\\\\overset {\\\\scriptstyle {\\\\rightarrow }}{\\\\alpha \\\\,}}={\\\\overset {\\\\scriptstyle {\\\\rightarrow }}{\\\\beta \\\\,}})\\\\vee ({\\\\overset {\\\\scriptstyle {\\\\leftarrow }}{\\\\alpha }}={\\\\overset {\\\\scriptstyle {\\\\leftarrow }}{\\\\beta }})}, which highlights the fact that a compacted automaton may be obtained by both gluing suffix tree vertices equivalent via={\\\\displaystyle {\\\\overset {\\\\scriptstyle {\\\\leftarrow }}{\\\\alpha }}={\\\\overset {\\\\scriptstyle {\\\\leftarrow }}{\\\\beta }}}relation (minimization of the suffix tree) and gluing suffix automaton states equivalent via={\\\\displaystyle {\\\\overset {\\\\scriptstyle {\\\\rightarrow }}{\\\\alpha \\\\,}}={\\\\overset {\\\\scriptstyle {\\\\rightarrow }}{\\\\beta \\\\,}}}relation (compaction of suffix automaton). If words{\\\\displaystyle \\\\alpha }and{\\\\displaystyle \\\\beta }have same right extensions, and words{\\\\displaystyle \\\\beta }and{\\\\displaystyle \\\\gamma }have same left extensions, then cumulatively all strings{\\\\displaystyle \\\\alpha },{\\\\displaystyle \\\\beta }and{\\\\displaystyle \\\\gamma }have same two-way extensions. At the same time it may happen that neither left nor right extensions of{\\\\displaystyle \\\\alpha }and{\\\\displaystyle \\\\gamma }coincide. As an example one may takeS==ab{\\\\displaystyle S=\\\\beta =ab},=a{\\\\displaystyle \\\\alpha =a}and=b{\\\\displaystyle \\\\gamma =b}, for which left and right extensions are as follows:==ab=={\\\\displaystyle {\\\\overset {\\\\scriptstyle {\\\\rightarrow }}{\\\\alpha \\\\,}}={\\\\overset {\\\\scriptstyle {\\\\rightarrow }}{\\\\beta \\\\,}}=ab={\\\\overset {\\\\scriptstyle {\\\\leftarrow }}{\\\\beta }}={\\\\overset {\\\\scriptstyle {\\\\leftarrow }}{\\\\gamma }}}, but=b{\\\\displaystyle {\\\\overset {\\\\scriptstyle {\\\\rightarrow }}{\\\\gamma \\\\,}}=b}and=a{\\\\displaystyle {\\\\overset {\\\\scriptstyle {\\\\leftarrow }}{\\\\alpha }}=a}. That being said, while equivalence relations of one-way extensions were formed by some continuous chain of nested prefixes or suffixes, bidirectional extensions equivalence relations are more complex and the only thing one may conclude for sure is that strings with the same two-way extension are substrings of the longest string having the same two-way extension, but it may even happen that they don\\'t have any non-empty substring in common. The total number of equivalence classes for this relation does not exceedn+1{\\\\displaystyle n+1}which implies that compacted suffix automaton of the string having lengthn{\\\\displaystyle n}has at mostn+1{\\\\displaystyle n+1}states. The amount of transitions in such automaton is at most2n2{\\\\displaystyle 2n-2}.Suffix automaton of several stringsConsider a set of wordsT={S1,S2,,Sk}{\\\\displaystyle T=\\\\{S_{1},S_{2},\\\\dots ,S_{k}\\\\}}. It is possible to construct a generalization of suffix automaton that would recognize the language formed up by suffixes of all words from the set. Constraints for the number of states and transitions in such automaton would stay the same as for a single-word automaton if you putn=|S1|+|S2|++|Sk|{\\\\displaystyle n=|S_{1}|+|S_{2}|+\\\\dots +|S_{k}|}. The algorithm is similar to the construction of single-word automaton except instead oflast{\\\\displaystyle last}state, function add_letter would work with the state corresponding to the wordi{\\\\displaystyle \\\\omega _{i}}assuming the transition from the set of words{1,,i,,k}{\\\\displaystyle \\\\{\\\\omega _{1},\\\\dots ,\\\\omega _{i},\\\\dots ,\\\\omega _{k}\\\\}}to the set{1,,ix,,k}{\\\\displaystyle \\\\{\\\\omega _{1},\\\\dots ,\\\\omega _{i}x,\\\\dots ,\\\\omega _{k}\\\\}}.This idea is further generalized to the case whenT{\\\\displaystyle T}is not given explicitly but instead is given by a prefix tree withQ{\\\\displaystyle Q}vertices. Mohri et al. showed such an automaton would have at most2Q2{\\\\displaystyle 2Q-2}and may be constructed in linear time from its size. At the same time, the number of transitions in such automaton may reachO(Q||){\\\\displaystyle O(Q|\\\\Sigma |)}, for example for the set of wordsT={1,a1,a21,,an1,an2,,ank}{\\\\displaystyle T=\\\\{\\\\sigma _{1},a\\\\sigma _{1},a^{2}\\\\sigma _{1},\\\\dots ,a^{n}\\\\sigma _{1},a^{n}\\\\sigma _{2},\\\\dots ,a^{n}\\\\sigma _{k}\\\\}}over the alphabet={a,1,,k}{\\\\displaystyle \\\\Sigma =\\\\{a,\\\\sigma _{1},\\\\dots ,\\\\sigma _{k}\\\\}}the total length of words is equal toO(n2+nk){\\\\textstyle O(n^{2}+nk)}, the number of vertices in corresponding suffix trie is equal toO(n+k){\\\\displaystyle O(n+k)}and corresponding suffix automaton is formed ofO(n+k){\\\\displaystyle O(n+k)}states andO(nk){\\\\displaystyle O(nk)}transitions. Algorithm suggested by Mohri mainly repeats the generic algorithm for building automaton of several strings but instead of growing words one by one, it traverses the trie in a breadth-first search order and append new characters as it meet them in the traversal, which guarantees amortized linear complexity.Sliding windowSome compression algorithms, such as LZ77 and RLE may benefit from storing suffix automaton or similar structure not for the whole string but for only lastk{\\\\displaystyle k}its characters while the string is updated. This is because compressing data is usually expressively large and usingO(n){\\\\displaystyle O(n)}memory is undesirable. In 1985, Janet Blumer developed an algorithm to maintain a suffix automaton on a sliding window of sizek{\\\\displaystyle k}inO(nk){\\\\displaystyle O(nk)}worst-case andO(nlog\\u2061k){\\\\displaystyle O(n\\\\log k)}on average, assuming characters are distributed independently and uniformly. She also showedO(nk){\\\\displaystyle O(nk)}complexity cannot be improved: if one considers words construed as a concatenation of several(ab)mc(ab)md{\\\\displaystyle (ab)^{m}c(ab)^{m}d}words, wherek=6m+2{\\\\displaystyle k=6m+2}, then the number of states for the window of sizek{\\\\displaystyle k}would frequently change with jumps of orderm{\\\\displaystyle m}, which renders even theoretical improvement ofO(nk){\\\\displaystyle O(nk)}for regular suffix automata impossible.The same should be true for the suffix tree because its vertices correspond to states of the suffix automaton of the reversed string but this problem may be resolved by not explicitly storing every vertex corresponding to the suffix of the whole string, thus only storing vertices with at least two out-going edges. A variation of McCreight\\'s suffix tree construction algorithm for this task was suggested in 1989 by Edward Fiala and Daniel Greene; several years later a similar result was obtained with the variation of Ukkonen\\'s algorithm by Jesper Larsson. The existence of such an algorithm, for compacted suffix automaton that absorbs some properties of both suffix trees and suffix automata, was an open question for a long time until it was discovered by Martin Senft and Tomasz Dvorak in 2008, that it is impossible if the alphabet\\'s size is at least two.One way to overcome this obstacle is to allow window width to vary a bit while stayingO(k){\\\\displaystyle O(k)}. It may be achieved by an approximate algorithm suggested by Inenaga et al. in 2004. The window for which suffix automaton is built in this algorithm is not guaranteed to be of lengthk{\\\\displaystyle k}but it is guaranteed to be at leastk{\\\\displaystyle k}and at most2k+1{\\\\displaystyle 2k+1}while providing linear overall complexity of the algorithm.ApplicationsSuffix automaton of the stringS{\\\\displaystyle S}may be used to solve such problems as:Counting the number of distinct substrings ofS{\\\\displaystyle S}inO(|S|){\\\\displaystyle O(|S|)}on-line,Finding the longest substring ofS{\\\\displaystyle S}occurring at least twice inO(|S|){\\\\displaystyle O(|S|)},Finding the longest common substring ofS{\\\\displaystyle S}andT{\\\\displaystyle T}inO(|T|){\\\\displaystyle O(|T|)},Counting the number of occurrences ofT{\\\\displaystyle T}inS{\\\\displaystyle S}inO(|T|){\\\\displaystyle O(|T|)},Finding all occurrences ofT{\\\\displaystyle T}inS{\\\\displaystyle S}inO(|T|+k){\\\\displaystyle O(|T|+k)}, wherek{\\\\displaystyle k}is the number of occurrences.It is assumed here thatT{\\\\displaystyle T}is given on the input after suffix automaton ofS{\\\\displaystyle S}is constructed.Suffix automata are also used in data compression, music retrieval and matching on genome sequences.ReferencesBibliographyExternal linksMedia related to Suffix automaton at Wikimedia CommonsSuffix automaton article on E-Maxx Algorithms in English',\n",
       " '= Radix sort =In computer science, radix sort is a non-comparative sorting algorithm. It avoids comparison by creating and distributing elements into buckets according to their radix. For elements with more than one significant digit, this bucketing process is repeated for each digit, while preserving the ordering of the prior step, until all digits have been considered. For this reason, radix sort has also been called bucket sort and digital sort.Radix sort can be applied to data that can be sorted lexicographically, be they integers, words, punch cards, playing cards, or the mail.HistoryRadix sort dates back as far as 1887 to the work of Herman Hollerith on tabulating machines. Radix sorting algorithms came into common use as a way to sort punched cards as early as 1923.The first memory-efficient computer algorithm for this sorting method was developed in 1954 at MIT by Harold H. Seward. Computerized radix sorts had previously been dismissed as impractical because of the perceived need for variable allocation of buckets of unknown size. Seward\\'s innovation was to use a linear scan to determine the required bucket sizes and offsets beforehand, allowing for a single static allocation of auxiliary memory. The linear scan is closely related to Seward\\'s other algorithm  counting sort.In the modern era, radix sorts are most commonly applied to collections of binary strings and integers. It has been shown in some benchmarks to be faster than other more general-purpose sorting algorithms, sometimes 50% to three times faster.Digit orderRadix sorts can be implemented to start at either the most significant digit (MSD) or least significant digit (LSD). For example, with 1234, one could start with 1 (MSD) or 4 (LSD).LSD radix sorts typically use the following sorting order: short keys come before longer keys, and then keys of the same length are sorted lexicographically. This coincides with the normal order of integer representations, like the sequence [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]. LSD sorts are generally stable sorts.MSD radix sorts are most suitable for sorting strings or fixed-length integer representations. A sequence like [b, c, e, d, f, g, ba] would be sorted as [b, ba, c, d, e, f, g]. If lexicographic ordering is used to sort variable-length integers in base 10, then numbers from 1 to 10 would be output as [1, 10, 2, 3, 4, 5, 6, 7, 8, 9], as if the shorter keys were left-justified and padded on the right with blank characters to make the shorter keys as long as the longest key. MSD sorts are not necessarily stable if the original ordering of duplicate keys must always be maintained.Other than the traversal order, MSD and LSD sorts differ in their handling of variable length input.LSD sorts can group by length, radix sort each group, then concatenate the groups in size order. MSD sorts must effectively \\'extend\\' all shorter keys to the size of the largest key and sort them accordingly, which can be more complicated than the grouping required by LSD.However, MSD sorts are more amenable to subdivision and recursion. Each bucket created by an MSD step can itself be radix sorted using the next most significant digit, without reference to any other buckets created in the previous step. Once the last digit is reached, concatenating the buckets is all that is required to complete the sort.ExamplesLeast significant digitInput list:[170, 45, 75, 90, 2, 802, 2, 66]Starting from the rightmost (last) digit, sort the numbers based on that digit:[{170, 90}, {2, 802, 2}, {45, 75}, {66}]Sorting by the next left digit:[{02, 802, 02}, {45}, {66}, {170, 75}, {90}]Notice that an implicit digit 0 is prepended for the two 2s so that 802 maintains its position between them.And finally by the leftmost digit:[{002, 002, 045, 066, 075, 090}, {170}, {802}]Notice that a 0 is prepended to all of the 1- or 2-digit numbers.Each step requires just a single pass over the data, since each item can be placed in its bucket without comparison with any other element.Some radix sort implementations allocate space for buckets by first counting the number of keys that belong in each bucket before moving keys into those buckets. The number of times that each digit occurs is stored in an array.Although it is always possible to pre-determine the bucket boundaries using counts, some implementations opt to use dynamic memory allocation instead.Most significant digit, forward recursiveInput list, fixed width numeric strings with leading zeros:[170, 045, 075, 025, 002, 024, 802, 066]First digit, with brackets indicating buckets:[{045, 075, 025, 002, 024, 066}, {170}, {802}]Notice that 170 and 802 are already complete because they are all that remain in their buckets, so no further recursion is neededNext digit:[{ {002}, {025, 024}, {045}, {066}, {075} }, 170, 802]Final digit:[ 002, { {024}, {025} }, 045, 066, 075 , 170, 802]All that remains is concatenation:[002, 024, 025, 045, 066, 075, 170, 802]Complexity and performanceRadix sort operates in O(nw) time, where n is the number of keys, and w is the key length. LSD variants can achieve a lower bound for w of \\'average key length\\' when splitting variable length keys into groups as discussed above.Optimized radix sorts can be very fast when working in a domain that suits them.They are constrained to lexicographic data, but for many practical applications this is not a limitation. Large key sizes can hinder LSD implementations when the induced number of passes becomes the bottleneck.Specialized variantsIn-place MSD radix sort implementationsBinary MSD radix sort, also called binary quicksort, can be implemented in-place by splitting the input array into two bins - the 0s bin and the 1s bin. The 0s bin is grown from the beginning of the array, whereas the 1s bin is grown from the end of the array. The 0s bin boundary is placed before the first array element. The 1s bin boundary is placed after the last array element. The most significant bit of the first array element is examined. If this bit is a 1, then the first element is swapped with the element in front of the 1s bin boundary (the last element of the array), and the 1s bin is grown by one element by decrementing the 1s boundary array index. If this bit is a 0, then the first element remains at its current location, and the 0s bin is grown by one element. The next array element examined is the one in front of the 0s bin boundary (i.e. the first element that is not in the 0s bin or the 1s bin). This process continues until the 0s bin and the 1s bin reach each other. The 0s bin and the 1s bin are then sorted recursively based on the next bit of each array element. Recursive processing continues until the least significant bit has been used for sorting. Handling signed integers requires treating the most significant bit with the opposite sense, followed by unsigned treatment of the rest of the bits.In-place MSD binary-radix sort can be extended to larger radix and retain in-place capability. Counting sort is used to determine the size of each bin and their starting index. Swapping is used to place the current element into its bin, followed by expanding the bin boundary. As the array elements are scanned the bins are skipped over and only elements between bins are processed, until the entire array has been processed and all elements end up in their respective bins. The number of bins is the same as the radix used - e.g. 16 bins for 16-radix. Each pass is based on a single digit (e.g. 4-bits per digit in the case of 16-radix), starting from the most significant digit. Each bin is then processed recursively using the next digit, until all digits have been used for sorting.Neither in-place binary-radix sort nor n-bit-radix sort, discussed in paragraphs above, are stable algorithms.Stable MSD radix sort implementationsMSD radix sort can be implemented as a stable algorithm, but requires the use of a memory buffer of the same size as the input array. This extra memory allows the input buffer to be scanned from the first array element to last, and move the array elements to the destination bins in the same order. Thus, equal elements will be placed in the memory buffer in the same order they were in the input array. The MSD-based algorithm uses the extra memory buffer as the output on the first level of recursion, but swaps the input and output on the next level of recursion, to avoid the overhead of copying the output result back to the input buffer. Each of the bins are recursively processed, as is done for the in-place MSD radix sort. After the sort by the last digit has been completed, the output buffer is checked to see if it is the original input array, and if it\\'s not, then a single copy is performed. If the digit size is chosen such that the key size divided by the digit size is an even number, the copy at the end is avoided.Hybrid approachesRadix sort, such as the two-pass method where counting sort is used during the first pass of each level of recursion, has a large constant overhead. Thus, when the bins get small, other sorting algorithms should be used, such as insertion sort. A good implementation of insertion sort is fast for small arrays, stable, in-place, and can significantly speed up radix sort.Application to parallel computingThis recursive sorting algorithm has particular application to parallel computing, as each of the bins can be sorted independently. In this case, each bin is passed to the next available processor. A single processor would be used at the start (the most significant digit). By the second or third digit, all available processors would likely be engaged. Ideally, as each subdivision is fully sorted, fewer and fewer processors would be utilized. In the worst case, all of the keys will be identical or nearly identical to each other, with the result that there will be little to no advantage to using parallel computing to sort the keys.In the top level of recursion, opportunity for parallelism is in the counting sort portion of the algorithm. Counting is highly parallel, amenable to the parallel_reduce pattern, and splits the work well across multiple cores until reaching memory bandwidth limit. This portion of the algorithm has data-independent parallelism. Processing each bin in subsequent recursion levels is data-dependent, however. For example, if all keys were of the same value, then there would be only a single bin with any elements in it, and no parallelism would be available. For random inputs all bins would be near equally populated and a large amount of parallelism opportunity would be available.There are faster parallel sorting algorithms available, for example optimal complexity O(log(n)) are those of the Three Hungarians and Richard Cole and Batcher\\'s bitonic merge sort has an algorithmic complexity of O(log2(n)), all of which have a lower algorithmic time complexity to radix sort on a CREW-PRAM. The fastest known PRAM sorts were described in 1991 by David Powers with a parallelized quicksort that can operate in O(log(n)) time on a CRCW-PRAM with n processors by performing partitioning implicitly, as well as a radixsort that operates using the same trick in O(k), where k is the maximum keylength. However, neither the PRAM architecture or a single sequential processor can actually be built in a way that will scale without the number of constant fan-out gate delays per cycle increasing as O(log(n)), so that in effect a pipelined version of Batcher\\'s bitonic mergesort and the O(log(n)) PRAM sorts are all O(log2(n)) in terms of clock cycles, with Powers acknowledging that Batcher\\'s would have lower constant in terms of gate delays than his Parallel quicksort and radix sort, or Cole\\'s merge sort, for a keylength-independent sorting network of O(nlog2(n)).Tree-based radix sortRadix sorting can also be accomplished by building a tree (or radix tree) from the input set, and doing a pre-order traversal. This is similar to the relationship between heapsort and the heap data structure. This can be useful for certain data types, see burstsort.See alsoIBM 80 series Card SortersOther distribution sortsKirkpatrick-Reisch sortingPrefix sumReferencesExternal linksExplanation, Pseudocode and implementation in C and JavaHigh Performance Implementation of LSD Radix sort in JavaScriptHigh Performance Implementation of LSD & MSD Radix sort in C# with source in GitHubVideo tutorial of MSD Radix SortDemonstration and comparison of Radix sort with Bubble sort, Merge sort and Quicksort implemented in JavaScriptArticle about Radix sorting IEEE floating-point numbers with implementation.Faster Floating Point Sorting and Multiple Histogramming with implementation in C++Pointers to radix sort visualizationsUSort library contains tuned implementations of radix sort for most numerical C types (C99)Donald Knuth. The Art of Computer Programming, Volume 3: Sorting and Searching, Third Edition. Addison-Wesley, 1997. ISBN 0-201-89685-0. Section 5.2.5: Sorting by Distribution, pp. 168179.Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to Algorithms, Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7. Section 8.3: Radix sort, pp. 170173.BRADSORT v1.50 source codeEfficient Trie-Based Sorting of Large Sets of Strings, by Ranjan Sinha and Justin Zobel. This paper describes a method of creating tries of buckets which figuratively burst into sub-tries when the buckets hold more than a predetermined capacity of strings, hence the name, \"Burstsort\".Open Data Structures - Java Edition - Section 11.2 - Counting Sort and Radix Sort, Pat MorinOpen Data Structures - C++ Edition - Section 11.2 - Counting Sort and Radix Sort, Pat Morin',\n",
       " '= 1-planar graph =In topological graph theory, a 1-planar graph is a graph that can be drawn in the Euclidean plane in such a way that each edge has at most one crossing point, where it crosses a single additional edge. If a 1-planar graph, one of the most natural generalizations of planar graphs, is drawn that way, the drawing is called a 1-plane graph or 1-planar embedding of the graph.Coloring1-planar graphs were first studied by Ringel (1965), who showed that they can be colored with at most seven colors. Later, the precise number of colors needed to color these graphs, in the worst case, was shown to be six. The example of the complete graph K6, which is 1-planar, shows that 1-planar graphs may sometimes require six colors. However, the proof that six colors are always enough is more complicated.Ringel\\'s motivation was in trying to solve a variation of total coloring for planar graphs, in which one simultaneously colors the vertices and faces of a planar graph in such a way that no two adjacent vertices have the same color, no two adjacent faces have the same color, and no vertex and face that are adjacent to each other have the same color. This can obviously be done using eight colors by applying the four color theorem to the given graph and its dual graph separately, using two disjoint sets of four colors. However, fewer colors may be obtained by forming an auxiliary graph that has a vertex for each vertex or face of the given planar graph, and in which two auxiliary graph vertices are adjacent whenever they correspond to adjacent features of the given planar graph. A vertex coloring of the auxiliary graph corresponds to a vertex-face coloring of the original planar graph. This auxiliary graph is 1-planar, from which it follows that Ringel\\'s vertex-face coloring problem may also be solved with six colors. The graph K6 cannot be formed as an auxiliary graph in this way, but nevertheless the vertex-face coloring problem also sometimes requires six colors; for instance, if the planar graph to be colored is a triangular prism, then its eleven vertices and faces require six colors, because no three of them may be given a single color.Edge densityEvery 1-planar graph with n vertices has at most 4n  8 edges. More strongly, each 1-planar drawing has at most n  2 crossings; removing one edge from each crossing pair of edges leaves a planar graph, which can have at most 3n  6 edges, from which the 4n  8 bound on the number of edges in the original 1-planar graph immediately follows.  However, unlike planar graphs (for which all maximal planar graphs on a given vertex set have the same number of edges as each other), there exist maximal 1-planar graphs (graphs to which no additional edges can be added while preserving 1-planarity) that have significantly fewer than 4n  8 edges. The bound of 4n  8 on the maximum possible number of edges in a 1-planar graph can be used to show that the complete graph K7 on seven vertices is not 1-planar, because this graph has 21 edges and in this case 4n  8 = 20 < 21.A 1-planar graph is said to be an optimal 1-planar graph if it has exactly 4n  8 edges, the maximum possible. In a 1-planar embedding of an optimal 1-planar graph, the uncrossed edges necessarily form a quadrangulation (a polyhedral graph in which every face is a quadrilateral). Every quadrangulation gives rise to an optimal 1-planar graph in this way, by adding the two diagonals to each of its quadrilateral faces. It follows that every optimal 1-planar graph is Eulerian (all of its vertices have even degree), that the minimum degree in such a graph is six, and that every optimal 1-planar graph has at least eight vertices of degree exactly six. Additionally, every optimal 1-planar graph is 4-vertex-connected, and every 4-vertex cut in such a graph is a separating cycle in the underlying quadrangulation.The graphs that have straight 1-planar drawings (that is, drawings in which each edge is represented by a line segment, and in which each line segment is crossed by at most one other edge) have a slightly tighter bound of 4n  9 on the maximum number of edges, achieved by infinitely many graphs.Complete multipartite graphsA complete classification of the 1-planar complete graphs, complete bipartite graphs, and more generally complete multipartite graphs is known. Every complete bipartite graph of the form K2,n is 1-planar, as is every complete tripartite graph of the form K1,1,n. Other than these infinite sets of examples, the only complete multipartite 1-planar graphs are K6, K1,1,1,6, K1,1,2,3, K2,2,2,2, K1,1,1,2,2, and their subgraphs. The minimal non-1-planar complete multipartite graphs are K3,7, K4,5, K1,3,4, K2,3,3, and K1,1,1,1,3.For instance, the complete bipartite graph K3,6 is 1-planar because it is a subgraph of K1,1,1,6, but K3,7 is not 1-planar.Computational complexityIt is NP-complete to test whether a given graph is 1-planar, and it remains NP-complete even for the graphs formed from planar graphs by adding a single edge and for graphs of bounded bandwidth. The problem is fixed-parameter tractable when parameterized by cyclomatic number or by tree-depth, so it may be solved in polynomial time when those parameters are bounded.In contrast to Fry\\'s theorem for planar graphs, not every 1-planar graph may be drawn 1-planarly with straight line segments for its edges. However, testing whether a 1-planar drawing may be straightened in this way can be done in polynomial time. Additionally, every 3-vertex-connected 1-planar graph has a 1-planar drawing in which at most one edge, on the outer face of the drawing, has a bend in it. This drawing can be constructed in linear time from a 1-planar embedding of the graph. The 1-planar graphs have bounded book thickness, but some 1-planar graphs including K2,2,2,2 have book thickness at least four.1-planar graphs have bounded local treewidth, meaning that there is a (linear) function f such that the 1-planar graphs of diameter d have treewidth at most f(d); the same property holds more generally for the graphs that can be embedded onto a surface of bounded genus with a bounded number of crossings per edge. They also have separators, small sets of vertices the removal of which decomposes the graph into connected components whose size is a constant fraction of the size of the whole graph. Based on these properties, numerous algorithms for planar graphs, such as Baker\\'s technique for designing approximation algorithms, can be extended to 1-planar graphs. For instance, this method leads to a polynomial-time approximation scheme for the maximum independent set of a 1-planar graph.Generalizations and related conceptsThe class of graphs analogous to outerplanar graphs for 1-planarity are called the outer-1-planar graphs. These are graphs that can be drawn in a disk, with the vertices on the boundary of the disk, and with at most one crossing per edge. These graphs can always be drawn (in an outer-1-planar way) with straight edges and right angle crossings. By using dynamic programming on the SPQR tree of a given graph, it is possible to test whether it is outer-1-planar in linear time. The triconnected components of the graph (nodes of the SPQR tree) can consist only of cycle graphs, bond graphs, and four-vertex complete graphs, from which it also follows that outer-1-planar graphs are planar and have treewidth at most three.The 1-planar graphs include the 4-map graphs, graphs formed from the adjacencies of regions in the plane with at most four regions meeting in any point. Conversely, every optimal 1-planar graph is a 4-map graph. However, 1-planar graphs that are not optimal 1-planar may not be map graphs.1-planar graphs have been generalized to k-planar graphs, graphs for which each edge is crossed at most k times (0-planar graphs are exactly the planar graphs). Ringel defined the local crossing number of G to be the least non-negative integer k such that G has a k-planar drawing. Because the local crossing number is the maximum degree of the intersection graph of the edges of an optimal drawing, and the thickness (minimum number of planar graphs into which the edges can be partitioned) can be seen as the chromatic number of an intersection graph of an appropriate drawing, it follows from Brooks\\' theorem that the thickness is at most one plus the local crossing number. The k-planar graphs with n vertices have at most O(k1/2n) edges, and treewidth O((kn)1/2). A shallow minor of a k-planar graph, with depth d, is itself a (2d + 1)k-planar graph, so the shallow minors of 1-planar graphs and of k-planar graphs are also sparse graphs, implying that the 1-planar and k-planar graphs have bounded expansion.Nonplanar graphs may also be parameterized by their crossing number, the minimum number of pairs of edges that cross in any drawing of the graph. A graph with crossing number k is necessarily k-planar, but not necessarily vice versa. For instance, the Heawood graph has crossing number 3, but it is not necessary for its three crossings to all occur on the same edge of the graph, so it is 1-planar, and can in fact be drawn in a way that simultaneously optimizes the total number of crossings and the crossings per edge.Another related concept for nonplanar graphs is graph skewness, the minimal number of edges that must be removed to make a graph planar.ReferencesFurther readingKobourov, Stephen; Liotta, Giuseppe; Montecchiani, Fabrizio (2017), \"An annotated bibliography on 1-planarity\", Computer Science Review, 25: 4967, arXiv:1703.02261, Bibcode:2017arXiv170302261K, doi:10.1016/j.cosrev.2017.06.002, S2CID 7732463',\n",
       " '= 26-fullerene graph =In the mathematical field of graph theory, the 26-fullerene graph is a polyhedral graph with V = 26 vertices and E = 39 edges. Its planar embedding has three hexagonal faces (including the one shown as the external face of the illustration) and twelve pentagonal faces. As a planar graph with only pentagonal and hexagonal faces, meeting in three faces per vertex, this graph is a fullerene. The existence of this fullerene has been known since at least 1968.PropertiesThe 26-fullerene graph hasD3h{\\\\displaystyle D_{3h}}prismatic symmetry, the same group of symmetries as the triangular prism. This symmetry group has 12 elements; it has six symmetries that arbitrarily permute the three hexagonal faces of the graph and preserve the orientation of its planar embedding, and another six orientation-reversing symmetries.The number of fullerenes with a given even number of vertices grows quickly in the number of vertices; 26 is the largest number of vertices for which the fullerene structure is unique. The only two smaller fullerenes are the graph of the regular dodecahedron (a fullerene with 20 vertices) and the graph of the truncated hexagonal trapezohedron (a 24-vertex fullerene), which are the two types of cells in the WeairePhelan structure.The 26-fullerene graph has many perfect matchings. One must remove at least five edges from the graph in order to obtain a subgraph that has exactly one perfect matching. This is a unique property of this graph among fullerenes in the sense that, for every other number of vertices of a fullerene, there exists at least one fullerene from which one can remove four edges to obtain a subgraph with a unique perfect matching.The vertices of the 26-fullerene graph can be labeled with sequences of 12 bits, in such a way that distance in the graph equals half of the Hamming distance between these bitvectors.This can also be interpreted as an isometric embedding from the graph into a 12-dimensional taxicab geometry. The 26-fullerene graph is one of only five fullerenes with such an embedding.In popular cultureIn 2009, The New York Times published a puzzle involving Hamiltonian paths in this graph, taking advantage of the correspondence between its 26 vertices and the 26 letters of the English alphabet.== References ==',\n",
       " '= 3-Way =In cryptography, 3-Way is a block cipher designed in 1994 by Joan Daemen. It is closely related to BaseKing; the two are variants of the same general cipher technique.3-Way has a block size of 96 bits, notably not a power of two such as the more common 64 or 128 bits. The key length is also 96 bits. The figure 96 arises from the use of three 32 bit words in the algorithm, from which also is derived the cipher\\'s name. When 3-Way was invented, 96-bit keys and blocks were quite strong, but more recent ciphers have a 128-bit block, and few now have keys shorter than 128 bits. 3-Way is an 11-round substitutionpermutation network.3-Way is designed to be very efficient in a wide range of platforms from 8-bit processors to specialized hardware, and has some elegant mathematical features which enable nearly all the decryption to be done in exactly the same circuits as did the encryption.3-Way, just as its counterpart BaseKing, is vulnerable to related key cryptanalysis. John Kelsey, Bruce Schneier, and David Wagner showed how it can be broken with one related key query and about222{\\\\displaystyle 2^{22}}chosen plaintexts.ReferencesJ. Daemen; R. Govaerts; Joos Vandewalle (1993). \"A New Approach to Block Cipher Design\". Fast Software Encryption (FSE) 1993. Springer-Verlag. pp. 1832.J. Kelsey; B. Schneier; D. Wagner (November 1997). \"Related-Key Cryptanalysis of 3-WAY, Biham-DES, CAST, DES-X, NewDES, RC2, and TEA\" (PDF/PostScript). ICICS \\'97 Proceedings. Springer-Verlag. pp. 233246. Retrieved 2007-02-14.External linksSCAN\\'s entry for 3-WayChapter 7 of Daemen\\'s thesis (gzipped Postscript)',\n",
       " '= 3D Topicscape =3D Topicscape, a software application, is a Personal Information Manager that provides a template loosely based on mind-mapping  or concept mapping.  It presents the mind map as a 3D scene where each node is a cone (or pyramid, or variation on such a shape).  It can also display in a 2D format.  Nodes are arranged in a way that indicates how they are related in much the same way as a mind map.  In addition to its use for information management it is claimed to be suitable as a task manager, and for use in project management.A Topicscape is created by importing folders (by Drag-and-drop or menus), importing from other mind mapping software including FreeMind, PersonalBrain and MindManager or by hand with mouse clicks or keyboard shortcuts.  Import sources may be converted to a new Topicscape or added as a portion of an existing one.The number of levels that can be stored is not limited, but up to seven levels of the hierarchy may be viewed at once.  Any node may be chosen as the centre of the 3D scene and choosing one at the edge will cause more to come into view.Topicscape\\'s most obvious difference from 2D mind mapping software is that it provides a zooming interface and simulates flying as noted by Wall Street Journal columnist Jeremy Wagstaff in his column \"Fly through your computer.\"  The BBC World Service and PC World  have also reviewed 3D Topicscape.Versions3D Topicscape public Beta in Jan 20063D Topicscape 1.0, May 20063D Topicscape Lite 1.05; 1.07, Dec 2007; 1.2, Aug 20083D Topicscape Pro 1.2, Feb 2007; 1.3, May 2007; 1.56, Dec 2007; 1.59, May 2008; 1.6, Jul 2008;  1.63, Sep 2008; 2.0, Apr 2009; 2.5 Dec 2009;  2.6 Feb 2010; 2.7 April 20103D Topicscape Student Edition Beta, Sep 2007; 1.0, Feb 2008; 2.0, Dec 2009File FormatUses an embedded Firebird relational database to store user-provided and operational metadata.  Files attached to nodes (topics) may be linked to in their original location or be held in a folder (directory) associated with a given Topicscape.  Links to files in a Topicscape\\'s folder are relative.  Topicscape folders may therefore be moved without breaking such links.Import file formats supported include FreeMind, OML, MindManager versions 5-8, PersonalBrain, and text (outline-numbered);Export file formats can be those for FreeMind, OPML, HTML and text structured for re-import, or text for reading.See alsoBrainstormingList of concept- and mind-mapping softwarePersonal information managersMind map== References ==']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = input()\n",
    "do_search(s)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "search_engine_ok.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "05f41805986645f4902bbe6799542d78": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "19df947696994810b806dfde8f3969f4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "335ad58438124feca452795d8db25e05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "35987165f9bb495693d0623513d16e76": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "46165965826b46f88f58c0250bfdda9c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6d5b01aa4383457d8edf67ea93ce5036",
       "IPY_MODEL_48dd34655649462c9386ffd71662ad56",
       "IPY_MODEL_f3beb585873646888beb8a8c930295f3"
      ],
      "layout": "IPY_MODEL_05f41805986645f4902bbe6799542d78"
     }
    },
    "48dd34655649462c9386ffd71662ad56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_79b4d4b2dc6742fd8da1b4b0d5449dd8",
      "max": 1047,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_35987165f9bb495693d0623513d16e76",
      "value": 1047
     }
    },
    "6d5b01aa4383457d8edf67ea93ce5036": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a7302dd57f914c55801ff49bf2d0a55a",
      "placeholder": "",
      "style": "IPY_MODEL_335ad58438124feca452795d8db25e05",
      "value": "100%"
     }
    },
    "79b4d4b2dc6742fd8da1b4b0d5449dd8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "90ee46515b75466e9bd0b824bb9a6ae4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a7302dd57f914c55801ff49bf2d0a55a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f3beb585873646888beb8a8c930295f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_19df947696994810b806dfde8f3969f4",
      "placeholder": "",
      "style": "IPY_MODEL_90ee46515b75466e9bd0b824bb9a6ae4",
      "value": " 1047/1047 [00:00&lt;00:00, 1452.33it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
