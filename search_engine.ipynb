{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "0CqJ8NLuTokR"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "from pprint import pprint\n",
        "from glob import glob\n",
        "import random\n",
        "import spacy\n",
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.spatial.distance import cosine\n",
        "import requests\n",
        "from sklearn.neighbors import KDTree"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip wiki_data"
      ],
      "metadata": {
        "id": "PyYekdtAYXuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1q9iK9FIbiiy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "2628e75144484be498372d8828f8e028",
            "575301a88cb241fe82fc023695676442",
            "59388f2165f94581a1e8274831770d00",
            "7df9b3b5a8f94e729e81c522dbd8b568",
            "2386b75f321a4f749a74fa1af8d9a280",
            "d4f56127b816442a9231b5cd64081375",
            "875908790ff84dec91e42e70ca2e7afe",
            "ca336ba0f7e348a993d451e37cd8e562",
            "5bdbc33f0e09463c98497b287347aaa8",
            "f016aec1ed3040469bd74b0bcc4a29c4",
            "2b11449a0d9340faaf7f76e893ffc208"
          ]
        },
        "outputId": "fb66b971-65b0-4090-c198-14ad5d629627"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1047 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2628e75144484be498372d8828f8e028"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1047,\n",
              " '= SFINKS =Sfinks (Polish for \"Sphynx\") was also the initial name of the Janusz A. Zajdel AwardIn cryptography, SFINKS is a stream cypher algorithm developed by An Braeken, Joseph Lano, Nele Mentens, Bart Preneel, and Ingrid Verbauwhede. It includes a message authentication code. It has been submitted to the eSTREAM Project of the eCRYPT network.')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "documents = []\n",
        "\n",
        "for fname in tqdm(glob('wiki_data/texts*.txt')):\n",
        "    with open(fname) as f:\n",
        "        document = \"\"\n",
        "        for line in f:\n",
        "            document = document + line.strip()\n",
        "        \n",
        "        documents.append(document)\n",
        "\n",
        "len(documents), documents[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model"
      ],
      "metadata": {
        "id": "NZivmL96lbiv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "c-wjC5unVQoQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e04675db-b425-48a5-96ab-acebc6139825"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "nlp = spacy.load(\"en\", disable=[\"parser\", \"ner\", \"tagger\"])\n",
        "\n",
        "def spacy_tokenizer(text):\n",
        "    return [t.lemma_ for t in nlp(text)]\n",
        "\n",
        "vec = TfidfVectorizer(tokenizer=spacy_tokenizer)\n",
        "trained_vectors = vec.fit_transform(documents).todense()\n",
        "tree = KDTree(trained_vectors)\n",
        "texts = [[document, vector] for document, vector in zip(documents, trained_vectors)]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = vec.vocabulary_"
      ],
      "metadata": {
        "id": "nB0upILImaN_"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing the cache"
      ],
      "metadata": {
        "id": "zesdyejfXyy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer = dict()"
      ],
      "metadata": {
        "id": "z6W-H8wZXx53"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Searching"
      ],
      "metadata": {
        "id": "KVQKy5G7WJTh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "S-Udd4uQxtvz"
      },
      "outputs": [],
      "source": [
        "def lcp(a: str, b:str):\n",
        "    '''\n",
        "    Computes the longest common prefix of the strings `a` and `b`\n",
        "    '''\n",
        "    lcp = 0\n",
        "    while lcp < len(a) and lcp < len(b) and a[lcp] == b[lcp]:\n",
        "        lcp += 1\n",
        "    return lcp\n",
        "\n",
        "def most_similar(text: str):\n",
        "    '''\n",
        "    Returns the most similar word to `text` in the training vocabulary \n",
        "    by using Edit Distance* (ED from now on) and Longest Common Prefix\n",
        "    (LCP from now).\n",
        "\n",
        "    In general:\n",
        "    - the lower the ED, the more similar the words are.\n",
        "    - the greater the LCP, the more similar the words are.\n",
        "\n",
        "    Assumption:\n",
        "    - Typos are more likely to happen in the middle and the end of the words.\n",
        "    That's why the LCP plays a major role in comparing the similarity of words.\n",
        "\n",
        "    Final Similarity criteria**:\n",
        "    - the similarity of two words `a` and `b` is ED(a, b) / exp(LCP(a, b))\n",
        "    \n",
        "\n",
        "    * Edit Distance is also called Levenshtein Distance\n",
        "    ** It might change later\n",
        "    '''\n",
        "\n",
        "    result = min([[nltk.edit_distance(word, text) / np.exp(lcp(word, text)), word] for word in all_words]) # [similarity, resulting_word]\n",
        "        \n",
        "    return result[1]\n",
        "\n",
        "def do_search(text: str):\n",
        "    text = text.lower()\n",
        "    if text not in all_words:\n",
        "        text = most_similar(text)\n",
        "    if text not in answer: # caching the result for this text\n",
        "        new_vector = vec.transform([text]).todense()\n",
        "        dist, ind = tree.query(new_vector, k = 10)\n",
        "        answer[text] = [documents[i] for i in list(ind[0])]\n",
        "    return answer[text]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "do_search('Dijkstra')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRy8EpDCosrW",
        "outputId": "03b81a5e-4a82-42b4-e8f5-67fd1cc8d815"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['= Dijkstra\\'s algorithm =Dijkstra\\'s algorithm ( DYKE-strəz) is an algorithm for finding the shortest paths between nodes in a graph, which may represent, for example, road networks.  It was conceived by computer scientist Edsger W. Dijkstra in 1956 and published three years later.The algorithm exists in many variants. Dijkstra\\'s original algorithm found the shortest path between two given nodes, but a more common variant fixes a single node as the \"source\" node and finds shortest paths from the source to all other nodes in the graph, producing a shortest-path tree.For a given source node in the graph, the algorithm finds the shortest path between that node and every other.:\\u200a196–206\\u200a It can also be used for finding the shortest paths from a single node to a single destination node by stopping the algorithm once the shortest path to the destination node has been determined. For example, if the nodes of the graph represent cities and edge path costs represent driving distances between pairs of cities connected by a direct road (for simplicity, ignore red lights, stop signs, toll roads and other obstructions), Dijkstra\\'s algorithm can be used to find the shortest route between one city and all other cities. A widely used application of shortest path algorithms is network routing protocols, most notably IS-IS (Intermediate System to Intermediate System) and Open Shortest Path First (OSPF). It is also employed as a subroutine in other algorithms such as Johnson\\'s.The Dijkstra algorithm uses labels that are positive integers or real numbers, which are totally ordered.  It can be generalized to use any labels that are partially ordered, provided the subsequent labels (a subsequent label is produced when traversing an edge) are monotonically non-decreasing.  This generalization is called the generic Dijkstra shortest-path algorithm.Dijkstra\\'s algorithm uses a data structure for storing and querying partial solutions sorted by distance from the start. While the original algorithm uses a min-priority queue and runs in timeΘ((|V|+|E|)log\\u2061|V|){\\\\displaystyle \\\\Theta ((|V|+|E|)\\\\log |V|)}(where|V|{\\\\displaystyle |V|}is the number of nodes and|E|{\\\\displaystyle |E|}is the number of edges), it can also be implemented inΘ(|V|2){\\\\displaystyle \\\\Theta (|V|^{2})}using an array. The idea of this algorithm is also given in Leyzorek et al. 1957. Fredman & Tarjan 1984 propose using a Fibonacci heap min-priority queue to optimize the running time complexity toΘ(|E|+|V|log\\u2061|V|){\\\\displaystyle \\\\Theta (|E|+|V|\\\\log |V|)}. This is asymptotically the fastest known single-source shortest-path algorithm for arbitrary directed graphs with unbounded non-negative weights. However, specialized cases (such as bounded/integer weights, directed acyclic graphs etc.) can indeed be improved further as detailed in Specialized variants. Additionally, if preprocessing is allowed algorithms such as contraction hierarchies can be up to seven orders of magnitude faster.In some fields, artificial intelligence in particular, Dijkstra\\'s algorithm or a variant of it is known as uniform cost search and formulated as an instance of the more general idea of best-first search.HistoryWhat is the shortest way to travel from Rotterdam to Groningen, in general: from given city to given city. It is the algorithm for the shortest path, which I designed in about twenty minutes. One morning I was shopping in Amsterdam with my young fiancée, and tired, we sat down on the café terrace to drink a cup of coffee and I was just thinking about whether I could do this, and I then designed the algorithm for the shortest path. As I said, it was a twenty-minute invention. In fact, it was published in \\'59, three years later. The publication is still readable, it is, in fact, quite nice. One of the reasons that it is so nice was that I designed it without pencil and paper. I learned later that one of the advantages of designing without pencil and paper is that you are almost forced to avoid all avoidable complexities. Eventually, that algorithm became to my great amazement, one of the cornerstones of my fame.Dijkstra thought about the shortest path problem when working at the Mathematical Center in Amsterdam in 1956 as a programmer to demonstrate the capabilities of a new computer called ARMAC. His objective was to choose both a problem and a solution (that would be produced by computer) that non-computing people could understand. He designed the shortest path algorithm and later implemented it for ARMAC for a slightly simplified transportation map of 64 cities in the Netherlands (64, so that 6 bits would be sufficient to encode the city number). A year later, he came across another problem from hardware engineers working on the institute\\'s next computer: minimize the amount of wire needed to connect the pins on the back panel of the machine. As a solution, he re-discovered the algorithm known as Prim\\'s minimal spanning tree algorithm (known earlier to Jarník, and also rediscovered by Prim). Dijkstra published the algorithm in 1959, two years after Prim and 29 years after Jarník.AlgorithmLet the node at which we are starting be called the initial node. Let the distance of node Y be the distance from the initial node to Y. Dijkstra\\'s algorithm will initially start with infinite distances and will try to improve them step by step.Mark all nodes unvisited. Create a set of all the unvisited nodes called the unvisited set.Assign to every node a tentative distance value: set it to zero for our initial node and to infinity for all other nodes. The tentative distance of a node v is the length of the shortest path discovered so far between the node v and the starting node. Since initially no path is known to any other vertex than the source itself (which is a path of length zero), all other tentative distances are initially set to infinity. Set the initial node as current.For the current node, consider all of its unvisited neighbors and calculate their tentative distances through the current node. Compare the newly calculated tentative distance to the current assigned value and assign the smaller one. For example, if the current node A is marked with a  distance of 6, and the edge connecting it with a neighbor B has length 2, then the distance to B through A will be 6 + 2 = 8. If B was previously marked with a distance greater than 8 then change it to 8. Otherwise, the current value will be kept.When we are done considering all of the unvisited neighbors of the current node, mark the current node as visited and remove it from the unvisited set. A visited node will never be checked again.If the destination node has been marked visited (when planning a route between two specific nodes) or if the smallest tentative distance among the nodes in the unvisited set is infinity (when planning a complete traversal; occurs when there is no connection between the initial node and remaining unvisited nodes), then stop. The algorithm has finished.Otherwise, select the unvisited node that is marked with the smallest tentative distance, set it as the new current node, and go back to step 3.When planning a route, it is actually not necessary to wait until the destination node is \"visited\" as above: the algorithm can stop once the destination node has the smallest tentative distance among all \"unvisited\" nodes (and thus could be selected as the next \"current\").DescriptionSuppose you would like to find the shortest path between two intersections on a city map: a starting point and a destination. Dijkstra\\'s algorithm initially marks the distance (from the starting point) to every other intersection on the map with infinity. This is done not to imply that there is an infinite distance, but to note that those intersections have not been visited yet. Some variants of this method leave the intersections\\' distances unlabeled. Now select the current intersection at each iteration.  For the first iteration, the current intersection will be the starting point, and the distance to it (the intersection\\'s label) will be zero. For subsequent iterations (after the first), the current intersection will be a closest unvisited intersection to the starting point (this will be easy to find).From the current intersection, update the distance to every unvisited intersection that is directly connected to it. This is done by determining the sum of the distance between an unvisited intersection and the value of the current intersection and then relabeling the unvisited intersection with this value (the sum) if it is less than the unvisited intersection\\'s current value. In effect, the intersection is relabeled if the path to it through the current intersection is shorter than the previously known paths.  To facilitate shortest path identification, in pencil, mark the road with an arrow pointing to the relabeled intersection if you label/relabel it, and erase all others pointing to it.  After you have updated the distances to each neighboring intersection, mark the current intersection as visited and select an unvisited intersection with minimal distance (from the starting point) – or the lowest label—as the current intersection. Intersections marked as visited are labeled with the shortest path from the starting point to it and will not be revisited or returned to.Continue this process of updating the neighboring intersections with the shortest distances, marking the current intersection as visited, and moving onto a closest unvisited intersection until you have marked the destination as visited. Once you have marked the destination as visited (as is the case with any visited intersection), you have determined the shortest path to it from the starting point and can trace your way back following the arrows in reverse. In the algorithm\\'s implementations, this is usually done (after the algorithm has reached the destination node) by following the nodes\\' parents from the destination node up to the starting node; that\\'s why we also keep track of each node\\'s parent.This algorithm makes no attempt of direct \"exploration\" towards the destination as one might expect. Rather, the sole consideration in determining the next \"current\" intersection is its distance from the starting point. This algorithm therefore expands outward from the starting point, interactively considering every node that is closer in terms of shortest path distance until it reaches the destination. When understood in this way, it is clear how the algorithm necessarily finds the shortest path. However, it may also reveal one of the algorithm\\'s weaknesses: its relative slowness in some topologies.PseudocodeIn the following pseudocode algorithm, dist is an array that contains the current distances from the source to other vertices, i.e. dist[u] is the current distance from the source to the vertex u. The prev array contains pointers to previous-hop nodes on the shortest path from source to the given vertex (equivalently, it is the next-hop on the path from the given vertex to the source). The code u ← vertex in Q with min dist[u], searches for the vertex u in the vertex set Q that has the least dist[u] value. Graph.Edges(u, v) returns the length of the edge joining (i.e. the distance between) the two neighbor-nodes u and v. The variable alt on line 14 is the length of the path from the root node to the neighbor node v if it were to go through u. If this path is shorter than the current shortest path recorded for v, that current path is replaced with this alt path.1  function Dijkstra(Graph, source):23      for each vertex v in Graph.Vertices:4          dist[v] ← INFINITY5          prev[v] ← UNDEFINED6          add v to Q7      dist[source] ← 089      while Q is not empty:10          u ← vertex in Q with min dist[u]1112          for each neighbor v of u still in Q:13              alt ← dist[u] + Graph.Edges(u, v)14              if alt < dist[v]:15                  dist[v] ← alt16                  prev[v] ← u1718      return dist[], prev[]If we are only interested in a shortest path between vertices source and target, we can terminate the search after line 10 if u = target.Now we can read the shortest path from source to target by reverse iteration:1  S ← empty sequence2  u ← target3  if prev[u] is defined or u = source:          // Do something only if the vertex is reachable4      while u is defined:                       // Construct the shortest path with a stack S5          insert u at the beginning of S        // Push the vertex onto the stack6          u ← prev[u]                           // Traverse from target to sourceNow sequence S is the list of vertices constituting one of the shortest paths from source to target, or the empty sequence if no path exists.A more general problem would be to find all the shortest paths between source and target (there might be several different ones of the same length). Then instead of storing only a single node in each entry of prev[] we would store all nodes satisfying the relaxation condition. For example, if both r and source connect to target and both of them lie on different shortest paths through target (because the edge cost is the same in both cases), then we would add both r and source to prev[target]. When the algorithm completes, prev[] data structure will actually describe a graph that is a subset of the original graph with some edges removed. Its key property will be that if the algorithm was run with some starting node, then every path from that node to any other node in the new graph will be the shortest path between those nodes in the original graph, and all paths of that length from the original graph will be present in the new graph. Then to actually find all these shortest paths between two given nodes we would use a path finding algorithm on the new graph, such as depth-first search.Using a priority queueA min-priority queue is an abstract data type that provides 3 basic operations: add_with_priority(), decrease_priority() and extract_min(). As mentioned earlier, using such a data structure can lead to faster computing times than using a basic queue. Notably, Fibonacci heap (Fredman & Tarjan 1984) or Brodal queue offer optimal implementations for those 3 operations. As the algorithm is slightly different, we mention it here, in pseudo-code as well :1  function Dijkstra(Graph, source):2      dist[source] ← 0                           // Initialization34      create vertex priority queue Q56      for each vertex v in Graph.Vertices:7          if v ≠ source8              dist[v] ← INFINITY                 // Unknown distance from source to v9              prev[v] ← UNDEFINED                // Predecessor of v1011         Q.add_with_priority(v, dist[v])121314     while Q is not empty:                      // The main loop15         u ← Q.extract_min()                    // Remove and return best vertex16         for each neighbor v of u:              // only v that are still in Q17             alt ← dist[u] + Graph.Edges(u, v)18             if alt < dist[v]19                 dist[v] ← alt20                 prev[v] ← u21                 Q.decrease_priority(v, alt)2223     return dist, prevInstead of filling the priority queue with all nodes in the initialization phase, it is also possible to initialize it to contain only source; then, inside the if alt < dist[v] block, the decrease_priority() becomes an add_with_priority() operation if the node is not already in the queue.:\\u200a198\\u200aYet another alternative is to add nodes unconditionally to the priority queue and to instead check after extraction that no shorter connection was found yet. This can be done by additionally extracting the associated priority p from the queue and only processing further if p == dist[u] inside the while Q is not empty loop. These alternatives can use entirely array-based priority queues without decrease-key functionality, which have been found to achieve even faster computing times in practice. However, the difference in performance was found to be narrower for denser graphs.Proof of correctnessProof of Dijkstra\\'s algorithm is constructed by induction on the number of visited nodes.Invariant hypothesis: For each node v, dist[v] is the shortest distance from source to v when traveling via visited nodes only, or infinity if no such path exists. (Note: we do not assume dist[v] is the actual shortest distance for unvisited nodes.)The base case is when there is just one visited node, namely the initial node source, in which case the hypothesis is trivial.Otherwise, assume the hypothesis for n-1 visited nodes. In which case, we choose an edge vu where u has the least dist[u] of any unvisited nodes such that dist[u] = dist[v] + Graph.Edges[v,u]. dist[u] is considered to be the shortest distance from source to u because if there were a shorter path, and if w was the first unvisited node on that path then by the original hypothesis dist[w] > dist[u] which creates a contradiction. Similarly if there were a shorter path to u without using unvisited nodes, and if the last but one node on that path were w, then we would have had dist[u] = dist[w] + Graph.Edges[w,u], also a contradiction.After processing u it will still be true that for each unvisited node w, dist[w] will be the shortest distance from source to w using visited nodes only, because if there were a shorter path that doesn\\'t go by u we would have found it previously, and if there were a shorter path using u we would have updated it when processing u.After all nodes are visited, the shortest path from source to any node v consists only of visited nodes, therefore dist[v] is the shortest distance.Running timeBounds of the running time of Dijkstra\\'s algorithm on a graph with edges E and vertices V can be expressed as a function of the number of edges, denoted|E|{\\\\displaystyle |E|}, and the number of vertices, denoted|V|{\\\\displaystyle |V|}, using big-O notation. The complexity bound depends mainly on the data structure used to represent the set Q. In the following, upper bounds can be simplified because|E|{\\\\displaystyle |E|}isO(|V|2){\\\\displaystyle O(|V|^{2})}for any graph, but that simplification disregards the fact that in some problems, other upper bounds on|E|{\\\\displaystyle |E|}may hold.For any data structure for the vertex set Q, the running time is inΘ(|E|⋅Tdk+|V|⋅Tem),{\\\\displaystyle \\\\Theta (|E|\\\\cdot T_{\\\\mathrm {dk} }+|V|\\\\cdot T_{\\\\mathrm {em} }),}whereTdk{\\\\displaystyle T_{\\\\mathrm {dk} }}andTem{\\\\displaystyle T_{\\\\mathrm {em} }}are the complexities of the decrease-key and extract-minimum operations in Q, respectively.The simplest version of Dijkstra\\'s algorithm stores the vertex set Q as an linked list or array, and edges as an adjacency list or matrix. In this case, extract-minimum is simply a linear search through all vertices in Q, so the running time isΘ(|E|+|V|2)=Θ(|V|2){\\\\displaystyle \\\\Theta (|E|+|V|^{2})=\\\\Theta (|V|^{2})}.For sparse graphs, that is, graphs with far fewer than|V|2{\\\\displaystyle |V|^{2}}edges, Dijkstra\\'s algorithm can be implemented more efficiently by storing the graph in the form of adjacency lists and using a self-balancing binary search tree, binary heap, pairing heap, or Fibonacci heap as a priority queue to implement extracting minimum efficiently. To perform decrease-key steps in a binary heap efficiently, it is necessary to use an auxiliary data structure that maps each vertex to its position in the heap, and to keep this structure up to date as the priority queue Q changes. With a self-balancing binary search tree or binary heap, the algorithm requiresΘ((|E|+|V|)log\\u2061|V|){\\\\displaystyle \\\\Theta ((|E|+|V|)\\\\log |V|)}time in the worst case (wherelog{\\\\displaystyle \\\\log }denotes the binary logarithmlog2{\\\\displaystyle \\\\log _{2}}); for connected graphs this time bound can be simplified toΘ(|E|log\\u2061|V|){\\\\displaystyle \\\\Theta (|E|\\\\log |V|)}.  The Fibonacci heap improves this toΘ(|E|+|V|log\\u2061|V|).{\\\\displaystyle \\\\Theta (|E|+|V|\\\\log |V|).}When using binary heaps, the average case time complexity is lower than the worst-case: assuming edge costs are drawn independently from a common probability distribution, the expected number of decrease-key operations is bounded byΘ(|V|log\\u2061(|E|/|V|)){\\\\displaystyle \\\\Theta (|V|\\\\log(|E|/|V|))}, giving a total running time of:\\u200a199–200O(|E|+|V|log\\u2061|E||V|log\\u2061|V|).{\\\\displaystyle O\\\\left(|E|+|V|\\\\log {\\\\frac {|E|}{|V|}}\\\\log |V|\\\\right).}Practical optimizations and infinite graphsIn common presentations of Dijkstra\\'s algorithm, initially all nodes are entered into the priority queue. This is, however, not necessary: the algorithm can start with a priority queue that contains only one item, and insert new items as they are discovered (instead of doing a decrease-key, check whether the key is in the queue; if it is, decrease its key, otherwise insert it).:\\u200a198\\u200a This variant has the same worst-case bounds as the common variant, but maintains a smaller priority queue in practice, speeding up the queue operations.Moreover, not inserting all nodes in a graph makes it possible to extend the algorithm to find the shortest path from a single source to the closest of a set of target nodes on infinite graphs or those too large to represent in memory. The resulting algorithm is called uniform-cost search (UCS) in the artificial intelligence literature and can be expressed in pseudocode asprocedure uniform_cost_search(start) isnode ← startfrontier ← priority queue containing node onlyexplored ← empty setdoif frontier is empty thenreturn failurenode ← frontier.pop()if node is a goal state thenreturn solution(node)explored.add(node)for each of node\\'s neighbors n doif n is not in explored and not in frontier thenfrontier.add(n)else if n is in frontier with higher costreplace existing node with nThe complexity of this algorithm can be expressed in an alternative way for very large graphs: when C* is the length of the shortest path from the start node to any node satisfying the \"goal\" predicate, each edge has cost at least ε, and the number of neighbors per node is bounded by b, then the algorithm\\'s worst-case time and space complexity are both in O(b1+⌊C* ⁄ ε⌋).Further optimizations of Dijkstra\\'s algorithm for the single-target case include bidirectional variants, goal-directed variants such as the A* algorithm (see § Related problems and algorithms), graph pruning to determine which nodes are likely to form the middle segment of shortest paths (reach-based routing), and hierarchical decompositions of the input graph that reduce s–t routing to connecting s and t to their respective \"transit nodes\" followed by shortest-path computation between these transit nodes using a \"highway\".Combinations of such techniques may be needed for optimal practical performance on specific problems.Specialized variantsWhen arc weights are small integers (bounded by a parameterC{\\\\displaystyle C}), specialized queues which take advantage of this fact can be used to speed up Dijkstra\\'s algorithm. The first algorithm of this type was Dial\\'s algorithm (Dial 1969) for graphs with positive integer edge weights, which uses a bucket queue to obtain a running timeO(|E|+|V|C){\\\\displaystyle O(|E|+|V|C)}. The use of a Van Emde Boas tree as the priority queue brings the complexity toO(|E|log\\u2061log\\u2061C){\\\\displaystyle O(|E|\\\\log \\\\log C)}(Ahuja et al. 1990). Another interesting variant based on a combination of a new radix heap and the well-known Fibonacci heap runs in timeO(|E|+|V|log\\u2061C){\\\\displaystyle O(|E|+|V|{\\\\sqrt {\\\\log C}})}(Ahuja et al. 1990). Finally, the best algorithms in this special case are as follows. The algorithm given by (Thorup 2000) runs inO(|E|log\\u2061log\\u2061|V|){\\\\displaystyle O(|E|\\\\log \\\\log |V|)}time and the algorithm given by (Raman 1997) runs inO(|E|+|V|min{(log\\u2061|V|)1/3+ε,(log\\u2061C)1/4+ε}){\\\\displaystyle O(|E|+|V|\\\\min\\\\{(\\\\log |V|)^{1/3+\\\\varepsilon },(\\\\log C)^{1/4+\\\\varepsilon }\\\\})}time.Related problems and algorithmsThe functionality of Dijkstra\\'s original algorithm can be extended with a variety of modifications. For example, sometimes it is desirable to present solutions which are less than mathematically optimal. To obtain a ranked list of less-than-optimal solutions, the optimal solution is first calculated. A single edge appearing in the optimal solution is removed from the graph, and the optimum solution to this new graph is calculated. Each edge of the original solution is suppressed in turn and a new shortest-path calculated. The secondary solutions are then ranked and presented after the first optimal solution.Dijkstra\\'s algorithm is usually the working principle behind link-state routing protocols, OSPF and IS-IS being the most common ones.Unlike Dijkstra\\'s algorithm, the Bellman–Ford algorithm can be used on graphs with negative edge weights, as long as the graph contains no negative cycle reachable from the source vertex s. The presence of such cycles means there is no shortest path, since the total weight becomes lower each time the cycle is traversed. (This statement assumes that a \"path\" is allowed to repeat vertices. In graph theory that is normally not allowed.  In theoretical computer science it often is allowed.) It is possible to adapt Dijkstra\\'s algorithm to handle negative weight edges by combining it with the Bellman-Ford algorithm (to remove negative edges and detect negative cycles), such an algorithm is called Johnson\\'s algorithm.The A* algorithm is a generalization of Dijkstra\\'s algorithm that cuts down on the size of the subgraph that must be explored, if additional information is available that provides a lower bound on the \"distance\" to the target. This approach can be viewed from the perspective of linear programming: there is a natural linear program for computing shortest paths, and solutions to its dual linear program are feasible if and only if they form a consistent heuristic (speaking roughly, since the sign conventions differ from place to place in the literature). This feasible dual / consistent heuristic defines a non-negative reduced cost and A* is essentially running Dijkstra\\'s algorithm with these reduced costs. If the dual satisfies the weaker condition of admissibility, then A* is instead more akin to the Bellman–Ford algorithm.The process that underlies Dijkstra\\'s algorithm is similar to the greedy process used in Prim\\'s algorithm.  Prim\\'s purpose is to find a minimum spanning tree that connects all nodes in the graph; Dijkstra is concerned with only two nodes. Prim\\'s does not evaluate the total weight of the path from the starting node, only the individual edges.Breadth-first search can be viewed as a special-case of Dijkstra\\'s algorithm on unweighted graphs, where the priority queue degenerates into a FIFO queue.The fast marching method can be viewed as a continuous version of Dijkstra\\'s algorithm which computes the geodesic distance on a triangle mesh.Dynamic programming perspectiveFrom a dynamic programming point of view, Dijkstra\\'s algorithm is a successive approximation scheme that solves the dynamic programming functional equation for the shortest path problem by the Reaching method.In fact, Dijkstra\\'s explanation of the logic behind the algorithm, namelyProblem 2. Find the path of minimum total length between two given nodesP{\\\\displaystyle P}andQ{\\\\displaystyle Q}.We use the fact that, ifR{\\\\displaystyle R}is a node on the minimal path fromP{\\\\displaystyle P}toQ{\\\\displaystyle Q}, knowledge of the latter implies the knowledge of the minimal path fromP{\\\\displaystyle P}toR{\\\\displaystyle R}.is a paraphrasing of Bellman\\'s famous Principle of Optimality in the context of the shortest path problem.ApplicationsLeast-cost paths are calculated for instance to establish tracks of electricity lines or oil pipelines. The algorithm has also been used to calculate optimal long-distance footpaths in Ethiopia and contrast them with the situation on the ground.See alsoA* search algorithmBellman–Ford algorithmEuclidean shortest pathFloyd–Warshall algorithmJohnson\\'s algorithmLongest path problemParallel all-pairs shortest path algorithmNotesReferencesCormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L.; Stein, Clifford (2001). \"Section 24.3: Dijkstra\\'s algorithm\". Introduction to Algorithms (Second ed.). MIT Press and McGraw–Hill. pp. 595–601. ISBN 0-262-03293-7.Dial, Robert B. (1969). \"Algorithm 360: Shortest-path forest with topological ordering [H]\". Communications of the ACM. 12 (11): 632–633. doi:10.1145/363269.363610. S2CID 6754003.Fredman, Michael Lawrence; Tarjan, Robert E. (1984). Fibonacci heaps and their uses in improved network optimization algorithms. 25th Annual Symposium on Foundations of Computer Science. IEEE. pp. 338–346. doi:10.1109/SFCS.1984.715934.Fredman, Michael Lawrence; Tarjan, Robert E. (1987). \"Fibonacci heaps and their uses in improved network optimization algorithms\". Journal of the Association for Computing Machinery. 34 (3): 596–615. doi:10.1145/28869.28874. S2CID 7904683.Zhan, F. Benjamin; Noon, Charles E. (February 1998). \"Shortest Path Algorithms: An Evaluation Using Real Road Networks\". Transportation Science. 32 (1): 65–73. doi:10.1287/trsc.32.1.65. S2CID 14986297.Leyzorek, M.; Gray, R. S.; Johnson, A. A.; Ladew, W. C.; Meaker, Jr., S. R.; Petry, R. M.; Seitz, R. N. (1957). Investigation of Model Techniques – First Annual Report – 6 June 1956 – 1 July 1957 – A Study of Model Techniques for Communication Systems. Cleveland, Ohio: Case Institute of Technology.Knuth, D.E. (1977). \"A Generalization of Dijkstra\\'s Algorithm\". Information Processing Letters. 6 (1): 1–5. doi:10.1016/0020-0190(77)90002-3.Ahuja, Ravindra K.; Mehlhorn, Kurt; Orlin, James B.; Tarjan, Robert E. (April 1990). \"Faster Algorithms for the Shortest Path Problem\" (PDF). Journal of the ACM. 37 (2): 213–223. doi:10.1145/77600.77615. hdl:1721.1/47994. S2CID 5499589.Raman, Rajeev (1997). \"Recent results on the single-source shortest paths problem\". SIGACT News. 28 (2): 81–87. doi:10.1145/261342.261352. S2CID 18031586.Thorup, Mikkel (2000). \"On RAM priority Queues\". SIAM Journal on Computing. 30 (1): 86–109. doi:10.1137/S0097539795288246. S2CID 5221089.Thorup, Mikkel (1999). \"Undirected single-source shortest paths with positive integer weights in linear time\". Journal of the ACM. 46 (3): 362–394. doi:10.1145/316542.316548. S2CID 207654795.External linksOral history interview with Edsger W. Dijkstra, Charles Babbage Institute, University of Minnesota, MinneapolisImplementation of Dijkstra\\'s algorithm using TDD, Robert Cecil Martin, The Clean Code Blog',\n",
              " \"= Johnson's algorithm =Johnson's algorithm is a way to find the shortest paths between all pairs of vertices in an edge-weighted directed graph. It allows some of the edge weights to be negative numbers, but no negative-weight cycles may exist. It works by using the Bellman–Ford algorithm to compute a transformation of the input graph that removes all negative weights, allowing Dijkstra's algorithm to be used on the transformed graph. It is named after Donald B. Johnson, who first published the technique in 1977.A similar reweighting technique is also used in Suurballe's algorithm for finding two disjoint paths of minimum total length between the same two vertices in a graph with non-negative edge weights.Algorithm descriptionJohnson's algorithm consists of the following steps:First, a new node q is added to the graph, connected by zero-weight edges to each of the other nodes.Second, the Bellman–Ford algorithm is used, starting from the new vertex q, to find for each vertex v the minimum weight h(v) of a path from q to v. If this step detects a negative cycle, the algorithm is terminated.Next the edges of the original graph are reweighted using the values computed by the Bellman–Ford algorithm: an edge from u to v, having lengthw(u,v){\\\\displaystyle w(u,v)}, is given the new length w(u,v) + h(u) − h(v).Finally, q is removed, and Dijkstra's algorithm is used to find the shortest paths from each node s to every other vertex in the reweighted graph. The distance in the original graph is then computed for each distance D(u , v), by adding h(v) − h(u) to the distance returned by Dijkstra's algorithm.ExampleThe first three stages of Johnson's algorithm are depicted in the illustration below.The graph on the left of the illustration has two negative edges, but no negative cycles. The center graph shows the new vertex q, a shortest path tree as computed by the Bellman–Ford algorithm with q as starting vertex, and the values h(v) computed at each other node as the length of the shortest path from q to that node. Note that these values are all non-positive, because q has a length-zero edge to each vertex and the shortest path can be no longer than that edge. On the right is shown the reweighted graph, formed by replacing each edge weightw(u,v){\\\\displaystyle w(u,v)}by w(u,v) + h(u) − h(v). In this reweighted graph, all edge weights are non-negative, but the shortest path between any two nodes uses the same sequence of edges as the shortest path between the same two nodes in the original graph. The algorithm concludes by applying Dijkstra's algorithm to each of the four starting nodes in the reweighted graph.CorrectnessIn the reweighted graph, all paths between a pair s and t of nodes have the same quantity h(s) − h(t) added to them. The previous statement can be proven as follows: Let p be ans−t{\\\\displaystyle s-t}path. Its weight W in the reweighted graph is given by the following expression:(w(s,p1)+h(s)−h(p1))+(w(p1,p2)+h(p1)−h(p2))+...+(w(pn,t)+h(pn)−h(t)).{\\\\displaystyle {\\\\bigl (}w(s,p_{1})+h(s)-h(p_{1}){\\\\bigr )}+{\\\\bigl (}w(p_{1},p_{2})+h(p_{1})-h(p_{2}){\\\\bigr )}+...+{\\\\bigl (}w(p_{n},t)+h(p_{n})-h(t){\\\\bigr )}.}Every+h(pi){\\\\displaystyle +h(p_{i})}is cancelled by−h(pi){\\\\displaystyle -h(p_{i})}in the previous bracketed expression; therefore, we are left with the following expression for W:(w(s,p1)+w(p1,p2)+...+w(pn,t))+h(s)−h(t){\\\\displaystyle {\\\\bigl (}w(s,p_{1})+w(p_{1},p_{2})+...+w(p_{n},t){\\\\bigr )}+h(s)-h(t)}The bracketed expression is the weight of p in the original weighting.Since the reweighting adds the same amount to the weight of everys−t{\\\\displaystyle s-t}path, a path is a shortest path in the original weighting if and only if it is a shortest path after reweighting. The weight of edges that belong to a shortest path from q to any node is zero, and therefore the lengths of the shortest paths from q to every node become zero in the reweighted graph; however, they still remain shortest paths. Therefore, there can be no negative edges: if edge uv had a negative weight after the reweighting, then the zero-length path from q to u together with this edge would form a negative-length path from q to v, contradicting the fact that all vertices have zero distance from q. The non-existence of negative edges ensures the optimality of the paths found by Dijkstra's algorithm. The distances in the original graph may be calculated from the distances calculated by Dijkstra's algorithm in the reweighted graph by reversing the reweighting transformation.AnalysisThe time complexity of this algorithm, using Fibonacci heaps in the implementation of Dijkstra's algorithm, isO(|V|2log\\u2061|V|+|V||E|){\\\\displaystyle O(|V|^{2}\\\\log |V|+|V||E|)}: the algorithm usesO(|V||E|){\\\\displaystyle O(|V||E|)}time for the Bellman–Ford stage of the algorithm, andO(|V|log\\u2061|V|+|E|){\\\\displaystyle O(|V|\\\\log |V|+|E|)}for each of the|V|{\\\\displaystyle |V|}instantiations of Dijkstra's algorithm. Thus, when the graph is sparse, the total time can be faster than the Floyd–Warshall algorithm, which solves the same problem in timeO(|V|3){\\\\displaystyle O(|V|^{3})}.ReferencesExternal linksBoost: All Pairs Shortest Paths\",\n",
              " \"= Dijkstra–Scholten algorithm =The Dijkstra–Scholten algorithm (named after Edsger W. Dijkstra and Carel S. Scholten) is an algorithm for detecting termination in a distributed system. The algorithm was proposed by Dijkstra and Scholten in 1980.First, consider the case of a simple process graph which is a tree. A distributed computation which is tree-structured is not uncommon. Such a process graph may arise when the computation is strictly a divide-and-conquer type. A node starts the computation and divides the problem in two (or more, usually a multiple of 2) roughly equal parts and distribute those parts to other processors. This process continues recursively until the problems are of sufficiently small size to solve in a single processor.AlgorithmThe Dijkstra–Scholten algorithm is a tree-based algorithm which can be described by the following:The initiator of a computation is the root of the tree.Upon receiving a computational message:If the receiving process is currently not in the computation: the process joins the tree by becoming a child of the sender of the message. (No acknowledgment message is sent at this point.)If the receiving process is already in the computation: the process immediately sends an acknowledgment message to the sender of the message.When a process has no more children and has become idle, the process detaches itself from the tree by sending an acknowledgment message to its tree parent.Termination occurs when the initiator has no children and has become idle.Dijkstra–Scholten algorithm for a treeFor a tree, it is easy to detect termination. When a leaf process determines that it has terminated, it sends a signal to its parent. In general, a process waits for all its children to send signals and then it sends a signal to its parent.The program terminates when the root receives signals from all its children.Dijkstra–Scholten algorithm for directed acyclic graphsThe algorithm for a tree can be extended to acyclic directed graphs. We add an additional integer attribute Deficit to each edge.On an incoming edge, Deficit will denote the difference between the number of messages received and the number of signals sent in reply.When a node wishes to terminate, it waits until it has received signals from outgoing edges reducing their deficits to zero.Then it sends enough signals to ensure that the deficit is zero on each incoming edge.Since the graph is acyclic, some nodes will have no outgoing edges and these nodes will be the first to terminate after sending enough signals to their incoming edges. After that the nodes at higher levels will terminate level by level.Dijkstra–Scholten algorithm for cyclic directed graphsIf cycles are allowed, the previous algorithm does not work. This is because, there may not be any node with zero outgoing edges. So, potentially there is no node which can terminate without consulting other nodes.The Dijkstra–Scholten algorithm solves this problem by implicitly creating a spanning tree of the graph. A spanning-tree is a tree which includes each node of the underlying graph once and the edge-set is a subset of the original set of edges.The tree will be directed (i.e., the channels will be directed) with the source node (which initiates the computation) as the root.The spanning-tree is created in the following way. A variable First_Edge is added to each node. When a node receives a message for the first time, it initializes First_Edge with the edge through which it received the message. First_Edge is never changed afterwards. Note that, the spanning tree is not unique and it depends on the order of messages in the system.Termination is handled by each node in three steps :Send signals on all incoming edges except the first edge. (Each node will send signals which reduces the deficit on each incoming edge to zero.)Wait for signals from all outgoing edges. (The number of signals received on each outgoing edge should reduce each of their deficits to zero.)Send signals on First_Edge. (Once steps 1 and 2 are complete, a node informs its parent in the spanning tree about its intention of terminating.)See alsoHuang's algorithm== References ==\",\n",
              " '= Suurballe\\'s algorithm =In theoretical computer science and network routing, Suurballe\\'s algorithm is an algorithm for finding two disjoint paths in a nonnegatively-weighted directed graph, so that both paths connect the same pair of vertices and have minimum total length. The algorithm was conceived by John W. Suurballe and published in 1974. The main idea of Suurballe\\'s algorithm is to use Dijkstra\\'s algorithm to find one path, to modify the weights of the graph edges, and then to run Dijkstra\\'s algorithm a second time. The output of the algorithm is formed by combining these two paths, discarding edges that are traversed in opposite directions by the paths, and using the remaining edges to form the two paths to return as the output.The modification to the weights is similar to the weight modification in Johnson\\'s algorithm, and preserves the non-negativity of the weights while allowing the second instance of Dijkstra\\'s algorithm to find the correct second path.The problem of finding two disjoint paths of minimum weight can be seen as a special case of a minimum cost flow problem, where in this case there are two units of \"flow\" and nodes have unit \"capacity\". Suurballe\\'s algorithm, also, can be seen as a special case of a minimum cost flow algorithm that repeatedly pushes the maximum possible amount of flow along a shortest augmenting path.The first path found by Suurballe\\'s algorithm is the shortest augmenting path for the initial (zero) flow, and the second path found by Suurballe\\'s algorithm is the shortest augmenting path for the residual graph left after pushing one unit of flow along the first path.DefinitionsLet G be a weighted directed graph with vertex set  V and edge set  E (figure A); let  s be a designated source vertex in  G, and let  t be a designated destination vertex. Let each edge  (u,v) in  E, from vertex  u to vertex  v, have a non-negative cost  w(u,v).Define d(s,u) to be the cost of the shortest path to vertex u from vertex s in the shortest path tree rooted at s (figure C).Note: Node and Vertex are often used interchangeably.AlgorithmSuurballe\\'s algorithm performs the following steps:Find the shortest path tree T rooted at node s by running Dijkstra\\'s algorithm (figure C). This tree contains for every vertex u, a shortest path from s to u. Let P1 be the shortest cost path from s to t (figure B). The edges in T are called tree edges and the remaining edges (the edges missing from figure C) are called non-tree edges.Modify the cost of each edge in the graph by replacing the cost w(u,v) of every edge (u,v) by w′(u,v) = w(u,v) − d(s,v) + d(s,u). According to the resulting modified cost function, all tree edges have a cost of 0, and non-tree edges have a non-negative cost. For example:  If u=B, v=E, then w′(u,v) = w(B,E) − d(A,E) + d(A,B) = 2 − 3 + 1 = 0  If u=E, v=B, then w′(u,v) = w(E,B) − d(A,B) + d(A,E) = 2 − 1 + 3 = 4Create a residual graph Gt formed from G by removing the edges of G on path P1 that are directed into s and then reverse the direction of the zero length edges along path P1 (figure D).Find the shortest path P2 in the residual graph Gt by running Dijkstra\\'s algorithm (figure E).Discard the reversed edges of P2 from both paths. The remaining edges of P1 and P2 form a subgraph with two outgoing edges at s, two incoming edges at t, and one incoming and one outgoing edge at each remaining vertex. Therefore, this subgraph consists of two edge-disjoint paths from s to t and possibly some additional (zero-length) cycles. Return the two disjoint paths from the subgraph.ExampleThe following example shows how Suurballe\\'s algorithm finds the shortest pair of disjoint paths from A to F.Figure A illustrates a weighted graph G.Figure B calculates the shortest path P1 from A to F (A–B–D–F).Figure C illustrates the shortest path tree T rooted at A, and the computed distances from A to every vertex (u).Figure D shows the residual graph Gt with the updated cost of each edge and the edges of path P1 reversed.Figure E calculates path P2 in the residual graph Gt (A–C–D–B–E–F).Figure F illustrates both path P1 and path P2.Figure G finds the shortest pair of disjoint paths by combining the edges of paths P1 and P2 and then discarding the common reversed edges between both paths (B–D). As a result, we get the two shortest pair of disjoint paths (A–B–E–F) and (A–C–D–F).CorrectnessThe weight of any path from s to t in the modified system of weights equals the weight in the original graph, minus d(s,t). Therefore, the shortest two disjoint paths under the modified weights are the same paths as the shortest two paths in the original graph, although they have different weights.Suurballe\\'s algorithm may be seen as a special case of the successive shortest paths method for finding a minimum cost flow with total flow amount two from s to t. The modification to the weights does not affect the sequence of paths found by this method, only their weights. Therefore, the correctness of the algorithm follows from the correctness of the successive shortest paths method.Analysis and running timeThis algorithm requires two iterations of Dijkstra\\'s algorithm. Using Fibonacci heaps, both iterations can be performed in timeO(|E|+|V|log\\u2061|V|){\\\\displaystyle O(|E|+|V|\\\\log |V|)}where|V|{\\\\displaystyle |V|}and|E|{\\\\displaystyle |E|}are the number of vertices and edges respectively. Therefore, the same time bound applies to Suurballe\\'s algorithm.VariationsThe version of Suurballe\\'s algorithm as described above finds paths that have disjoint edges, but that may share vertices. It is possible to use the same algorithm to find vertex-disjoint paths, by replacing each vertex by a pair of adjacent vertices, one with all of the incoming adjacencies  u-in of the original vertex, and one with all of the outgoing adjacencies  u-out. Two edge-disjoint paths in this modified graph necessarily correspond to two vertex-disjoint paths in the original graph, and vice versa, so applying Suurballe\\'s algorithm to the modified graph results in the construction of two vertex-disjoint paths in the original graph. Suurballe\\'s original 1974 algorithm was for the vertex-disjoint version of the problem, and was extended in 1984 by Suurballe and Tarjan to the edge-disjoint version.By using a modified version of Dijkstra\\'s algorithm that simultaneously computes the distances to each vertex t in the graphs Gt, it is also possible to find the total lengths of the shortest pairs of paths from a given source vertex s to every other vertex in the graph, in an amount of time that is proportional to a single instance of Dijkstra\\'s algorithm.Note: The pair of adjacent vertices resulting from the split are connected by a zero cost uni-directional edge from the incoming to outgoing vertex. The source vertex becomes s-out and the destination vertex becomes t-in.See alsoEdge disjoint shortest pair algorithm== References ==',\n",
              " '= Yen\\'s algorithm =Yen\\'s algorithm computes single-source K-shortest loopless paths for a graph with non-negative edge cost.  The algorithm was published by Jin Y. Yen in 1971 and employs any shortest path algorithm to find the best path, then proceeds to find K − 1 deviations of the best path.AlgorithmTerminology and notationDescriptionThe algorithm can be broken down into two parts, determining the first k-shortest path,A1{\\\\displaystyle A^{1}}, and then determining all other k-shortest paths. It is assumed that the containerA{\\\\displaystyle A}will hold the k-shortest path, whereas the containerB{\\\\displaystyle B}, will hold the potential k-shortest paths. To determineA1{\\\\displaystyle A^{1}}, the shortest path from the source to the sink, any efficient shortest path algorithm can be used.To find theAk{\\\\displaystyle A^{k}}, wherek{\\\\displaystyle k}ranges from2{\\\\displaystyle 2}toK{\\\\displaystyle K}, the algorithm assumes that all paths fromA1{\\\\displaystyle A^{1}}toAk−1{\\\\displaystyle A^{k-1}}have previously been found. Thek{\\\\displaystyle k}iteration can be divided into two processes, finding all the deviationsAki{\\\\displaystyle {A^{k}}_{i}}and choosing a minimum length path to becomeAk{\\\\displaystyle A^{k}}. Note that in this iteration,i{\\\\displaystyle i}ranges from1{\\\\displaystyle 1}toQkk{\\\\displaystyle {Q^{k}}_{k}}.The first process can be further subdivided into three operations, choosing theRki{\\\\displaystyle {R^{k}}_{i}}, findingSki{\\\\displaystyle {S^{k}}_{i}}, and then addingAki{\\\\displaystyle {A^{k}}_{i}}to the containerB{\\\\displaystyle B}. The root path,Rki{\\\\displaystyle {R^{k}}_{i}}, is chosen by finding the subpath inAk−1{\\\\displaystyle A^{k-1}}that follows the firsti{\\\\displaystyle i}nodes ofAj{\\\\displaystyle A^{j}}, wherej{\\\\displaystyle j}ranges from1{\\\\displaystyle 1}tok−1{\\\\displaystyle k-1}. Then, if a path is found, the cost of edgedi(i+1){\\\\displaystyle d_{i(i+1)}}ofAj{\\\\displaystyle A^{j}}is set to infinity. Next, the spur path,Ski{\\\\displaystyle {S^{k}}_{i}}, is found by computing the shortest path from the spur node, nodei{\\\\displaystyle i}, to the sink. The removal of previous used edges from(i){\\\\displaystyle (i)}to(i+1){\\\\displaystyle (i+1)}ensures that the spur path is different.Aki=Rki+Ski{\\\\displaystyle {A^{k}}_{i}={R^{k}}_{i}+{S^{k}}_{i}}, the addition of the root path and the spur path, is added toB{\\\\displaystyle B}. Next, the edges that were removed, i.e. had their cost set to infinity, are restored to their initial values.The second process determines a suitable path forAk{\\\\displaystyle A^{k}}by finding the path in containerB{\\\\displaystyle B}with the lowest cost. This path is removed from containerB{\\\\displaystyle B}and inserted into containerA{\\\\displaystyle A}and the algorithm continues to the next iteration.PseudocodeThe algorithm assumes that the Dijkstra algorithm is used to find the shortest path between two nodes, but any shortest path algorithm can be used in its place.function YenKSP(Graph, source, sink, K):// Determine the shortest path from the source to the sink.A[0] = Dijkstra(Graph, source, sink);// Initialize the set to store the potential kth shortest path.B = [];for k from 1 to K:// The spur node ranges from the first node to the next to last node in the previous k-shortest path.for i from 0 to size(A[k − 1]) − 2:// Spur node is retrieved from the previous k-shortest path, k − 1.spurNode = A[k-1].node(i);// The sequence of nodes from the source to the spur node of the previous k-shortest path.rootPath = A[k-1].nodes(0, i);for each path p in A:if rootPath == p.nodes(0, i):// Remove the links that are part of the previous shortest paths which share the same root path.remove p.edge(i,i + 1) from Graph;for each node rootPathNode in rootPath except spurNode:remove rootPathNode from Graph;// Calculate the spur path from the spur node to the sink.// Consider also checking if any spurPath foundspurPath = Dijkstra(Graph, spurNode, sink);// Entire path is made up of the root path and spur path.totalPath = rootPath + spurPath;// Add the potential k-shortest path to the heap.if (totalPath not in B):B.append(totalPath);// Add back the edges and nodes that were removed from the graph.restore edges to Graph;restore nodes in rootPath to Graph;if B is empty:// This handles the case of there being no spur paths, or no spur paths left.// This could happen if the spur paths have already been exhausted (added to A),// or there are no spur paths at all - such as when both the source and sink vertices// lie along a \"dead end\".break;// Sort the potential k-shortest paths by cost.B.sort();// Add the lowest cost path becomes the k-shortest path.A[k] = B[0];// In fact we should rather use shift since we are removing the first elementB.pop();return A;ExampleThe example uses Yen\\'s K-Shortest Path Algorithm to compute three paths from(C){\\\\displaystyle (C)}to(H){\\\\displaystyle (H)}. Dijkstra\\'s algorithm is used to calculate the best path from(C){\\\\displaystyle (C)}to(H){\\\\displaystyle (H)}, which is(C)−(E)−(F)−(H){\\\\displaystyle (C)-(E)-(F)-(H)}with cost 5. This path is appended to containerA{\\\\displaystyle A}and becomes the first k-shortest path,A1{\\\\displaystyle A^{1}}.Node(C){\\\\displaystyle (C)}ofA1{\\\\displaystyle A^{1}}becomes the spur node with a root path of itself,R21=(C){\\\\displaystyle {R^{2}}_{1}=(C)}. The edge,(C)−(E){\\\\displaystyle (C)-(E)}, is removed because it coincides with the root path and a path in containerA{\\\\displaystyle A}. Dijkstra\\'s algorithm is used to compute the spur pathS21{\\\\displaystyle {S^{2}}_{1}}, which is(C)−(D)−(F)−(H){\\\\displaystyle (C)-(D)-(F)-(H)}, with a cost of 8.A21=R21+S21=(C)−(D)−(F)−(H){\\\\displaystyle {A^{2}}_{1}={R^{2}}_{1}+{S^{2}}_{1}=(C)-(D)-(F)-(H)}is added to containerB{\\\\displaystyle B}as a potential k-shortest path.Node(E){\\\\displaystyle (E)}ofA1{\\\\displaystyle A^{1}}becomes the spur node withR22=(C)−(E){\\\\displaystyle {R^{2}}_{2}=(C)-(E)}. The edge,(E)−(F){\\\\displaystyle (E)-(F)}, is removed because it coincides with the root path and a path in containerA{\\\\displaystyle A}. Dijkstra\\'s algorithm is used to compute the spur pathS22{\\\\displaystyle {S^{2}}_{2}}, which is(E)−(G)−(H){\\\\displaystyle (E)-(G)-(H)}, with a cost of 7.A22=R22+S22=(C)−(E)−(G)−(H){\\\\displaystyle {A^{2}}_{2}={R^{2}}_{2}+{S^{2}}_{2}=(C)-(E)-(G)-(H)}is added to containerB{\\\\displaystyle B}as a potential k-shortest path.Node(F){\\\\displaystyle (F)}ofA1{\\\\displaystyle A^{1}}becomes the spur node with a root path,R23=(C)−(E)−(F){\\\\displaystyle {R^{2}}_{3}=(C)-(E)-(F)}. The edge,(F)−(H){\\\\displaystyle (F)-(H)}, is removed because it coincides with the root path and a path in containerA{\\\\displaystyle A}. Dijkstra\\'s algorithm is used to compute the spur pathS23{\\\\displaystyle {S^{2}}_{3}}, which is(F)−(G)−(H){\\\\displaystyle (F)-(G)-(H)}, with a cost of 8.A23=R23+S23=(C)−(E)−(F)−(G)−(H){\\\\displaystyle {A^{2}}_{3}={R^{2}}_{3}+{S^{2}}_{3}=(C)-(E)-(F)-(G)-(H)}is added to containerB{\\\\displaystyle B}as a potential k-shortest path.Of the three paths in container B,A22{\\\\displaystyle {A^{2}}_{2}}is chosen to becomeA2{\\\\displaystyle A^{2}}because it has the lowest cost of 7. This process is continued to the 3rd k-shortest path. However, within this 3rd iteration, note that some spur paths do not exist. And the path that is chosen to becomeA3{\\\\displaystyle A^{3}}is(C)−(D)−(F)−(H){\\\\displaystyle (C)-(D)-(F)-(H)}.FeaturesSpace complexityTo store the edges of the graph, the shortest path listA{\\\\displaystyle A}, and the potential shortest path listB{\\\\displaystyle B},N2+KN{\\\\displaystyle N^{2}+KN}memory addresses are required. At worse case, the every node in the graph has an edge to every other node in the graph, thusN2{\\\\displaystyle N^{2}}addresses are needed. OnlyKN{\\\\displaystyle KN}addresses are need for both listA{\\\\displaystyle A}andB{\\\\displaystyle B}because at most onlyK{\\\\displaystyle K}paths will be stored, where it is possible for each path to haveN{\\\\displaystyle N}nodes.Time complexityThe time complexity of Yen\\'s algorithm is dependent on the shortest path algorithm used in the computation of the spur paths, so the Dijkstra algorithm is assumed. Dijkstra\\'s algorithm has a worse case time complexity ofO(N2){\\\\displaystyle O(N^{2})}, but using a Fibonacci heap it becomesO(M+Nlog\\u2061N){\\\\displaystyle O(M+N\\\\log N)}, whereM{\\\\displaystyle M}is the amount of edges in the graph. Since Yen\\'s algorithm makesKl{\\\\displaystyle Kl}calls to the Dijkstra in computing the spur paths, wherel{\\\\displaystyle l}is the length of spur paths. In a condensed graph, the expected value ofl{\\\\displaystyle l}isO(log\\u2061N){\\\\displaystyle O(\\\\log N)}, while the worst case isN{\\\\displaystyle N}., the time complexity becomesO(KN(M+Nlog\\u2061N)){\\\\displaystyle O(KN(M+N\\\\log N))}.ImprovementsYen\\'s algorithm can be improved by using a heap to storeB{\\\\displaystyle B}, the set of potential k-shortest paths. Using a heap instead of a list will improve the performance of the algorithm, but not the complexity.  One method to slightly decrease complexity is to skip the nodes where there are non-existent spur paths. This case is produced when all the spur paths from a spur node have been used in the previousAk{\\\\displaystyle A^{k}}. Also, if containerB{\\\\displaystyle B}hasK−k{\\\\displaystyle K-k}paths of minimum length, in reference to those in containerA{\\\\displaystyle A}, then they can be extract and inserted into containerA{\\\\displaystyle A}since no shorter paths will be found.Lawler\\'s modificationEugene Lawler proposed a modification to Yen\\'s algorithm in which duplicates path are not calculated as opposed to the original algorithm where they are calculated and then discarded when they are found to be duplicates. These duplicates paths result from calculating spur paths of nodes in the root ofAk{\\\\displaystyle A^{k}}. For instance,Ak{\\\\displaystyle A^{k}}deviates fromAk−1{\\\\displaystyle A^{k-1}}at some node(i){\\\\displaystyle (i)}. Any spur path,Skj{\\\\displaystyle {S^{k}}_{j}}wherej=0,…,i{\\\\displaystyle j=0,\\\\ldots ,i}, that is calculated will be a duplicate because they have already been calculated during thek−1{\\\\displaystyle k-1}iteration. Therefore, only spur paths for nodes that were on the spur path ofAk−1{\\\\displaystyle A^{k-1}}must be calculated, i.e. onlySkh{\\\\displaystyle {S^{k}}_{h}}whereh{\\\\displaystyle h}ranges from(i+1)k−1{\\\\displaystyle (i+1)^{k-1}}to(Qk)k−1{\\\\displaystyle (Q_{k})^{k-1}}. To perform this operation forAk{\\\\displaystyle A^{k}}, a record is needed to identify the node whereAk−1{\\\\displaystyle A^{k-1}}branched fromAk−2{\\\\displaystyle A^{k-2}}.See alsoYen\\'s improvement to the Bellman–Ford algorithmReferencesExternal linksOpen Source Python Implementation on GitHubOpen Source C++ ImplementationOpen Source C++ Implementation using Boost Graph Library',\n",
              " '= Path-based strong component algorithm =In graph theory, the strongly connected components of a directed graph may be found using an algorithm that uses depth-first search in combination with two stacks, one to keep track of the vertices in the current component and the second to keep track of the current search path. Versions of this algorithm have been proposed by Purdom (1970), Munro (1971), Dijkstra (1976), Cheriyan & Mehlhorn (1996), and Gabow (2000); of these, Dijkstra\\'s version was the first to achieve linear time.DescriptionThe algorithm performs a depth-first search of the given graph G, maintaining as it does two stacks S and P (in addition to the normal call stack for a recursive function).Stack S contains all the vertices that have not yet been assigned to a strongly connected component, in the order in which the depth-first search reaches the vertices.Stack P contains vertices that have not yet been determined to belong to different strongly connected components from each other. It also uses a counter C of the number of vertices reached so far, which it uses to compute the preorder numbers of the vertices.When the depth-first search reaches a vertex v, the algorithm performs the following steps:Set the preorder number of v to C, and increment C.Push v onto S and also onto P.For each edge from v to a neighboring vertex w:If the preorder number of w has not yet been assigned (the edge is a tree edge), recursively search w;Otherwise, if w has not yet been assigned to a strongly connected component (the edge is a forward/back/cross edge):Repeatedly pop vertices from P until the top element of P has a preorder number less than or equal to the preorder number of w.If v is the top element of P:Pop vertices from S until v has been popped, and assign the popped vertices to a new component.Pop v from P.The overall algorithm consists of a loop through the vertices of the graph, calling this recursive search on each vertex that does not yet have a preorder number assigned to it.Related algorithmsLike this algorithm, Tarjan\\'s strongly connected components algorithm also uses depth first search together with a stack to keep track of vertices that have not yet been assigned to a component, and moves these vertices into a new component when it finishes expanding the final vertex of its component. However, in place of the stack P, Tarjan\\'s algorithm uses  a vertex-indexed array of preorder numbers, assigned in the order that vertices are first visited in the depth-first search. The preorder array is used to keep track of when to form a new component.NotesReferencesCheriyan, J.; Mehlhorn, K. (1996), \"Algorithms for dense graphs and networks on the random access computer\", Algorithmica, 15 (6): 521–549, doi:10.1007/BF01940880, S2CID 8930091.Dijkstra, Edsger (1976), A Discipline of Programming, NJ: Prentice Hall, Ch. 25.Gabow, Harold N. (2000), \"Path-based depth-first search for strong and biconnected components\", Information Processing Letters, 74 (3–4): 107–114, doi:10.1016/S0020-0190(00)00051-X, MR 1761551.Munro, Ian (1971), \"Efficient determination of the transitive closure of a directed graph\", Information Processing Letters, 1 (2): 56–58, doi:10.1016/0020-0190(71)90006-8.Purdom, P., Jr. (1970), \"A transitive closure algorithm\", BIT, 10: 76–94, doi:10.1007/bf01940892, S2CID 20818200.Sedgewick, R. (2004), \"19.8 Strong Components in Digraphs\", Algorithms in Java, Part 5 – Graph Algorithms (3rd ed.), Cambridge MA: Addison-Wesley, pp. 205–216.',\n",
              " '= A* search algorithm =A* (pronounced \"A-star\") is a graph traversal and path search algorithm, which is often used in many fields of computer science due to its completeness, optimality, and optimal efficiency. One major practical drawback is itsO(bd){\\\\displaystyle O(b^{d})}space complexity, as it stores all generated nodes in memory. Thus, in practical travel-routing systems, it is generally outperformed by algorithms which can pre-process the graph to attain better performance, as well as memory-bounded approaches; however, A* is still the best solution in many cases.Peter Hart, Nils Nilsson and Bertram Raphael of Stanford Research Institute (now SRI International) first published the algorithm in 1968. It can be seen as an extension of Dijkstra\\'s algorithm. A* achieves better performance by using heuristics to guide its search.HistoryA* was created as part of the Shakey project, which had the aim of building a mobile robot that could plan its own actions. Nils Nilsson originally proposed using the Graph Traverser algorithm for Shakey\\'s path planning. Graph Traverser is guided by a heuristic function h(n), the estimated distance from node n to the goal node: it entirely ignores g(n), the distance from the start node to n. Bertram Raphael suggested using the sum, g(n) + h(n). Peter Hart invented the concepts we now call admissibility and consistency of heuristic functions. A* was originally designed for finding least-cost paths when the cost of a path is the sum of its  costs, but it has been shown that A* can be used to find optimal paths for any problem satisfying the conditions of a cost algebra.The original 1968 A* paper contained a theorem stating that no A*-like algorithm could expand fewer nodes than A* if the heuristic function is consistent and A*\\'s tie-breaking rule is suitably chosen. A ″correction″ was published a few years later claiming that consistency was not required, but this was shown to be false in Dechter and Pearl\\'s definitive study of A*\\'s optimality (now called optimal efficiency), which gave an example of A* with a heuristic that was admissible but not consistent expanding arbitrarily more nodes than an alternative A*-like algorithm.DescriptionA* is an informed search algorithm, or a best-first search, meaning that it is formulated in terms of weighted graphs: starting from a specific starting node of a graph, it aims to find a path to the given goal node having the smallest cost (least distance travelled, shortest time, etc.).  It does this by maintaining a tree of paths originating at the start node and extending those paths one edge at a time until its termination criterion is satisfied.At each iteration of its main loop, A* needs to determine which of its paths to extend. It does so based on the cost of the path and an estimate of the cost required to extend the path all the way to the goal. Specifically, A* selects the path that minimizesf(n)=g(n)+h(n){\\\\displaystyle f(n)=g(n)+h(n)}where n is the next node on the path, g(n) is the cost of the path from the start node to n, and h(n) is a heuristic function that estimates the cost of the cheapest path from n to the goal. A* terminates when the path it chooses to extend is a path from start to goal or if there are no paths eligible to be extended. The heuristic function is problem-specific. If the heuristic function is admissible, meaning that it never overestimates the actual cost to get to the goal, A* is guaranteed to return a least-cost path from start to goal.Typical implementations of A* use a priority queue to perform the repeated selection of minimum (estimated) cost nodes to expand. This priority queue is known as the open set or fringe. At each step of the algorithm, the node with the lowest f(x) value is removed from the queue, the f and g values of its neighbors are updated accordingly, and these neighbors are added to the queue. The algorithm continues until a removed node (thus the node with the lowest f value out of all fringe nodes) is a goal node. The f value of that goal is then also the cost of the shortest path, since h at the goal is zero in an admissible heuristic.The algorithm described so far gives us only the length of the shortest path. To find the actual sequence of steps, the algorithm can be easily revised so that each node on the path keeps track of its predecessor. After this algorithm is run, the ending node will point to its predecessor, and so on, until some node\\'s predecessor is the start node.As an example, when searching for the shortest route on a map, h(x) might represent the straight-line distance to the goal, since that is physically the smallest possible distance between any two points. For a grid map from a video game, using the Manhattan distance or the octile distance becomes better depending on the set of movements available (4-way or 8-way).If the heuristic h satisfies the additional condition h(x) ≤ d(x, y) + h(y) for every edge (x, y) of the graph (where d denotes the length of that edge), then h is called monotone, or consistent. With a consistent heuristic, A* is guaranteed to find an optimal path without processing any node more than once and A* is equivalent to running Dijkstra\\'s algorithm with the reduced cost d\\'(x, y) = d(x, y) + h(y) − h(x).PseudocodeThe following pseudocode describes the algorithm:Remark: In this pseudocode, if a node is reached by one path, removed from openSet, and subsequently reached by a cheaper path, it will be added to openSet again. This is essential to guarantee that the path returned is optimal if the heuristic function is admissible but not  consistent.   If the heuristic is consistent, when a node is removed from openSet the path to it is guaranteed to be optimal so the test ‘tentative_gScore < gScore[neighbor]’ will always fail if the node is reached again.ExampleAn example of an A* algorithm in action where nodes are cities connected with roads and h(x) is the     straight-line distance to target point:Key: green: start; blue: goal; orange: visitedThe A* algorithm also has real-world applications. In this example, edges are railroads and h(x) is the great-circle distance (the shortest possible distance on a sphere) to the target. The algorithm is searching for a path between Washington, D.C. and Los Angeles.Implementation detailsThere are a number of simple optimizations or implementation details that can significantly affect the performance of an A* implementation.  The first detail to note is that the way the priority queue handles ties can have a significant effect on performance in some situations.  If ties are broken so the queue behaves in a LIFO manner, A* will behave like depth-first search among equal cost paths (avoiding exploring more than one equally optimal solution).When a path is required at the end of the search, it is common to keep with each node a reference to that node\\'s parent.  At the end of the search these references can be used to recover the optimal path.  If these references are being kept then it can be important that the same node doesn\\'t appear in the priority queue more than once (each entry corresponding to a different path to the node, and each with a different cost).  A standard approach here is to check if a node about to be added already appears in the priority queue.  If it does, then the priority and parent pointers are changed to correspond to the lower cost path. A standard binary heap based priority queue does not directly support the operation of searching for one of its elements, but it can be augmented with a hash table that maps elements to their position in the heap, allowing this decrease-priority operation to be performed in logarithmic time. Alternatively, a Fibonacci heap can perform the same decrease-priority operations in constant amortized time.Special casesDijkstra\\'s algorithm, as another example of a uniform-cost search algorithm, can be viewed as a special case of A* whereh(x)=0{\\\\displaystyle h(x)=0}for all x. General depth-first search can be implemented using A* by considering that there is a global counter C initialized with a very large value. Every time we process a node we assign C to all of its newly discovered neighbors. After each single assignment, we decrease the counter C by one. Thus the earlier a node is discovered, the higher itsh(x){\\\\displaystyle h(x)}value. Both Dijkstra\\'s algorithm and depth-first search can be implemented more efficiently without including anh(x){\\\\displaystyle h(x)}value at each node.PropertiesTermination and CompletenessOn finite graphs with non-negative edge weights A* is guaranteed to terminate and is complete, i.e. it will always find a solution (a path from start to goal) if one exists. On infinite graphs with a finite branching factor and edge costs that are bounded away from zero (d(x,y)>ε>0{\\\\textstyle d(x,y)>\\\\varepsilon >0}for some fixedε{\\\\displaystyle \\\\varepsilon }), A* is guaranteed to terminate only if there exists a solution.AdmissibilityA search algorithm is said to be admissible if it is guaranteed to return an optimal solution. If the heuristic function used by A* is admissible, then A* is admissible. An intuitive ″proof″ of this  is as follows:When A* terminates its search, it has found a path from start to goal whose actual cost is lower than the estimated cost of any path from start to goal through any open node (the node\\'sf{\\\\displaystyle f}value). When the heuristic is admissible, those estimates are optimistic (not quite—see the next paragraph), so A* can safely ignore those nodes because they cannot possibly lead to a cheaper solution than the one it already has. In other words, A* will never overlook the possibility of a lower-cost path from start to goal and so it will continue to search until no such possibilities exist.The actual proof is a bit more involved because thef{\\\\displaystyle f}values of open nodes are not guaranteed to be optimistic even if the heuristic is admissible. This is because theg{\\\\displaystyle g}values of open nodes are not guaranteed to be optimal, so the sumg+h{\\\\displaystyle g+h}is not guaranteed to be optimistic.Optimality and ConsistencyAlgorithm A is optimally efficient with respect to a set of alternative algorithms Alts on a set of problems P if for every problem P in P and every algorithm A′ in Alts, the set of nodes expanded by A in solving P is a subset (possibly equal) of the set of nodes expanded by A′ in solving P. The definitive study of the optimal efficiency of A* is due to Rina Dechter and Judea Pearl.They considered a variety of  definitions of Alts and P  in combination with A*\\'s heuristic being merely admissible or being both consistent and admissible.  The most interesting positive result they proved is that A*, with a consistent heuristic, is optimally efficient with respect to all admissible A*-like search algorithms on all ″non-pathological″ search problems.  Roughly speaking, their notion of non-pathological problem is what we now mean by ″up to tie-breaking″.  This result does not hold if A*\\'s heuristic is admissible but not consistent. In that case, Dechter and Pearl showed there exist admissible A*-like algorithms that can expand arbitrarily fewer nodes than A* on some non-pathological problems.Optimal efficiency is about the set of nodes expanded, not the number of node expansions (the number of iterations of A*\\'s main loop).  When the heuristic being used is admissible but not consistent, it is possible for a node to be expanded by A* many times, an exponential number of times in the worst case.In such circumstances Dijkstra\\'s algorithm could outperform A* by a large margin. However, more recent research found that this pathological case only occurs in certain contrived situations where the edge weight of the search graph is exponential in the size of the graph, and that certain inconsistent (but admissible) heuristics can lead to a reduced number of node expansions in A* searches.Bounded relaxationWhile the admissibility criterion guarantees an optimal solution path, it also means that A* must examine all equally meritorious paths to find the optimal path. To compute approximate shortest paths, it is possible to speed up the search at the expense of optimality by relaxing the admissibility criterion. Oftentimes we want to bound this relaxation, so that we can guarantee that the solution path is no worse than (1 + ε) times the optimal solution path. This new guarantee is referred to as ε-admissible.There are a number of ε-admissible algorithms:Weighted A*/Static Weighting\\'s. If ha(n) is an admissible heuristic function, in the weighted version of the A* search one uses hw(n) = ε ha(n), ε > 1 as the heuristic function, and perform the A* search as usual (which eventually happens faster than using ha since fewer nodes are expanded). The path hence found by the search algorithm can have a cost of at most ε times that of the least cost path in the graph.Dynamic Weighting uses the cost functionf(n)=g(n)+(1+εw(n))h(n){\\\\displaystyle f(n)=g(n)+(1+\\\\varepsilon w(n))h(n)}, wherew(n)={1−d(n)Nd(n)≤N0otherwise{\\\\displaystyle w(n)={\\\\begin{cases}1-{\\\\frac {d(n)}{N}}&d(n)\\\\leq N\\\\\\\\0&{\\\\text{otherwise}}\\\\end{cases}}}, and whered(n){\\\\displaystyle d(n)}is the depth of the search and N is the anticipated length of the solution path.Sampled Dynamic Weighting uses sampling of nodes to better estimate and debias the heuristic error.Aε∗{\\\\displaystyle A_{\\\\varepsilon }^{*}}. uses two heuristic functions. The first is the FOCAL list, which is used to select candidate nodes, and the second hF is used to select the most promising node from the FOCAL list.Aε selects nodes with the functionAf(n)+BhF(n){\\\\displaystyle Af(n)+Bh_{F}(n)}, where A and B are constants. If no nodes can be selected, the algorithm will backtrack with the functionCf(n)+DhF(n){\\\\displaystyle Cf(n)+Dh_{F}(n)}, where C and D are constants.AlphA* attempts to promote depth-first exploitation by preferring recently expanded nodes. AlphA* uses the cost functionfα(n)=(1+wα(n))f(n){\\\\displaystyle f_{\\\\alpha }(n)=(1+w_{\\\\alpha }(n))f(n)}, wherewα(n)={λg(π(n))≤g(n~)Λotherwise{\\\\displaystyle w_{\\\\alpha }(n)={\\\\begin{cases}\\\\lambda &g(\\\\pi (n))\\\\leq g({\\\\tilde {n}})\\\\\\\\\\\\Lambda &{\\\\text{otherwise}}\\\\end{cases}}}, where λ and Λ are constants withλ≤Λ{\\\\displaystyle \\\\lambda \\\\leq \\\\Lambda }, π(n) is the parent of n, and ñ is the most recently expanded node.ComplexityThe time complexity of A* depends on the heuristic. In the worst case of an unbounded search space, the number of nodes expanded is exponential in the depth of the solution (the shortest path) d: O(bd), where b is the branching factor (the average number of successors per state). This assumes that a goal state exists at all, and is reachable from the start state; if it is not, and the state space is infinite, the algorithm will not terminate.The heuristic function has a major effect on the practical performance of A* search, since a good heuristic allows A* to prune away many of the bd nodes that an uninformed search would expand. Its quality can be expressed in terms of the effective branching factor b*, which can be determined empirically for a problem instance by measuring the number of nodes generated by expansion, N, and the depth of the solution, then solvingN+1=1+b∗+(b∗)2+⋯+(b∗)d.{\\\\displaystyle N+1=1+b^{*}+(b^{*})^{2}+\\\\dots +(b^{*})^{d}.}Good heuristics are those with low effective branching factor (the optimal being b* = 1).The time complexity is polynomial when the search space is a tree, there is a single goal state, and the heuristic function h meets the following condition:|h(x)−h∗(x)|=O(log\\u2061h∗(x)){\\\\displaystyle |h(x)-h^{*}(x)|=O(\\\\log h^{*}(x))}where h* is the optimal heuristic, the exact cost to get from x to the goal. In other words, the error of h will not grow faster than the logarithm of the \"perfect heuristic\" h* that returns the true distance from x to the goal.The space complexity of A* is roughly the same as that of all other graph search algorithms, as it keeps all generated nodes in memory. In practice, this turns out to be the biggest drawback of A* search, leading to the development of memory-bounded heuristic searches, such as Iterative deepening A*, memory bounded A*, and SMA*.ApplicationsA* is often used for the common pathfinding problem in applications such as video games, but was originally designed as a general graph traversal algorithm.It finds applications in diverse problems, including the problem of parsing using stochastic grammars in NLP.Other cases include an Informational search with online learning.Relations to other algorithmsWhat sets A* apart from a greedy best-first search algorithm is that it takes the cost/distance already traveled, g(n), into account.Some common variants of Dijkstra\\'s algorithm can be viewed as a special case of A* where the heuristich(n)=0{\\\\displaystyle h(n)=0}for all nodes; in turn, both Dijkstra and A* are special cases of dynamic programming.A* itself is a special case of a generalization of branch and bound.VariantsAnytime A*Block A*D*Field D*FringeFringe Saving A* (FSA*)Generalized Adaptive A* (GAA*)Incremental heuristic searchReduced A*Iterative deepening A* (IDA*)Jump point searchLifelong Planning A* (LPA*)New Bidirectional A* (NBA*)Simplified Memory bounded A* (SMA*)Theta*A* can also be adapted to a bidirectional search algorithm. Special care needs to be taken for the stopping criterion.See alsoBreadth-first searchDepth-first searchAny-angle path planning, search for paths that are not limited to move along graph edges but rather can take on any angleNotesReferencesFurther readingNilsson, N. J. (1980). Principles of Artificial Intelligence. Palo Alto, California: Tioga Publishing Company. ISBN 978-0-935382-01-3.External linksClear visual A* explanation, with advice and thoughts on path-findingVariation on A* called Hierarchical Path-Finding A* (HPA*)Brian Grinstead. \"A* Search Algorithm in JavaScript (Updated)\". Archived from the original on 15 February 2020. Retrieved 8 February 2021.',\n",
              " '= Bellman–Ford algorithm =The Bellman–Ford algorithm is an algorithm that computes shortest paths from a single source vertex to all of the other vertices in a weighted digraph.It is slower than Dijkstra\\'s algorithm for the same problem, but more versatile, as it is capable of handling graphs in which some of the edge weights are negative numbers.The algorithm was first proposed by Alfonso Shimbel (1955), but is instead named after Richard Bellman and Lester Ford Jr., who published it in 1958 and 1956, respectively. Edward F. Moore also published a variation of the algorithm in 1959, and for this reason it is also sometimes called the Bellman–Ford–Moore algorithm.Negative edge weights are found in various applications of graphs, hence the usefulness of this algorithm.If a graph contains a \"negative cycle\" (i.e. a cycle whose edges sum to a negative value) that is reachable from the source, then there is no cheapest path: any path that has a point on the negative cycle can be made cheaper by one more walk around the negative cycle. In such a case, the Bellman–Ford algorithm can detect and report the negative cycle.AlgorithmLike Dijkstra\\'s algorithm, Bellman–Ford proceeds by relaxation, in which approximations to the correct distance are replaced by better ones until they eventually reach the solution. In both algorithms, the approximate distance to each vertex is always an overestimate of the true distance, and is replaced by the minimum of its old value and the length of a newly found path.However, Dijkstra\\'s algorithm uses a priority queue to greedily select the closest vertex that has not yet been processed, and performs this relaxation process on all of its outgoing edges; by contrast, the Bellman–Ford algorithm simply relaxes all the edges, and does this|V|−1{\\\\displaystyle |V|-1}times, where|V|{\\\\displaystyle |V|}is the number of vertices in the graph. In each of these repetitions, the number of vertices with correctly calculated distances grows, from which it follows that eventually all vertices will have their correct distances. This method allows the Bellman–Ford algorithm to be applied to a wider class of inputs than Dijkstra. The intermediate answers depend on the order of edges relaxed, but the final answer remains the same.Bellman–Ford runs inO(|V|⋅|E|){\\\\displaystyle O(|V|\\\\cdot |E|)}time, where|V|{\\\\displaystyle |V|}and|E|{\\\\displaystyle |E|}are the number of vertices and edges respectively.function BellmanFord(list vertices, list edges, vertex source) is// This implementation takes in a graph, represented as// lists of vertices (represented as integers [0..n-1]) and edges,// and fills two arrays (distance and predecessor) holding// the shortest path from the source to each vertexdistance := list of size npredecessor := list of size n// Step 1: initialize graphfor each vertex v in vertices dodistance[v] := inf             // Initialize the distance to all vertices to infinitypredecessor[v] := null         // And having a null predecessordistance[source] := 0              // The distance from the source to itself is, of course, zero// Step 2: relax edges repeatedlyrepeat |V|−1 times:for each edge (u, v) with weight w in edges doif distance[u] + w < distance[v] thendistance[v] := distance[u] + wpredecessor[v] := u// Step 3: check for negative-weight cyclesfor each edge (u, v) with weight w in edges doif distance[u] + w < distance[v] thenerror \"Graph contains a negative-weight cycle\"return distance, predecessorSimply put, the algorithm initializes the distance to the source to 0 and all other nodes to infinity. Then for all edges, if the distance to the destination can be shortened by taking the edge, the distance is updated to the new lower value. At each iteration i that the edges are scanned, the algorithm finds all shortest paths of at most length i edges (and possibly some paths longer than i edges). Since the longest possible path without a cycle can be|V|−1{\\\\displaystyle |V|-1}edges, the edges must be scanned|V|−1{\\\\displaystyle |V|-1}times to ensure the shortest path has been found for all nodes. A final scan of all the edges is performed and if any distance is updated, then a path of length|V|{\\\\displaystyle |V|}edges has been found which can only occur if at least one negative cycle exists in the graph.Proof of correctnessThe correctness of the algorithm can be shown by induction:Lemma. After i repetitions of for loop,if Distance(u) is not infinity, it is equal to the length of some path from s to u; andif there is a path from s to u with at most i edges, then Distance(u) is at most the length of the shortest path from s to u with at most i edges.Proof. For the base case of induction, consider i=0 and the moment before for loop is executed for the first time. Then, for the source vertex, source.distance = 0, which is correct. For other vertices u, u.distance = infinity, which is also correct because there is no path from source to u with 0 edges.For the inductive case, we first prove the first part. Consider a moment when a vertex\\'s distance is updated byv.distance := u.distance + uv.weight. By inductive assumption, u.distance is the length of some path from source to u. Then u.distance + uv.weight is the length of the path from source to v that follows the path from  source to u and then goes to v.For the second part, consider a shortest path P (there may be more than one) from source to v with at most i edges. Let u be the last vertex before v on this path. Then, the part of the path from source to u is a shortest path from source to u with at most i-1 edges, since if it were not, then there must be some strictly shorter path from source to u with at most i-1 edges, and we could then append the edge uv to this path to obtain a path with at most i edges that is strictly shorter than P—a contradiction. By inductive assumption, u.distance after i−1 iterations is at most the length of this path from source to u. Therefore, uv.weight + u.distance is at most the length of P. In the ith iteration, v.distance gets compared with uv.weight + u.distance, and is set equal to it if uv.weight + u.distance is smaller. Therefore, after i iterations, v.distance is at most the length of P, i.e., the length of the shortest path from source to v that uses at most i edges.If there are no negative-weight cycles, then every shortest path visits each vertex at most once, so at step 3 no further improvements can be made. Conversely, suppose no improvement can be made. Then for any cycle with vertices v[0], ..., v[k−1],v[i].distance <= v[i-1 (mod k)].distance + v[i-1 (mod k)]v[i].weightSumming around the cycle, the v[i].distance and v[i−1 (mod k)].distance terms cancel, leaving0 <= sum from 1 to k of v[i-1 (mod k)]v[i].weightI.e., every cycle has nonnegative weight.Finding negative cyclesWhen the algorithm is used to find shortest paths, the existence of negative cycles is a problem, preventing the algorithm from finding a correct answer. However, since it terminates upon finding a negative cycle, the Bellman–Ford algorithm can be used for applications in which this is the target to be sought – for example in cycle-cancelling techniques in network flow analysis.Applications in routingA distributed variant of the Bellman–Ford algorithm is used in distance-vector routing protocols, for example the Routing Information Protocol (RIP). The algorithm is distributed because it involves a number of nodes (routers) within an Autonomous system (AS), a collection of IP networks typically owned by an ISP.It consists of the following steps:Each node calculates the distances between itself and all other nodes within the AS and stores this information as a table.Each node sends its table to all neighboring nodes.When a node receives distance tables from its neighbors, it calculates the shortest routes to all other nodes and updates its own table to reflect any changes.The main disadvantages of the Bellman–Ford algorithm in this setting are as follows:It does not scale well.Changes in network topology are not reflected quickly since updates are spread node-by-node.Count to infinity if link or node failures render a node unreachable from some set of other nodes, those nodes may spend forever gradually increasing their estimates of the distance to it, and in the meantime there may be routing loops.ImprovementsThe Bellman–Ford algorithm may be improved in practice (although not in the worst case) by the observation that, if an iteration of the main loop of the algorithm terminates without making any changes, the algorithm can be immediately terminated, as subsequent iterations will not make any more changes. With this early termination condition, the main loop may in some cases use many fewer than |V| − 1 iterations, even though the worst case of the algorithm remains unchanged. The following improvements all maintain theO(|V|⋅|E|){\\\\displaystyle O(|V|\\\\cdot |E|)}worst-case time complexity.A variation of the Bellman-Ford algorithm known as Shortest Path Faster Algorithm, first described by Moore (1959), reduces the number of relaxation steps that need to be performed within each iteration of the algorithm. If a vertex v has a distance value that has not changed since the last time the edges out of v were relaxed, then there is no need to relax the edges out of v a second time. In this way, as the number of vertices with correct distance values grows, the number whose outgoing edges that need to be relaxed in each iteration shrinks, leading to a constant-factor savings in time for dense graphs.Yen (1970) described another improvement to the Bellman–Ford algorithm. His improvement first assigns some arbitrary linear order on all vertices and then partitions the set of all edges into two subsets. The first subset, Ef, contains all edges (vi, vj) such that i < j; the second, Eb, contains edges (vi, vj) such that i > j. Each vertex is visited in the order v1, v2, ..., v|V|, relaxing each outgoing edge from that vertex in Ef. Each vertex is then visited in the order v|V|, v|V|−1, ..., v1, relaxing each outgoing edge from that vertex in Eb. Each iteration of the main loop of the algorithm, after the first one, adds at least two edges to the set of edges whose relaxed distances match the correct shortest path distances: one from Ef and one from Eb. This modification reduces the worst-case number of iterations of the main loop of the algorithm from |V| − 1 to|V|/2{\\\\displaystyle |V|/2}.Another improvement, by Bannister & Eppstein (2012), replaces the arbitrary linear order of the vertices used in Yen\\'s second improvement by a random permutation. This change makes the worst case for Yen\\'s improvement (in which the edges of a shortest path strictly alternate between the two subsets Ef and Eb) very unlikely to happen. With a randomly permuted vertex ordering, the expected number of iterations needed in the main loop is at most|V|/3{\\\\displaystyle |V|/3}.NotesReferencesOriginal sourcesShimbel, A. (1955). Structure in communication nets. Proceedings of the Symposium on Information Networks. New York, New York: Polytechnic Press of the Polytechnic Institute of Brooklyn. pp. 199–203.Bellman, Richard (1958). \"On a routing problem\". Quarterly of Applied Mathematics. 16: 87–90. doi:10.1090/qam/102435. MR 0102435.Ford, Lester R. Jr. (August 14, 1956). Network Flow Theory. Paper P-923. Santa Monica, California: RAND Corporation.Moore, Edward F. (1959). The shortest path through a maze. Proc. Internat. Sympos. Switching Theory 1957, Part II. Cambridge, Massachusetts: Harvard Univ. Press. pp. 285–292. MR 0114710.Yen, Jin Y. (1970). \"An algorithm for finding shortest routes from all source nodes to a given destination in general networks\". Quarterly of Applied Mathematics. 27 (4): 526–530. doi:10.1090/qam/253822. MR 0253822.Bannister, M. J.; Eppstein, D. (2012). Randomized speedup of the Bellman–Ford algorithm. Analytic Algorithmics and Combinatorics (ANALCO12), Kyoto, Japan. pp. 41–47. arXiv:1111.5414. doi:10.1137/1.9781611973020.6.Secondary sourcesBang-Jensen, Jørgen; Gutin, Gregory (2000). \"Section 2.3.4: The Bellman-Ford-Moore algorithm\". Digraphs: Theory, Algorithms and Applications (First ed.). ISBN 978-1-84800-997-4.Schrijver, Alexander (2005). \"On the history of combinatorial optimization (till 1960)\" (PDF). Handbook of Discrete Optimization. Elsevier: 1–68.Cormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L. Introduction to Algorithms. MIT Press and McGraw-Hill., Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7. Section 24.1: The Bellman–Ford algorithm, pp. 588–592. Problem 24-1, pp. 614–615. Third Edition. MIT Press, 2009. ISBN 978-0-262-53305-8. Section 24.1: The Bellman–Ford algorithm, pp. 651–655.Heineman, George T.; Pollice, Gary; Selkow, Stanley (2008). \"Chapter 6: Graph Algorithms\". Algorithms in a Nutshell. O\\'Reilly Media. pp. 160–164. ISBN 978-0-596-51624-6.Kleinberg, Jon; Tardos, Éva (2006). Algorithm Design. New York: Pearson Education, Inc.Sedgewick, Robert (2002). \"Section 21.7: Negative Edge Weights\". Algorithms in Java (3rd ed.). ISBN 0-201-36121-3. Archived from the original on 2008-05-31. Retrieved 2007-05-28.',\n",
              " '= K shortest path routing =The k shortest path routing problem is a generalization of the shortest path routing problem in a given network. It asks not only about a shortest path but also about next k−1 shortest paths (which may be longer than the shortest path). A variation of the problem is the loopless k shortest paths.Finding k shortest paths is possible by extending Dijkstra algorithm or Bellman-Ford algorithm and extend them to find more than one path.HistorySince 1957 many papers were published on the k shortest path routing problem. Most of the fundamental works were done between 1960s and 2001. Since then, most of the research has been on the problem\\'s applications and its variants. In 2010, Michael Günther et al. published a book on Symbolic calculation of k-shortest paths and related measures with the stochastic process algebra tool CASPA.AlgorithmThe Dijkstra algorithm can be generalized to find the k shortest paths.VariationsThere are two main variations of the k shortest path routing problem.  In one variation, paths are allowed to visit the same node more than once, thus creating loops.  In another variation, paths are required to be simple and loopless.  The loopy version is solvable using Eppstein\\'s algorithm and the loopless variation is solvable by Yen\\'s algorithm.Loopy variantIn this variant, the problem is simplified by not requiring paths to be loopless.  A solution was given by B. L. Fox in 1975 in which the k-shortest paths are determined in O(m + kn log n) asymptotic time complexity (using big O notation.  In 1998, David Eppstein reported an approach that maintains an asymptotic complexity of O(m + n log n + k) by computing an implicit representation of the paths, each of which can be output in O(n) extra time. In 2015, Akuba et al. devised an indexing method as a significantly faster alternative for Eppstein\\'s algorithm, in which a data structure called an index is constructed from a graph and then top-k distances between arbitrary pairs of vertices can be rapidly obtained.Loopless variantIn the loopless variant, the paths are forbidden to contain loops which adds an additional level of complexity. It can be solved using Yen\\'s algorithm to find the lengths of all shortest paths from a fixed node to all other nodes in an n-node non negative-distance network, a technique requiring only 2n2 additions and n2 comparison, fewer than other available shortest path algorithms need.  The running time complexity is pseudo-polynomial, being O(kn(m + n log n)) (where m and n represent the number of edges and vertices, respectively). In 2007, John Hershberger and Subhash Suri proposed a replacement paths algorithm, a more efficient implementation of Lawler\\'s  and Yen\\'s algorithm with O(n) improvement in time.Some examples and descriptionExample #1The following example makes use of Yen’s model to find k shortest paths between communicating end nodes. That is, it finds a shortest path, second shortest path, etc. up to the Kth shortest path. More details can be found here.The code provided in this example attempts to solve the k shortest path routing problem for a 15-nodes network containing a combination of unidirectional and bidirectional links:Example #2Another example is the use of k shortest paths algorithm to track multiple objects. The technique implements a multiple object tracker based on the k shortest paths routing algorithm. A set of probabilistic occupancy maps is used as input. An object detector provides the input.The complete details can be found at \"Computer Vision Laboratory – CVLAB\".Example #3Another use of k shortest paths algorithms is to design a transit network that enhances passengers\\' experience in public transportation systems. Such an example of a transit network can be constructed by putting traveling time under consideration. In addition to traveling time, other conditions may be taken depending upon economical and geographical limitations. Despite variations in parameters, the k shortest path algorithms finds the most optimal solutions that satisfies almost all user needs. Such applications of k shortest path algorithms are becoming common, recently Xu, He, Song, and Chaudry (2012) studied the k shortest path problems in transit network systems.ApplicationsThe k shortest path routing is a good alternative for:Geographic path planningNetwork routing, especially in optical mesh network where there are additional constraints that cannot be solved by using ordinary shortest path algorithms.Hypothesis generation in computational linguisticsSequence alignment and metabolic pathway finding in bioinformaticsMultiple object tracking as described aboveRoad Networks: road junctions are the nodes (vertices) and each  edge (link) of the graph is associated with a road segment between two junctions.Related problemsThe breadth-first search algorithm is used when the search is only limited to two operations.The Floyd–Warshall algorithm solves all pairs shortest paths.Johnson\\'s algorithm solves all pairs\\' shortest paths, and may be faster than Floyd–Warshall on sparse graphs.Perturbation theory finds (at worst) the locally shortest path.Cherkassky et al. provide more algorithms and associated evaluations.See alsoConstrained shortest path routingNotesExternal linksImplementation of Yen\\'s algorithmImplementation of Yen\\'s and fastest k shortest simple paths algorithmshttp://www.technical-recipes.com/2012/the-k-shortest-paths-algorithm-in-c/#more-2432Multiple objects tracking technique using K-shortest path algorithm: http://cvlab.epfl.ch/software/ksp/Computer Vision Laboratory: http://cvlab.epfl.ch/software/ksp/',\n",
              " \"= Widest path problem =In graph algorithms, the widest path problem is the problem of finding a path between two designated vertices in a weighted graph, maximizing the weight of the minimum-weight edge in the path. The widest path problem is also known as the maximum capacity path problem. It is possible to adapt most shortest path algorithms to compute widest paths, by modifying them to use the bottleneck distance instead of path length. However, in many cases even faster algorithms are possible.For instance, in a graph that represents connections between routers in the Internet, where the weight of an edge represents the bandwidth of a connection between two routers, the widest path problem is the problem of finding an end-to-end path between two Internet nodes that has the maximum possible bandwidth.  The smallest edge weight on this path is known as the capacity or bandwidth of the path. As well as its applications in network routing, the widest path problem is also an important component of the Schulze method for deciding the winner of a multiway election, and has been applied to digital compositing, metabolic pathway analysis, and the computation of maximum flows.A closely related problem, the minimax path problem or bottleneck shortest path problem asks for the path that minimizes the maximum weight of any of its edges. It has applications that include transportation planning. Any algorithm for the widest path problem can be transformed into an algorithm for the minimax path problem, or vice versa, by reversing the sense of all the weight comparisons performed by the algorithm, or equivalently by replacing every edge weight by its negation.Undirected graphsIn an undirected graph, a widest path may be found as the path between the two vertices in the maximum spanning tree of the graph, and a minimax path may be found as the path between the two vertices in the minimum spanning tree.In any graph, directed or undirected, there is a straightforward algorithm for finding a widest path once the weight of its minimum-weight edge is known: simply delete all smaller edges and search for any path among the remaining edges using breadth first search or depth first search. Based on this test, there also exists a linear time algorithm for finding a widest s-t path in an undirected graph, that does not use the maximum spanning tree. The main idea of the algorithm is to apply the linear-time path-finding algorithm to the median edge weight in the graph, and then either to delete all smaller edges or contract all larger edges according to whether a path does or does not exist, and recurse in the resulting smaller graph.Fernández, Garfinkel & Arbiol (1998) use undirected bottleneck shortest paths in order to form composite aerial photographs that combine multiple images of overlapping areas. In the subproblem to which the widest path problem applies, two images have already been transformed into a common coordinate system; the remaining task is to select a seam, a curve that passes through the region of overlap and divides one of the two images from the other. Pixels on one side of the seam will be copied from one of the images, and pixels on the other side of the seam will be copied from the other image. Unlike other compositing methods that average pixels from both images, this produces a valid photographic image of every part of the region being photographed. They weight the edges of a grid graph by a numeric estimate of how visually apparent a seam across that edge would be, and find a bottleneck shortest path for these weights. Using this path as the seam, rather than a more conventional shortest path, causes their system to find a seam that is difficult to discern at all of its points, rather than allowing it to trade off greater visibility in one part of the image for lesser visibility elsewhere.A solution to the minimax path problem between the two opposite corners of a grid graph can be used to find the weak Fréchet distance between two polygonal chains. Here, each grid graph vertex represents a pair of line segments, one from each chain, and the weight of an edge represents the Fréchet distance needed to pass from one pair of segments to another.If all edge weights of an undirected graph are positive, then the minimax distances between pairs of points (the maximum edge weights of minimax paths) form an ultrametric; conversely every finite ultrametric space comes from minimax distances in this way. A data structure constructed from the minimum spanning tree allows the minimax distance between any pair of vertices to be queried in constant time per query, using lowest common ancestor queries in a Cartesian tree. The root of the Cartesian tree represents the heaviest minimum spanning tree edge, and the children of the root are Cartesian trees recursively constructed from the subtrees of the minimum spanning tree formed by removing the heaviest edge. The leaves of the Cartesian tree represent the vertices of the input graph, and the minimax distance between two vertices equals the weight of the Cartesian tree node that is their lowest common ancestor. Once the minimum spanning tree edges have been sorted, this Cartesian tree can be constructed in linear time.Directed graphsIn directed graphs, the maximum spanning tree solution cannot be used. Instead, several different algorithms are known; the choice of which algorithm to use depends on whether a start or destination vertex for the path is fixed, or whether paths for many start or destination vertices must be found simultaneously.All pairsThe all-pairs widest path problem has applications in the Schulze method for choosing a winner in multiway elections in which voters rank the candidates in preference order. The Schulze method constructs a complete directed graph in which the vertices represent the candidates and every two vertices are connected by an edge. Each edge is directed from the winner to the loser of a pairwise contest between the two candidates it connects, and is labeled with the margin of victory of that contest. Then the method computes widest paths between all pairs of vertices, and the winner is the candidate whose vertex has wider paths to each opponent than vice versa. The results of an election using this method are consistent with the Condorcet method – a candidate who wins all pairwise contests automatically wins the whole election – but it generally allows a winner to be selected, even in situations where the Concorcet method itself fails. The Schulze method has been used by several organizations including the Wikimedia Foundation.To compute the widest path widths for all pairs of nodes in a dense directed graph, such as the ones that arise in the voting application, the asymptotically fastest known approach takes time O(n(3+ω)/2) where ω is the exponent for fast matrix multiplication. Using the best known algorithms for matrix multiplication, this time bound becomes  O(n2.688). Instead, the reference implementation for the Schulze method uses a modified version of the simpler Floyd–Warshall algorithm, which takes O(n3) time. For sparse graphs, it may be more efficient to repeatedly apply a single-source widest path algorithm.Single sourceIf the edges are sorted by their weights, then a modified version of Dijkstra's algorithm can compute the bottlenecks between a designated start vertex and every other vertex in the graph, in linear time. The key idea behind the speedup over a conventional version of Dijkstra's algorithm is that the sequence of bottleneck distances to each vertex, in the order that the vertices are considered by this algorithm, is a monotonic subsequence of the sorted sequence of edge weights; therefore, the priority queue of Dijkstra's algorithm can be implemented as a bucket queue: an array indexed by the numbers from 1 to m (the number of edges in the graph), where array cell i contains the vertices whose bottleneck distance is the weight of the edge with position i in the sorted order. This method allows the widest path problem to be solved as quickly as sorting; for instance, if the edge weights are represented as integers, then the time bounds for integer sorting a list of m integers would apply also to this problem.Single source and single destinationBerman & Handler (1987) suggest that service vehicles and emergency vehicles should use minimax paths when returning from a service call to their base. In this application, the time to return is less important than the response time if another service call occurs while the vehicle is in the process of returning. By using a minimax path, where the weight of an edge is the maximum travel time from a point on the edge to the farthest possible service call, one can plan a route that minimizes the maximum possible delay between receipt of a service call and arrival of a responding vehicle. Ullah, Lee & Hassoun (2009) use maximin paths to model the dominant reaction chains in metabolic networks; in their model, the weight of an edge is the free energy of the metabolic reaction represented by the edge.Another application of widest paths arises in the Ford–Fulkerson algorithm for the maximum flow problem. Repeatedly augmenting a flow along a maximum capacity path in the residual network of the flow leads to a small bound, O(m log U), on the number of augmentations needed to find a maximum flow; here, the edge capacities are assumed to be integers that are at most U. However, this analysis does not depend on finding a path that has the exact maximum of capacity; any path whose capacity is within a constant factor of the maximum suffices. Combining this approximation idea with the shortest path augmentation method of the Edmonds–Karp algorithm leads to a maximum flow algorithm with running time O(mn log U).It is possible to find maximum-capacity paths and minimax paths with a single source and single destination very efficiently even in models of computation that allow only comparisons of the input graph's edge weights and not arithmetic on them. The algorithm maintains a set S of edges that are known to contain the bottleneck edge of the optimal path; initially, S is just the set of all m edges of the graph. At each iteration of the algorithm, it splits S into an ordered sequence of subsets S1, S2, ... of approximately equal size; the number of subsets in this partition is chosen in such a way that all of the split points between subsets can be found by repeated median-finding in time O(m). The algorithm then reweights each edge of the graph by the index of the subset containing the edge, and uses the modified Dijkstra algorithm on the reweighted graph; based on the results of this computation, it can determine in linear time which of the subsets contains the bottleneck edge weight. It then replaces S by the subset Si that it has determined to contain the bottleneck weight, and starts the next iteration with this new set S. The number of subsets into which S can be split increases exponentially with each step, so the number of iterations is proportional to the iterated logarithm function, O(log*n), and the total time is O(m log*n). In a model of computation where each edge weight is a machine integer, the use of repeated bisection in this algorithm can be replaced by a list-splitting technique of Han & Thorup (2002), allowing S to be split into O(√m) smaller sets Si in a single step and leading to a linear overall time bound.Euclidean point setsA variant of the minimax path problem has also been considered for sets of points in the Euclidean plane. As in the undirected graph problem, this Euclidean minimax path problem can be solved efficiently by finding a Euclidean minimum spanning tree: every path in the tree is a minimax path. However, the problem becomes more complicated when a path is desired that not only minimizes the hop length but also, among paths with the same hop length, minimizes or approximately minimizes the total length of the path. The solution can be approximated using geometric spanners.In number theory, the unsolved Gaussian moat problem asks whether or not minimax paths in the Gaussian prime numbers have bounded or unbounded minimax length. That is, does there exist a constant B such that, for every pair of points p and q in the infinite Euclidean point set defined by the Gaussian primes, the minimax path in the Gaussian primes between p and q has minimax edge length at most B?== References ==\"]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "do_search('Dijtra') # typo on purpose (Dijtra instead of Dijkstra)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pLrQ3IFYnu0",
        "outputId": "aeab19fa-cfe3-4387-a562-db79cedfd44c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['= Dijkstra\\'s algorithm =Dijkstra\\'s algorithm ( DYKE-strəz) is an algorithm for finding the shortest paths between nodes in a graph, which may represent, for example, road networks.  It was conceived by computer scientist Edsger W. Dijkstra in 1956 and published three years later.The algorithm exists in many variants. Dijkstra\\'s original algorithm found the shortest path between two given nodes, but a more common variant fixes a single node as the \"source\" node and finds shortest paths from the source to all other nodes in the graph, producing a shortest-path tree.For a given source node in the graph, the algorithm finds the shortest path between that node and every other.:\\u200a196–206\\u200a It can also be used for finding the shortest paths from a single node to a single destination node by stopping the algorithm once the shortest path to the destination node has been determined. For example, if the nodes of the graph represent cities and edge path costs represent driving distances between pairs of cities connected by a direct road (for simplicity, ignore red lights, stop signs, toll roads and other obstructions), Dijkstra\\'s algorithm can be used to find the shortest route between one city and all other cities. A widely used application of shortest path algorithms is network routing protocols, most notably IS-IS (Intermediate System to Intermediate System) and Open Shortest Path First (OSPF). It is also employed as a subroutine in other algorithms such as Johnson\\'s.The Dijkstra algorithm uses labels that are positive integers or real numbers, which are totally ordered.  It can be generalized to use any labels that are partially ordered, provided the subsequent labels (a subsequent label is produced when traversing an edge) are monotonically non-decreasing.  This generalization is called the generic Dijkstra shortest-path algorithm.Dijkstra\\'s algorithm uses a data structure for storing and querying partial solutions sorted by distance from the start. While the original algorithm uses a min-priority queue and runs in timeΘ((|V|+|E|)log\\u2061|V|){\\\\displaystyle \\\\Theta ((|V|+|E|)\\\\log |V|)}(where|V|{\\\\displaystyle |V|}is the number of nodes and|E|{\\\\displaystyle |E|}is the number of edges), it can also be implemented inΘ(|V|2){\\\\displaystyle \\\\Theta (|V|^{2})}using an array. The idea of this algorithm is also given in Leyzorek et al. 1957. Fredman & Tarjan 1984 propose using a Fibonacci heap min-priority queue to optimize the running time complexity toΘ(|E|+|V|log\\u2061|V|){\\\\displaystyle \\\\Theta (|E|+|V|\\\\log |V|)}. This is asymptotically the fastest known single-source shortest-path algorithm for arbitrary directed graphs with unbounded non-negative weights. However, specialized cases (such as bounded/integer weights, directed acyclic graphs etc.) can indeed be improved further as detailed in Specialized variants. Additionally, if preprocessing is allowed algorithms such as contraction hierarchies can be up to seven orders of magnitude faster.In some fields, artificial intelligence in particular, Dijkstra\\'s algorithm or a variant of it is known as uniform cost search and formulated as an instance of the more general idea of best-first search.HistoryWhat is the shortest way to travel from Rotterdam to Groningen, in general: from given city to given city. It is the algorithm for the shortest path, which I designed in about twenty minutes. One morning I was shopping in Amsterdam with my young fiancée, and tired, we sat down on the café terrace to drink a cup of coffee and I was just thinking about whether I could do this, and I then designed the algorithm for the shortest path. As I said, it was a twenty-minute invention. In fact, it was published in \\'59, three years later. The publication is still readable, it is, in fact, quite nice. One of the reasons that it is so nice was that I designed it without pencil and paper. I learned later that one of the advantages of designing without pencil and paper is that you are almost forced to avoid all avoidable complexities. Eventually, that algorithm became to my great amazement, one of the cornerstones of my fame.Dijkstra thought about the shortest path problem when working at the Mathematical Center in Amsterdam in 1956 as a programmer to demonstrate the capabilities of a new computer called ARMAC. His objective was to choose both a problem and a solution (that would be produced by computer) that non-computing people could understand. He designed the shortest path algorithm and later implemented it for ARMAC for a slightly simplified transportation map of 64 cities in the Netherlands (64, so that 6 bits would be sufficient to encode the city number). A year later, he came across another problem from hardware engineers working on the institute\\'s next computer: minimize the amount of wire needed to connect the pins on the back panel of the machine. As a solution, he re-discovered the algorithm known as Prim\\'s minimal spanning tree algorithm (known earlier to Jarník, and also rediscovered by Prim). Dijkstra published the algorithm in 1959, two years after Prim and 29 years after Jarník.AlgorithmLet the node at which we are starting be called the initial node. Let the distance of node Y be the distance from the initial node to Y. Dijkstra\\'s algorithm will initially start with infinite distances and will try to improve them step by step.Mark all nodes unvisited. Create a set of all the unvisited nodes called the unvisited set.Assign to every node a tentative distance value: set it to zero for our initial node and to infinity for all other nodes. The tentative distance of a node v is the length of the shortest path discovered so far between the node v and the starting node. Since initially no path is known to any other vertex than the source itself (which is a path of length zero), all other tentative distances are initially set to infinity. Set the initial node as current.For the current node, consider all of its unvisited neighbors and calculate their tentative distances through the current node. Compare the newly calculated tentative distance to the current assigned value and assign the smaller one. For example, if the current node A is marked with a  distance of 6, and the edge connecting it with a neighbor B has length 2, then the distance to B through A will be 6 + 2 = 8. If B was previously marked with a distance greater than 8 then change it to 8. Otherwise, the current value will be kept.When we are done considering all of the unvisited neighbors of the current node, mark the current node as visited and remove it from the unvisited set. A visited node will never be checked again.If the destination node has been marked visited (when planning a route between two specific nodes) or if the smallest tentative distance among the nodes in the unvisited set is infinity (when planning a complete traversal; occurs when there is no connection between the initial node and remaining unvisited nodes), then stop. The algorithm has finished.Otherwise, select the unvisited node that is marked with the smallest tentative distance, set it as the new current node, and go back to step 3.When planning a route, it is actually not necessary to wait until the destination node is \"visited\" as above: the algorithm can stop once the destination node has the smallest tentative distance among all \"unvisited\" nodes (and thus could be selected as the next \"current\").DescriptionSuppose you would like to find the shortest path between two intersections on a city map: a starting point and a destination. Dijkstra\\'s algorithm initially marks the distance (from the starting point) to every other intersection on the map with infinity. This is done not to imply that there is an infinite distance, but to note that those intersections have not been visited yet. Some variants of this method leave the intersections\\' distances unlabeled. Now select the current intersection at each iteration.  For the first iteration, the current intersection will be the starting point, and the distance to it (the intersection\\'s label) will be zero. For subsequent iterations (after the first), the current intersection will be a closest unvisited intersection to the starting point (this will be easy to find).From the current intersection, update the distance to every unvisited intersection that is directly connected to it. This is done by determining the sum of the distance between an unvisited intersection and the value of the current intersection and then relabeling the unvisited intersection with this value (the sum) if it is less than the unvisited intersection\\'s current value. In effect, the intersection is relabeled if the path to it through the current intersection is shorter than the previously known paths.  To facilitate shortest path identification, in pencil, mark the road with an arrow pointing to the relabeled intersection if you label/relabel it, and erase all others pointing to it.  After you have updated the distances to each neighboring intersection, mark the current intersection as visited and select an unvisited intersection with minimal distance (from the starting point) – or the lowest label—as the current intersection. Intersections marked as visited are labeled with the shortest path from the starting point to it and will not be revisited or returned to.Continue this process of updating the neighboring intersections with the shortest distances, marking the current intersection as visited, and moving onto a closest unvisited intersection until you have marked the destination as visited. Once you have marked the destination as visited (as is the case with any visited intersection), you have determined the shortest path to it from the starting point and can trace your way back following the arrows in reverse. In the algorithm\\'s implementations, this is usually done (after the algorithm has reached the destination node) by following the nodes\\' parents from the destination node up to the starting node; that\\'s why we also keep track of each node\\'s parent.This algorithm makes no attempt of direct \"exploration\" towards the destination as one might expect. Rather, the sole consideration in determining the next \"current\" intersection is its distance from the starting point. This algorithm therefore expands outward from the starting point, interactively considering every node that is closer in terms of shortest path distance until it reaches the destination. When understood in this way, it is clear how the algorithm necessarily finds the shortest path. However, it may also reveal one of the algorithm\\'s weaknesses: its relative slowness in some topologies.PseudocodeIn the following pseudocode algorithm, dist is an array that contains the current distances from the source to other vertices, i.e. dist[u] is the current distance from the source to the vertex u. The prev array contains pointers to previous-hop nodes on the shortest path from source to the given vertex (equivalently, it is the next-hop on the path from the given vertex to the source). The code u ← vertex in Q with min dist[u], searches for the vertex u in the vertex set Q that has the least dist[u] value. Graph.Edges(u, v) returns the length of the edge joining (i.e. the distance between) the two neighbor-nodes u and v. The variable alt on line 14 is the length of the path from the root node to the neighbor node v if it were to go through u. If this path is shorter than the current shortest path recorded for v, that current path is replaced with this alt path.1  function Dijkstra(Graph, source):23      for each vertex v in Graph.Vertices:4          dist[v] ← INFINITY5          prev[v] ← UNDEFINED6          add v to Q7      dist[source] ← 089      while Q is not empty:10          u ← vertex in Q with min dist[u]1112          for each neighbor v of u still in Q:13              alt ← dist[u] + Graph.Edges(u, v)14              if alt < dist[v]:15                  dist[v] ← alt16                  prev[v] ← u1718      return dist[], prev[]If we are only interested in a shortest path between vertices source and target, we can terminate the search after line 10 if u = target.Now we can read the shortest path from source to target by reverse iteration:1  S ← empty sequence2  u ← target3  if prev[u] is defined or u = source:          // Do something only if the vertex is reachable4      while u is defined:                       // Construct the shortest path with a stack S5          insert u at the beginning of S        // Push the vertex onto the stack6          u ← prev[u]                           // Traverse from target to sourceNow sequence S is the list of vertices constituting one of the shortest paths from source to target, or the empty sequence if no path exists.A more general problem would be to find all the shortest paths between source and target (there might be several different ones of the same length). Then instead of storing only a single node in each entry of prev[] we would store all nodes satisfying the relaxation condition. For example, if both r and source connect to target and both of them lie on different shortest paths through target (because the edge cost is the same in both cases), then we would add both r and source to prev[target]. When the algorithm completes, prev[] data structure will actually describe a graph that is a subset of the original graph with some edges removed. Its key property will be that if the algorithm was run with some starting node, then every path from that node to any other node in the new graph will be the shortest path between those nodes in the original graph, and all paths of that length from the original graph will be present in the new graph. Then to actually find all these shortest paths between two given nodes we would use a path finding algorithm on the new graph, such as depth-first search.Using a priority queueA min-priority queue is an abstract data type that provides 3 basic operations: add_with_priority(), decrease_priority() and extract_min(). As mentioned earlier, using such a data structure can lead to faster computing times than using a basic queue. Notably, Fibonacci heap (Fredman & Tarjan 1984) or Brodal queue offer optimal implementations for those 3 operations. As the algorithm is slightly different, we mention it here, in pseudo-code as well :1  function Dijkstra(Graph, source):2      dist[source] ← 0                           // Initialization34      create vertex priority queue Q56      for each vertex v in Graph.Vertices:7          if v ≠ source8              dist[v] ← INFINITY                 // Unknown distance from source to v9              prev[v] ← UNDEFINED                // Predecessor of v1011         Q.add_with_priority(v, dist[v])121314     while Q is not empty:                      // The main loop15         u ← Q.extract_min()                    // Remove and return best vertex16         for each neighbor v of u:              // only v that are still in Q17             alt ← dist[u] + Graph.Edges(u, v)18             if alt < dist[v]19                 dist[v] ← alt20                 prev[v] ← u21                 Q.decrease_priority(v, alt)2223     return dist, prevInstead of filling the priority queue with all nodes in the initialization phase, it is also possible to initialize it to contain only source; then, inside the if alt < dist[v] block, the decrease_priority() becomes an add_with_priority() operation if the node is not already in the queue.:\\u200a198\\u200aYet another alternative is to add nodes unconditionally to the priority queue and to instead check after extraction that no shorter connection was found yet. This can be done by additionally extracting the associated priority p from the queue and only processing further if p == dist[u] inside the while Q is not empty loop. These alternatives can use entirely array-based priority queues without decrease-key functionality, which have been found to achieve even faster computing times in practice. However, the difference in performance was found to be narrower for denser graphs.Proof of correctnessProof of Dijkstra\\'s algorithm is constructed by induction on the number of visited nodes.Invariant hypothesis: For each node v, dist[v] is the shortest distance from source to v when traveling via visited nodes only, or infinity if no such path exists. (Note: we do not assume dist[v] is the actual shortest distance for unvisited nodes.)The base case is when there is just one visited node, namely the initial node source, in which case the hypothesis is trivial.Otherwise, assume the hypothesis for n-1 visited nodes. In which case, we choose an edge vu where u has the least dist[u] of any unvisited nodes such that dist[u] = dist[v] + Graph.Edges[v,u]. dist[u] is considered to be the shortest distance from source to u because if there were a shorter path, and if w was the first unvisited node on that path then by the original hypothesis dist[w] > dist[u] which creates a contradiction. Similarly if there were a shorter path to u without using unvisited nodes, and if the last but one node on that path were w, then we would have had dist[u] = dist[w] + Graph.Edges[w,u], also a contradiction.After processing u it will still be true that for each unvisited node w, dist[w] will be the shortest distance from source to w using visited nodes only, because if there were a shorter path that doesn\\'t go by u we would have found it previously, and if there were a shorter path using u we would have updated it when processing u.After all nodes are visited, the shortest path from source to any node v consists only of visited nodes, therefore dist[v] is the shortest distance.Running timeBounds of the running time of Dijkstra\\'s algorithm on a graph with edges E and vertices V can be expressed as a function of the number of edges, denoted|E|{\\\\displaystyle |E|}, and the number of vertices, denoted|V|{\\\\displaystyle |V|}, using big-O notation. The complexity bound depends mainly on the data structure used to represent the set Q. In the following, upper bounds can be simplified because|E|{\\\\displaystyle |E|}isO(|V|2){\\\\displaystyle O(|V|^{2})}for any graph, but that simplification disregards the fact that in some problems, other upper bounds on|E|{\\\\displaystyle |E|}may hold.For any data structure for the vertex set Q, the running time is inΘ(|E|⋅Tdk+|V|⋅Tem),{\\\\displaystyle \\\\Theta (|E|\\\\cdot T_{\\\\mathrm {dk} }+|V|\\\\cdot T_{\\\\mathrm {em} }),}whereTdk{\\\\displaystyle T_{\\\\mathrm {dk} }}andTem{\\\\displaystyle T_{\\\\mathrm {em} }}are the complexities of the decrease-key and extract-minimum operations in Q, respectively.The simplest version of Dijkstra\\'s algorithm stores the vertex set Q as an linked list or array, and edges as an adjacency list or matrix. In this case, extract-minimum is simply a linear search through all vertices in Q, so the running time isΘ(|E|+|V|2)=Θ(|V|2){\\\\displaystyle \\\\Theta (|E|+|V|^{2})=\\\\Theta (|V|^{2})}.For sparse graphs, that is, graphs with far fewer than|V|2{\\\\displaystyle |V|^{2}}edges, Dijkstra\\'s algorithm can be implemented more efficiently by storing the graph in the form of adjacency lists and using a self-balancing binary search tree, binary heap, pairing heap, or Fibonacci heap as a priority queue to implement extracting minimum efficiently. To perform decrease-key steps in a binary heap efficiently, it is necessary to use an auxiliary data structure that maps each vertex to its position in the heap, and to keep this structure up to date as the priority queue Q changes. With a self-balancing binary search tree or binary heap, the algorithm requiresΘ((|E|+|V|)log\\u2061|V|){\\\\displaystyle \\\\Theta ((|E|+|V|)\\\\log |V|)}time in the worst case (wherelog{\\\\displaystyle \\\\log }denotes the binary logarithmlog2{\\\\displaystyle \\\\log _{2}}); for connected graphs this time bound can be simplified toΘ(|E|log\\u2061|V|){\\\\displaystyle \\\\Theta (|E|\\\\log |V|)}.  The Fibonacci heap improves this toΘ(|E|+|V|log\\u2061|V|).{\\\\displaystyle \\\\Theta (|E|+|V|\\\\log |V|).}When using binary heaps, the average case time complexity is lower than the worst-case: assuming edge costs are drawn independently from a common probability distribution, the expected number of decrease-key operations is bounded byΘ(|V|log\\u2061(|E|/|V|)){\\\\displaystyle \\\\Theta (|V|\\\\log(|E|/|V|))}, giving a total running time of:\\u200a199–200O(|E|+|V|log\\u2061|E||V|log\\u2061|V|).{\\\\displaystyle O\\\\left(|E|+|V|\\\\log {\\\\frac {|E|}{|V|}}\\\\log |V|\\\\right).}Practical optimizations and infinite graphsIn common presentations of Dijkstra\\'s algorithm, initially all nodes are entered into the priority queue. This is, however, not necessary: the algorithm can start with a priority queue that contains only one item, and insert new items as they are discovered (instead of doing a decrease-key, check whether the key is in the queue; if it is, decrease its key, otherwise insert it).:\\u200a198\\u200a This variant has the same worst-case bounds as the common variant, but maintains a smaller priority queue in practice, speeding up the queue operations.Moreover, not inserting all nodes in a graph makes it possible to extend the algorithm to find the shortest path from a single source to the closest of a set of target nodes on infinite graphs or those too large to represent in memory. The resulting algorithm is called uniform-cost search (UCS) in the artificial intelligence literature and can be expressed in pseudocode asprocedure uniform_cost_search(start) isnode ← startfrontier ← priority queue containing node onlyexplored ← empty setdoif frontier is empty thenreturn failurenode ← frontier.pop()if node is a goal state thenreturn solution(node)explored.add(node)for each of node\\'s neighbors n doif n is not in explored and not in frontier thenfrontier.add(n)else if n is in frontier with higher costreplace existing node with nThe complexity of this algorithm can be expressed in an alternative way for very large graphs: when C* is the length of the shortest path from the start node to any node satisfying the \"goal\" predicate, each edge has cost at least ε, and the number of neighbors per node is bounded by b, then the algorithm\\'s worst-case time and space complexity are both in O(b1+⌊C* ⁄ ε⌋).Further optimizations of Dijkstra\\'s algorithm for the single-target case include bidirectional variants, goal-directed variants such as the A* algorithm (see § Related problems and algorithms), graph pruning to determine which nodes are likely to form the middle segment of shortest paths (reach-based routing), and hierarchical decompositions of the input graph that reduce s–t routing to connecting s and t to their respective \"transit nodes\" followed by shortest-path computation between these transit nodes using a \"highway\".Combinations of such techniques may be needed for optimal practical performance on specific problems.Specialized variantsWhen arc weights are small integers (bounded by a parameterC{\\\\displaystyle C}), specialized queues which take advantage of this fact can be used to speed up Dijkstra\\'s algorithm. The first algorithm of this type was Dial\\'s algorithm (Dial 1969) for graphs with positive integer edge weights, which uses a bucket queue to obtain a running timeO(|E|+|V|C){\\\\displaystyle O(|E|+|V|C)}. The use of a Van Emde Boas tree as the priority queue brings the complexity toO(|E|log\\u2061log\\u2061C){\\\\displaystyle O(|E|\\\\log \\\\log C)}(Ahuja et al. 1990). Another interesting variant based on a combination of a new radix heap and the well-known Fibonacci heap runs in timeO(|E|+|V|log\\u2061C){\\\\displaystyle O(|E|+|V|{\\\\sqrt {\\\\log C}})}(Ahuja et al. 1990). Finally, the best algorithms in this special case are as follows. The algorithm given by (Thorup 2000) runs inO(|E|log\\u2061log\\u2061|V|){\\\\displaystyle O(|E|\\\\log \\\\log |V|)}time and the algorithm given by (Raman 1997) runs inO(|E|+|V|min{(log\\u2061|V|)1/3+ε,(log\\u2061C)1/4+ε}){\\\\displaystyle O(|E|+|V|\\\\min\\\\{(\\\\log |V|)^{1/3+\\\\varepsilon },(\\\\log C)^{1/4+\\\\varepsilon }\\\\})}time.Related problems and algorithmsThe functionality of Dijkstra\\'s original algorithm can be extended with a variety of modifications. For example, sometimes it is desirable to present solutions which are less than mathematically optimal. To obtain a ranked list of less-than-optimal solutions, the optimal solution is first calculated. A single edge appearing in the optimal solution is removed from the graph, and the optimum solution to this new graph is calculated. Each edge of the original solution is suppressed in turn and a new shortest-path calculated. The secondary solutions are then ranked and presented after the first optimal solution.Dijkstra\\'s algorithm is usually the working principle behind link-state routing protocols, OSPF and IS-IS being the most common ones.Unlike Dijkstra\\'s algorithm, the Bellman–Ford algorithm can be used on graphs with negative edge weights, as long as the graph contains no negative cycle reachable from the source vertex s. The presence of such cycles means there is no shortest path, since the total weight becomes lower each time the cycle is traversed. (This statement assumes that a \"path\" is allowed to repeat vertices. In graph theory that is normally not allowed.  In theoretical computer science it often is allowed.) It is possible to adapt Dijkstra\\'s algorithm to handle negative weight edges by combining it with the Bellman-Ford algorithm (to remove negative edges and detect negative cycles), such an algorithm is called Johnson\\'s algorithm.The A* algorithm is a generalization of Dijkstra\\'s algorithm that cuts down on the size of the subgraph that must be explored, if additional information is available that provides a lower bound on the \"distance\" to the target. This approach can be viewed from the perspective of linear programming: there is a natural linear program for computing shortest paths, and solutions to its dual linear program are feasible if and only if they form a consistent heuristic (speaking roughly, since the sign conventions differ from place to place in the literature). This feasible dual / consistent heuristic defines a non-negative reduced cost and A* is essentially running Dijkstra\\'s algorithm with these reduced costs. If the dual satisfies the weaker condition of admissibility, then A* is instead more akin to the Bellman–Ford algorithm.The process that underlies Dijkstra\\'s algorithm is similar to the greedy process used in Prim\\'s algorithm.  Prim\\'s purpose is to find a minimum spanning tree that connects all nodes in the graph; Dijkstra is concerned with only two nodes. Prim\\'s does not evaluate the total weight of the path from the starting node, only the individual edges.Breadth-first search can be viewed as a special-case of Dijkstra\\'s algorithm on unweighted graphs, where the priority queue degenerates into a FIFO queue.The fast marching method can be viewed as a continuous version of Dijkstra\\'s algorithm which computes the geodesic distance on a triangle mesh.Dynamic programming perspectiveFrom a dynamic programming point of view, Dijkstra\\'s algorithm is a successive approximation scheme that solves the dynamic programming functional equation for the shortest path problem by the Reaching method.In fact, Dijkstra\\'s explanation of the logic behind the algorithm, namelyProblem 2. Find the path of minimum total length between two given nodesP{\\\\displaystyle P}andQ{\\\\displaystyle Q}.We use the fact that, ifR{\\\\displaystyle R}is a node on the minimal path fromP{\\\\displaystyle P}toQ{\\\\displaystyle Q}, knowledge of the latter implies the knowledge of the minimal path fromP{\\\\displaystyle P}toR{\\\\displaystyle R}.is a paraphrasing of Bellman\\'s famous Principle of Optimality in the context of the shortest path problem.ApplicationsLeast-cost paths are calculated for instance to establish tracks of electricity lines or oil pipelines. The algorithm has also been used to calculate optimal long-distance footpaths in Ethiopia and contrast them with the situation on the ground.See alsoA* search algorithmBellman–Ford algorithmEuclidean shortest pathFloyd–Warshall algorithmJohnson\\'s algorithmLongest path problemParallel all-pairs shortest path algorithmNotesReferencesCormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L.; Stein, Clifford (2001). \"Section 24.3: Dijkstra\\'s algorithm\". Introduction to Algorithms (Second ed.). MIT Press and McGraw–Hill. pp. 595–601. ISBN 0-262-03293-7.Dial, Robert B. (1969). \"Algorithm 360: Shortest-path forest with topological ordering [H]\". Communications of the ACM. 12 (11): 632–633. doi:10.1145/363269.363610. S2CID 6754003.Fredman, Michael Lawrence; Tarjan, Robert E. (1984). Fibonacci heaps and their uses in improved network optimization algorithms. 25th Annual Symposium on Foundations of Computer Science. IEEE. pp. 338–346. doi:10.1109/SFCS.1984.715934.Fredman, Michael Lawrence; Tarjan, Robert E. (1987). \"Fibonacci heaps and their uses in improved network optimization algorithms\". Journal of the Association for Computing Machinery. 34 (3): 596–615. doi:10.1145/28869.28874. S2CID 7904683.Zhan, F. Benjamin; Noon, Charles E. (February 1998). \"Shortest Path Algorithms: An Evaluation Using Real Road Networks\". Transportation Science. 32 (1): 65–73. doi:10.1287/trsc.32.1.65. S2CID 14986297.Leyzorek, M.; Gray, R. S.; Johnson, A. A.; Ladew, W. C.; Meaker, Jr., S. R.; Petry, R. M.; Seitz, R. N. (1957). Investigation of Model Techniques – First Annual Report – 6 June 1956 – 1 July 1957 – A Study of Model Techniques for Communication Systems. Cleveland, Ohio: Case Institute of Technology.Knuth, D.E. (1977). \"A Generalization of Dijkstra\\'s Algorithm\". Information Processing Letters. 6 (1): 1–5. doi:10.1016/0020-0190(77)90002-3.Ahuja, Ravindra K.; Mehlhorn, Kurt; Orlin, James B.; Tarjan, Robert E. (April 1990). \"Faster Algorithms for the Shortest Path Problem\" (PDF). Journal of the ACM. 37 (2): 213–223. doi:10.1145/77600.77615. hdl:1721.1/47994. S2CID 5499589.Raman, Rajeev (1997). \"Recent results on the single-source shortest paths problem\". SIGACT News. 28 (2): 81–87. doi:10.1145/261342.261352. S2CID 18031586.Thorup, Mikkel (2000). \"On RAM priority Queues\". SIAM Journal on Computing. 30 (1): 86–109. doi:10.1137/S0097539795288246. S2CID 5221089.Thorup, Mikkel (1999). \"Undirected single-source shortest paths with positive integer weights in linear time\". Journal of the ACM. 46 (3): 362–394. doi:10.1145/316542.316548. S2CID 207654795.External linksOral history interview with Edsger W. Dijkstra, Charles Babbage Institute, University of Minnesota, MinneapolisImplementation of Dijkstra\\'s algorithm using TDD, Robert Cecil Martin, The Clean Code Blog',\n",
              " \"= Johnson's algorithm =Johnson's algorithm is a way to find the shortest paths between all pairs of vertices in an edge-weighted directed graph. It allows some of the edge weights to be negative numbers, but no negative-weight cycles may exist. It works by using the Bellman–Ford algorithm to compute a transformation of the input graph that removes all negative weights, allowing Dijkstra's algorithm to be used on the transformed graph. It is named after Donald B. Johnson, who first published the technique in 1977.A similar reweighting technique is also used in Suurballe's algorithm for finding two disjoint paths of minimum total length between the same two vertices in a graph with non-negative edge weights.Algorithm descriptionJohnson's algorithm consists of the following steps:First, a new node q is added to the graph, connected by zero-weight edges to each of the other nodes.Second, the Bellman–Ford algorithm is used, starting from the new vertex q, to find for each vertex v the minimum weight h(v) of a path from q to v. If this step detects a negative cycle, the algorithm is terminated.Next the edges of the original graph are reweighted using the values computed by the Bellman–Ford algorithm: an edge from u to v, having lengthw(u,v){\\\\displaystyle w(u,v)}, is given the new length w(u,v) + h(u) − h(v).Finally, q is removed, and Dijkstra's algorithm is used to find the shortest paths from each node s to every other vertex in the reweighted graph. The distance in the original graph is then computed for each distance D(u , v), by adding h(v) − h(u) to the distance returned by Dijkstra's algorithm.ExampleThe first three stages of Johnson's algorithm are depicted in the illustration below.The graph on the left of the illustration has two negative edges, but no negative cycles. The center graph shows the new vertex q, a shortest path tree as computed by the Bellman–Ford algorithm with q as starting vertex, and the values h(v) computed at each other node as the length of the shortest path from q to that node. Note that these values are all non-positive, because q has a length-zero edge to each vertex and the shortest path can be no longer than that edge. On the right is shown the reweighted graph, formed by replacing each edge weightw(u,v){\\\\displaystyle w(u,v)}by w(u,v) + h(u) − h(v). In this reweighted graph, all edge weights are non-negative, but the shortest path between any two nodes uses the same sequence of edges as the shortest path between the same two nodes in the original graph. The algorithm concludes by applying Dijkstra's algorithm to each of the four starting nodes in the reweighted graph.CorrectnessIn the reweighted graph, all paths between a pair s and t of nodes have the same quantity h(s) − h(t) added to them. The previous statement can be proven as follows: Let p be ans−t{\\\\displaystyle s-t}path. Its weight W in the reweighted graph is given by the following expression:(w(s,p1)+h(s)−h(p1))+(w(p1,p2)+h(p1)−h(p2))+...+(w(pn,t)+h(pn)−h(t)).{\\\\displaystyle {\\\\bigl (}w(s,p_{1})+h(s)-h(p_{1}){\\\\bigr )}+{\\\\bigl (}w(p_{1},p_{2})+h(p_{1})-h(p_{2}){\\\\bigr )}+...+{\\\\bigl (}w(p_{n},t)+h(p_{n})-h(t){\\\\bigr )}.}Every+h(pi){\\\\displaystyle +h(p_{i})}is cancelled by−h(pi){\\\\displaystyle -h(p_{i})}in the previous bracketed expression; therefore, we are left with the following expression for W:(w(s,p1)+w(p1,p2)+...+w(pn,t))+h(s)−h(t){\\\\displaystyle {\\\\bigl (}w(s,p_{1})+w(p_{1},p_{2})+...+w(p_{n},t){\\\\bigr )}+h(s)-h(t)}The bracketed expression is the weight of p in the original weighting.Since the reweighting adds the same amount to the weight of everys−t{\\\\displaystyle s-t}path, a path is a shortest path in the original weighting if and only if it is a shortest path after reweighting. The weight of edges that belong to a shortest path from q to any node is zero, and therefore the lengths of the shortest paths from q to every node become zero in the reweighted graph; however, they still remain shortest paths. Therefore, there can be no negative edges: if edge uv had a negative weight after the reweighting, then the zero-length path from q to u together with this edge would form a negative-length path from q to v, contradicting the fact that all vertices have zero distance from q. The non-existence of negative edges ensures the optimality of the paths found by Dijkstra's algorithm. The distances in the original graph may be calculated from the distances calculated by Dijkstra's algorithm in the reweighted graph by reversing the reweighting transformation.AnalysisThe time complexity of this algorithm, using Fibonacci heaps in the implementation of Dijkstra's algorithm, isO(|V|2log\\u2061|V|+|V||E|){\\\\displaystyle O(|V|^{2}\\\\log |V|+|V||E|)}: the algorithm usesO(|V||E|){\\\\displaystyle O(|V||E|)}time for the Bellman–Ford stage of the algorithm, andO(|V|log\\u2061|V|+|E|){\\\\displaystyle O(|V|\\\\log |V|+|E|)}for each of the|V|{\\\\displaystyle |V|}instantiations of Dijkstra's algorithm. Thus, when the graph is sparse, the total time can be faster than the Floyd–Warshall algorithm, which solves the same problem in timeO(|V|3){\\\\displaystyle O(|V|^{3})}.ReferencesExternal linksBoost: All Pairs Shortest Paths\",\n",
              " \"= Dijkstra–Scholten algorithm =The Dijkstra–Scholten algorithm (named after Edsger W. Dijkstra and Carel S. Scholten) is an algorithm for detecting termination in a distributed system. The algorithm was proposed by Dijkstra and Scholten in 1980.First, consider the case of a simple process graph which is a tree. A distributed computation which is tree-structured is not uncommon. Such a process graph may arise when the computation is strictly a divide-and-conquer type. A node starts the computation and divides the problem in two (or more, usually a multiple of 2) roughly equal parts and distribute those parts to other processors. This process continues recursively until the problems are of sufficiently small size to solve in a single processor.AlgorithmThe Dijkstra–Scholten algorithm is a tree-based algorithm which can be described by the following:The initiator of a computation is the root of the tree.Upon receiving a computational message:If the receiving process is currently not in the computation: the process joins the tree by becoming a child of the sender of the message. (No acknowledgment message is sent at this point.)If the receiving process is already in the computation: the process immediately sends an acknowledgment message to the sender of the message.When a process has no more children and has become idle, the process detaches itself from the tree by sending an acknowledgment message to its tree parent.Termination occurs when the initiator has no children and has become idle.Dijkstra–Scholten algorithm for a treeFor a tree, it is easy to detect termination. When a leaf process determines that it has terminated, it sends a signal to its parent. In general, a process waits for all its children to send signals and then it sends a signal to its parent.The program terminates when the root receives signals from all its children.Dijkstra–Scholten algorithm for directed acyclic graphsThe algorithm for a tree can be extended to acyclic directed graphs. We add an additional integer attribute Deficit to each edge.On an incoming edge, Deficit will denote the difference between the number of messages received and the number of signals sent in reply.When a node wishes to terminate, it waits until it has received signals from outgoing edges reducing their deficits to zero.Then it sends enough signals to ensure that the deficit is zero on each incoming edge.Since the graph is acyclic, some nodes will have no outgoing edges and these nodes will be the first to terminate after sending enough signals to their incoming edges. After that the nodes at higher levels will terminate level by level.Dijkstra–Scholten algorithm for cyclic directed graphsIf cycles are allowed, the previous algorithm does not work. This is because, there may not be any node with zero outgoing edges. So, potentially there is no node which can terminate without consulting other nodes.The Dijkstra–Scholten algorithm solves this problem by implicitly creating a spanning tree of the graph. A spanning-tree is a tree which includes each node of the underlying graph once and the edge-set is a subset of the original set of edges.The tree will be directed (i.e., the channels will be directed) with the source node (which initiates the computation) as the root.The spanning-tree is created in the following way. A variable First_Edge is added to each node. When a node receives a message for the first time, it initializes First_Edge with the edge through which it received the message. First_Edge is never changed afterwards. Note that, the spanning tree is not unique and it depends on the order of messages in the system.Termination is handled by each node in three steps :Send signals on all incoming edges except the first edge. (Each node will send signals which reduces the deficit on each incoming edge to zero.)Wait for signals from all outgoing edges. (The number of signals received on each outgoing edge should reduce each of their deficits to zero.)Send signals on First_Edge. (Once steps 1 and 2 are complete, a node informs its parent in the spanning tree about its intention of terminating.)See alsoHuang's algorithm== References ==\",\n",
              " '= Suurballe\\'s algorithm =In theoretical computer science and network routing, Suurballe\\'s algorithm is an algorithm for finding two disjoint paths in a nonnegatively-weighted directed graph, so that both paths connect the same pair of vertices and have minimum total length. The algorithm was conceived by John W. Suurballe and published in 1974. The main idea of Suurballe\\'s algorithm is to use Dijkstra\\'s algorithm to find one path, to modify the weights of the graph edges, and then to run Dijkstra\\'s algorithm a second time. The output of the algorithm is formed by combining these two paths, discarding edges that are traversed in opposite directions by the paths, and using the remaining edges to form the two paths to return as the output.The modification to the weights is similar to the weight modification in Johnson\\'s algorithm, and preserves the non-negativity of the weights while allowing the second instance of Dijkstra\\'s algorithm to find the correct second path.The problem of finding two disjoint paths of minimum weight can be seen as a special case of a minimum cost flow problem, where in this case there are two units of \"flow\" and nodes have unit \"capacity\". Suurballe\\'s algorithm, also, can be seen as a special case of a minimum cost flow algorithm that repeatedly pushes the maximum possible amount of flow along a shortest augmenting path.The first path found by Suurballe\\'s algorithm is the shortest augmenting path for the initial (zero) flow, and the second path found by Suurballe\\'s algorithm is the shortest augmenting path for the residual graph left after pushing one unit of flow along the first path.DefinitionsLet G be a weighted directed graph with vertex set  V and edge set  E (figure A); let  s be a designated source vertex in  G, and let  t be a designated destination vertex. Let each edge  (u,v) in  E, from vertex  u to vertex  v, have a non-negative cost  w(u,v).Define d(s,u) to be the cost of the shortest path to vertex u from vertex s in the shortest path tree rooted at s (figure C).Note: Node and Vertex are often used interchangeably.AlgorithmSuurballe\\'s algorithm performs the following steps:Find the shortest path tree T rooted at node s by running Dijkstra\\'s algorithm (figure C). This tree contains for every vertex u, a shortest path from s to u. Let P1 be the shortest cost path from s to t (figure B). The edges in T are called tree edges and the remaining edges (the edges missing from figure C) are called non-tree edges.Modify the cost of each edge in the graph by replacing the cost w(u,v) of every edge (u,v) by w′(u,v) = w(u,v) − d(s,v) + d(s,u). According to the resulting modified cost function, all tree edges have a cost of 0, and non-tree edges have a non-negative cost. For example:  If u=B, v=E, then w′(u,v) = w(B,E) − d(A,E) + d(A,B) = 2 − 3 + 1 = 0  If u=E, v=B, then w′(u,v) = w(E,B) − d(A,B) + d(A,E) = 2 − 1 + 3 = 4Create a residual graph Gt formed from G by removing the edges of G on path P1 that are directed into s and then reverse the direction of the zero length edges along path P1 (figure D).Find the shortest path P2 in the residual graph Gt by running Dijkstra\\'s algorithm (figure E).Discard the reversed edges of P2 from both paths. The remaining edges of P1 and P2 form a subgraph with two outgoing edges at s, two incoming edges at t, and one incoming and one outgoing edge at each remaining vertex. Therefore, this subgraph consists of two edge-disjoint paths from s to t and possibly some additional (zero-length) cycles. Return the two disjoint paths from the subgraph.ExampleThe following example shows how Suurballe\\'s algorithm finds the shortest pair of disjoint paths from A to F.Figure A illustrates a weighted graph G.Figure B calculates the shortest path P1 from A to F (A–B–D–F).Figure C illustrates the shortest path tree T rooted at A, and the computed distances from A to every vertex (u).Figure D shows the residual graph Gt with the updated cost of each edge and the edges of path P1 reversed.Figure E calculates path P2 in the residual graph Gt (A–C–D–B–E–F).Figure F illustrates both path P1 and path P2.Figure G finds the shortest pair of disjoint paths by combining the edges of paths P1 and P2 and then discarding the common reversed edges between both paths (B–D). As a result, we get the two shortest pair of disjoint paths (A–B–E–F) and (A–C–D–F).CorrectnessThe weight of any path from s to t in the modified system of weights equals the weight in the original graph, minus d(s,t). Therefore, the shortest two disjoint paths under the modified weights are the same paths as the shortest two paths in the original graph, although they have different weights.Suurballe\\'s algorithm may be seen as a special case of the successive shortest paths method for finding a minimum cost flow with total flow amount two from s to t. The modification to the weights does not affect the sequence of paths found by this method, only their weights. Therefore, the correctness of the algorithm follows from the correctness of the successive shortest paths method.Analysis and running timeThis algorithm requires two iterations of Dijkstra\\'s algorithm. Using Fibonacci heaps, both iterations can be performed in timeO(|E|+|V|log\\u2061|V|){\\\\displaystyle O(|E|+|V|\\\\log |V|)}where|V|{\\\\displaystyle |V|}and|E|{\\\\displaystyle |E|}are the number of vertices and edges respectively. Therefore, the same time bound applies to Suurballe\\'s algorithm.VariationsThe version of Suurballe\\'s algorithm as described above finds paths that have disjoint edges, but that may share vertices. It is possible to use the same algorithm to find vertex-disjoint paths, by replacing each vertex by a pair of adjacent vertices, one with all of the incoming adjacencies  u-in of the original vertex, and one with all of the outgoing adjacencies  u-out. Two edge-disjoint paths in this modified graph necessarily correspond to two vertex-disjoint paths in the original graph, and vice versa, so applying Suurballe\\'s algorithm to the modified graph results in the construction of two vertex-disjoint paths in the original graph. Suurballe\\'s original 1974 algorithm was for the vertex-disjoint version of the problem, and was extended in 1984 by Suurballe and Tarjan to the edge-disjoint version.By using a modified version of Dijkstra\\'s algorithm that simultaneously computes the distances to each vertex t in the graphs Gt, it is also possible to find the total lengths of the shortest pairs of paths from a given source vertex s to every other vertex in the graph, in an amount of time that is proportional to a single instance of Dijkstra\\'s algorithm.Note: The pair of adjacent vertices resulting from the split are connected by a zero cost uni-directional edge from the incoming to outgoing vertex. The source vertex becomes s-out and the destination vertex becomes t-in.See alsoEdge disjoint shortest pair algorithm== References ==',\n",
              " '= Yen\\'s algorithm =Yen\\'s algorithm computes single-source K-shortest loopless paths for a graph with non-negative edge cost.  The algorithm was published by Jin Y. Yen in 1971 and employs any shortest path algorithm to find the best path, then proceeds to find K − 1 deviations of the best path.AlgorithmTerminology and notationDescriptionThe algorithm can be broken down into two parts, determining the first k-shortest path,A1{\\\\displaystyle A^{1}}, and then determining all other k-shortest paths. It is assumed that the containerA{\\\\displaystyle A}will hold the k-shortest path, whereas the containerB{\\\\displaystyle B}, will hold the potential k-shortest paths. To determineA1{\\\\displaystyle A^{1}}, the shortest path from the source to the sink, any efficient shortest path algorithm can be used.To find theAk{\\\\displaystyle A^{k}}, wherek{\\\\displaystyle k}ranges from2{\\\\displaystyle 2}toK{\\\\displaystyle K}, the algorithm assumes that all paths fromA1{\\\\displaystyle A^{1}}toAk−1{\\\\displaystyle A^{k-1}}have previously been found. Thek{\\\\displaystyle k}iteration can be divided into two processes, finding all the deviationsAki{\\\\displaystyle {A^{k}}_{i}}and choosing a minimum length path to becomeAk{\\\\displaystyle A^{k}}. Note that in this iteration,i{\\\\displaystyle i}ranges from1{\\\\displaystyle 1}toQkk{\\\\displaystyle {Q^{k}}_{k}}.The first process can be further subdivided into three operations, choosing theRki{\\\\displaystyle {R^{k}}_{i}}, findingSki{\\\\displaystyle {S^{k}}_{i}}, and then addingAki{\\\\displaystyle {A^{k}}_{i}}to the containerB{\\\\displaystyle B}. The root path,Rki{\\\\displaystyle {R^{k}}_{i}}, is chosen by finding the subpath inAk−1{\\\\displaystyle A^{k-1}}that follows the firsti{\\\\displaystyle i}nodes ofAj{\\\\displaystyle A^{j}}, wherej{\\\\displaystyle j}ranges from1{\\\\displaystyle 1}tok−1{\\\\displaystyle k-1}. Then, if a path is found, the cost of edgedi(i+1){\\\\displaystyle d_{i(i+1)}}ofAj{\\\\displaystyle A^{j}}is set to infinity. Next, the spur path,Ski{\\\\displaystyle {S^{k}}_{i}}, is found by computing the shortest path from the spur node, nodei{\\\\displaystyle i}, to the sink. The removal of previous used edges from(i){\\\\displaystyle (i)}to(i+1){\\\\displaystyle (i+1)}ensures that the spur path is different.Aki=Rki+Ski{\\\\displaystyle {A^{k}}_{i}={R^{k}}_{i}+{S^{k}}_{i}}, the addition of the root path and the spur path, is added toB{\\\\displaystyle B}. Next, the edges that were removed, i.e. had their cost set to infinity, are restored to their initial values.The second process determines a suitable path forAk{\\\\displaystyle A^{k}}by finding the path in containerB{\\\\displaystyle B}with the lowest cost. This path is removed from containerB{\\\\displaystyle B}and inserted into containerA{\\\\displaystyle A}and the algorithm continues to the next iteration.PseudocodeThe algorithm assumes that the Dijkstra algorithm is used to find the shortest path between two nodes, but any shortest path algorithm can be used in its place.function YenKSP(Graph, source, sink, K):// Determine the shortest path from the source to the sink.A[0] = Dijkstra(Graph, source, sink);// Initialize the set to store the potential kth shortest path.B = [];for k from 1 to K:// The spur node ranges from the first node to the next to last node in the previous k-shortest path.for i from 0 to size(A[k − 1]) − 2:// Spur node is retrieved from the previous k-shortest path, k − 1.spurNode = A[k-1].node(i);// The sequence of nodes from the source to the spur node of the previous k-shortest path.rootPath = A[k-1].nodes(0, i);for each path p in A:if rootPath == p.nodes(0, i):// Remove the links that are part of the previous shortest paths which share the same root path.remove p.edge(i,i + 1) from Graph;for each node rootPathNode in rootPath except spurNode:remove rootPathNode from Graph;// Calculate the spur path from the spur node to the sink.// Consider also checking if any spurPath foundspurPath = Dijkstra(Graph, spurNode, sink);// Entire path is made up of the root path and spur path.totalPath = rootPath + spurPath;// Add the potential k-shortest path to the heap.if (totalPath not in B):B.append(totalPath);// Add back the edges and nodes that were removed from the graph.restore edges to Graph;restore nodes in rootPath to Graph;if B is empty:// This handles the case of there being no spur paths, or no spur paths left.// This could happen if the spur paths have already been exhausted (added to A),// or there are no spur paths at all - such as when both the source and sink vertices// lie along a \"dead end\".break;// Sort the potential k-shortest paths by cost.B.sort();// Add the lowest cost path becomes the k-shortest path.A[k] = B[0];// In fact we should rather use shift since we are removing the first elementB.pop();return A;ExampleThe example uses Yen\\'s K-Shortest Path Algorithm to compute three paths from(C){\\\\displaystyle (C)}to(H){\\\\displaystyle (H)}. Dijkstra\\'s algorithm is used to calculate the best path from(C){\\\\displaystyle (C)}to(H){\\\\displaystyle (H)}, which is(C)−(E)−(F)−(H){\\\\displaystyle (C)-(E)-(F)-(H)}with cost 5. This path is appended to containerA{\\\\displaystyle A}and becomes the first k-shortest path,A1{\\\\displaystyle A^{1}}.Node(C){\\\\displaystyle (C)}ofA1{\\\\displaystyle A^{1}}becomes the spur node with a root path of itself,R21=(C){\\\\displaystyle {R^{2}}_{1}=(C)}. The edge,(C)−(E){\\\\displaystyle (C)-(E)}, is removed because it coincides with the root path and a path in containerA{\\\\displaystyle A}. Dijkstra\\'s algorithm is used to compute the spur pathS21{\\\\displaystyle {S^{2}}_{1}}, which is(C)−(D)−(F)−(H){\\\\displaystyle (C)-(D)-(F)-(H)}, with a cost of 8.A21=R21+S21=(C)−(D)−(F)−(H){\\\\displaystyle {A^{2}}_{1}={R^{2}}_{1}+{S^{2}}_{1}=(C)-(D)-(F)-(H)}is added to containerB{\\\\displaystyle B}as a potential k-shortest path.Node(E){\\\\displaystyle (E)}ofA1{\\\\displaystyle A^{1}}becomes the spur node withR22=(C)−(E){\\\\displaystyle {R^{2}}_{2}=(C)-(E)}. The edge,(E)−(F){\\\\displaystyle (E)-(F)}, is removed because it coincides with the root path and a path in containerA{\\\\displaystyle A}. Dijkstra\\'s algorithm is used to compute the spur pathS22{\\\\displaystyle {S^{2}}_{2}}, which is(E)−(G)−(H){\\\\displaystyle (E)-(G)-(H)}, with a cost of 7.A22=R22+S22=(C)−(E)−(G)−(H){\\\\displaystyle {A^{2}}_{2}={R^{2}}_{2}+{S^{2}}_{2}=(C)-(E)-(G)-(H)}is added to containerB{\\\\displaystyle B}as a potential k-shortest path.Node(F){\\\\displaystyle (F)}ofA1{\\\\displaystyle A^{1}}becomes the spur node with a root path,R23=(C)−(E)−(F){\\\\displaystyle {R^{2}}_{3}=(C)-(E)-(F)}. The edge,(F)−(H){\\\\displaystyle (F)-(H)}, is removed because it coincides with the root path and a path in containerA{\\\\displaystyle A}. Dijkstra\\'s algorithm is used to compute the spur pathS23{\\\\displaystyle {S^{2}}_{3}}, which is(F)−(G)−(H){\\\\displaystyle (F)-(G)-(H)}, with a cost of 8.A23=R23+S23=(C)−(E)−(F)−(G)−(H){\\\\displaystyle {A^{2}}_{3}={R^{2}}_{3}+{S^{2}}_{3}=(C)-(E)-(F)-(G)-(H)}is added to containerB{\\\\displaystyle B}as a potential k-shortest path.Of the three paths in container B,A22{\\\\displaystyle {A^{2}}_{2}}is chosen to becomeA2{\\\\displaystyle A^{2}}because it has the lowest cost of 7. This process is continued to the 3rd k-shortest path. However, within this 3rd iteration, note that some spur paths do not exist. And the path that is chosen to becomeA3{\\\\displaystyle A^{3}}is(C)−(D)−(F)−(H){\\\\displaystyle (C)-(D)-(F)-(H)}.FeaturesSpace complexityTo store the edges of the graph, the shortest path listA{\\\\displaystyle A}, and the potential shortest path listB{\\\\displaystyle B},N2+KN{\\\\displaystyle N^{2}+KN}memory addresses are required. At worse case, the every node in the graph has an edge to every other node in the graph, thusN2{\\\\displaystyle N^{2}}addresses are needed. OnlyKN{\\\\displaystyle KN}addresses are need for both listA{\\\\displaystyle A}andB{\\\\displaystyle B}because at most onlyK{\\\\displaystyle K}paths will be stored, where it is possible for each path to haveN{\\\\displaystyle N}nodes.Time complexityThe time complexity of Yen\\'s algorithm is dependent on the shortest path algorithm used in the computation of the spur paths, so the Dijkstra algorithm is assumed. Dijkstra\\'s algorithm has a worse case time complexity ofO(N2){\\\\displaystyle O(N^{2})}, but using a Fibonacci heap it becomesO(M+Nlog\\u2061N){\\\\displaystyle O(M+N\\\\log N)}, whereM{\\\\displaystyle M}is the amount of edges in the graph. Since Yen\\'s algorithm makesKl{\\\\displaystyle Kl}calls to the Dijkstra in computing the spur paths, wherel{\\\\displaystyle l}is the length of spur paths. In a condensed graph, the expected value ofl{\\\\displaystyle l}isO(log\\u2061N){\\\\displaystyle O(\\\\log N)}, while the worst case isN{\\\\displaystyle N}., the time complexity becomesO(KN(M+Nlog\\u2061N)){\\\\displaystyle O(KN(M+N\\\\log N))}.ImprovementsYen\\'s algorithm can be improved by using a heap to storeB{\\\\displaystyle B}, the set of potential k-shortest paths. Using a heap instead of a list will improve the performance of the algorithm, but not the complexity.  One method to slightly decrease complexity is to skip the nodes where there are non-existent spur paths. This case is produced when all the spur paths from a spur node have been used in the previousAk{\\\\displaystyle A^{k}}. Also, if containerB{\\\\displaystyle B}hasK−k{\\\\displaystyle K-k}paths of minimum length, in reference to those in containerA{\\\\displaystyle A}, then they can be extract and inserted into containerA{\\\\displaystyle A}since no shorter paths will be found.Lawler\\'s modificationEugene Lawler proposed a modification to Yen\\'s algorithm in which duplicates path are not calculated as opposed to the original algorithm where they are calculated and then discarded when they are found to be duplicates. These duplicates paths result from calculating spur paths of nodes in the root ofAk{\\\\displaystyle A^{k}}. For instance,Ak{\\\\displaystyle A^{k}}deviates fromAk−1{\\\\displaystyle A^{k-1}}at some node(i){\\\\displaystyle (i)}. Any spur path,Skj{\\\\displaystyle {S^{k}}_{j}}wherej=0,…,i{\\\\displaystyle j=0,\\\\ldots ,i}, that is calculated will be a duplicate because they have already been calculated during thek−1{\\\\displaystyle k-1}iteration. Therefore, only spur paths for nodes that were on the spur path ofAk−1{\\\\displaystyle A^{k-1}}must be calculated, i.e. onlySkh{\\\\displaystyle {S^{k}}_{h}}whereh{\\\\displaystyle h}ranges from(i+1)k−1{\\\\displaystyle (i+1)^{k-1}}to(Qk)k−1{\\\\displaystyle (Q_{k})^{k-1}}. To perform this operation forAk{\\\\displaystyle A^{k}}, a record is needed to identify the node whereAk−1{\\\\displaystyle A^{k-1}}branched fromAk−2{\\\\displaystyle A^{k-2}}.See alsoYen\\'s improvement to the Bellman–Ford algorithmReferencesExternal linksOpen Source Python Implementation on GitHubOpen Source C++ ImplementationOpen Source C++ Implementation using Boost Graph Library',\n",
              " '= Path-based strong component algorithm =In graph theory, the strongly connected components of a directed graph may be found using an algorithm that uses depth-first search in combination with two stacks, one to keep track of the vertices in the current component and the second to keep track of the current search path. Versions of this algorithm have been proposed by Purdom (1970), Munro (1971), Dijkstra (1976), Cheriyan & Mehlhorn (1996), and Gabow (2000); of these, Dijkstra\\'s version was the first to achieve linear time.DescriptionThe algorithm performs a depth-first search of the given graph G, maintaining as it does two stacks S and P (in addition to the normal call stack for a recursive function).Stack S contains all the vertices that have not yet been assigned to a strongly connected component, in the order in which the depth-first search reaches the vertices.Stack P contains vertices that have not yet been determined to belong to different strongly connected components from each other. It also uses a counter C of the number of vertices reached so far, which it uses to compute the preorder numbers of the vertices.When the depth-first search reaches a vertex v, the algorithm performs the following steps:Set the preorder number of v to C, and increment C.Push v onto S and also onto P.For each edge from v to a neighboring vertex w:If the preorder number of w has not yet been assigned (the edge is a tree edge), recursively search w;Otherwise, if w has not yet been assigned to a strongly connected component (the edge is a forward/back/cross edge):Repeatedly pop vertices from P until the top element of P has a preorder number less than or equal to the preorder number of w.If v is the top element of P:Pop vertices from S until v has been popped, and assign the popped vertices to a new component.Pop v from P.The overall algorithm consists of a loop through the vertices of the graph, calling this recursive search on each vertex that does not yet have a preorder number assigned to it.Related algorithmsLike this algorithm, Tarjan\\'s strongly connected components algorithm also uses depth first search together with a stack to keep track of vertices that have not yet been assigned to a component, and moves these vertices into a new component when it finishes expanding the final vertex of its component. However, in place of the stack P, Tarjan\\'s algorithm uses  a vertex-indexed array of preorder numbers, assigned in the order that vertices are first visited in the depth-first search. The preorder array is used to keep track of when to form a new component.NotesReferencesCheriyan, J.; Mehlhorn, K. (1996), \"Algorithms for dense graphs and networks on the random access computer\", Algorithmica, 15 (6): 521–549, doi:10.1007/BF01940880, S2CID 8930091.Dijkstra, Edsger (1976), A Discipline of Programming, NJ: Prentice Hall, Ch. 25.Gabow, Harold N. (2000), \"Path-based depth-first search for strong and biconnected components\", Information Processing Letters, 74 (3–4): 107–114, doi:10.1016/S0020-0190(00)00051-X, MR 1761551.Munro, Ian (1971), \"Efficient determination of the transitive closure of a directed graph\", Information Processing Letters, 1 (2): 56–58, doi:10.1016/0020-0190(71)90006-8.Purdom, P., Jr. (1970), \"A transitive closure algorithm\", BIT, 10: 76–94, doi:10.1007/bf01940892, S2CID 20818200.Sedgewick, R. (2004), \"19.8 Strong Components in Digraphs\", Algorithms in Java, Part 5 – Graph Algorithms (3rd ed.), Cambridge MA: Addison-Wesley, pp. 205–216.',\n",
              " '= A* search algorithm =A* (pronounced \"A-star\") is a graph traversal and path search algorithm, which is often used in many fields of computer science due to its completeness, optimality, and optimal efficiency. One major practical drawback is itsO(bd){\\\\displaystyle O(b^{d})}space complexity, as it stores all generated nodes in memory. Thus, in practical travel-routing systems, it is generally outperformed by algorithms which can pre-process the graph to attain better performance, as well as memory-bounded approaches; however, A* is still the best solution in many cases.Peter Hart, Nils Nilsson and Bertram Raphael of Stanford Research Institute (now SRI International) first published the algorithm in 1968. It can be seen as an extension of Dijkstra\\'s algorithm. A* achieves better performance by using heuristics to guide its search.HistoryA* was created as part of the Shakey project, which had the aim of building a mobile robot that could plan its own actions. Nils Nilsson originally proposed using the Graph Traverser algorithm for Shakey\\'s path planning. Graph Traverser is guided by a heuristic function h(n), the estimated distance from node n to the goal node: it entirely ignores g(n), the distance from the start node to n. Bertram Raphael suggested using the sum, g(n) + h(n). Peter Hart invented the concepts we now call admissibility and consistency of heuristic functions. A* was originally designed for finding least-cost paths when the cost of a path is the sum of its  costs, but it has been shown that A* can be used to find optimal paths for any problem satisfying the conditions of a cost algebra.The original 1968 A* paper contained a theorem stating that no A*-like algorithm could expand fewer nodes than A* if the heuristic function is consistent and A*\\'s tie-breaking rule is suitably chosen. A ″correction″ was published a few years later claiming that consistency was not required, but this was shown to be false in Dechter and Pearl\\'s definitive study of A*\\'s optimality (now called optimal efficiency), which gave an example of A* with a heuristic that was admissible but not consistent expanding arbitrarily more nodes than an alternative A*-like algorithm.DescriptionA* is an informed search algorithm, or a best-first search, meaning that it is formulated in terms of weighted graphs: starting from a specific starting node of a graph, it aims to find a path to the given goal node having the smallest cost (least distance travelled, shortest time, etc.).  It does this by maintaining a tree of paths originating at the start node and extending those paths one edge at a time until its termination criterion is satisfied.At each iteration of its main loop, A* needs to determine which of its paths to extend. It does so based on the cost of the path and an estimate of the cost required to extend the path all the way to the goal. Specifically, A* selects the path that minimizesf(n)=g(n)+h(n){\\\\displaystyle f(n)=g(n)+h(n)}where n is the next node on the path, g(n) is the cost of the path from the start node to n, and h(n) is a heuristic function that estimates the cost of the cheapest path from n to the goal. A* terminates when the path it chooses to extend is a path from start to goal or if there are no paths eligible to be extended. The heuristic function is problem-specific. If the heuristic function is admissible, meaning that it never overestimates the actual cost to get to the goal, A* is guaranteed to return a least-cost path from start to goal.Typical implementations of A* use a priority queue to perform the repeated selection of minimum (estimated) cost nodes to expand. This priority queue is known as the open set or fringe. At each step of the algorithm, the node with the lowest f(x) value is removed from the queue, the f and g values of its neighbors are updated accordingly, and these neighbors are added to the queue. The algorithm continues until a removed node (thus the node with the lowest f value out of all fringe nodes) is a goal node. The f value of that goal is then also the cost of the shortest path, since h at the goal is zero in an admissible heuristic.The algorithm described so far gives us only the length of the shortest path. To find the actual sequence of steps, the algorithm can be easily revised so that each node on the path keeps track of its predecessor. After this algorithm is run, the ending node will point to its predecessor, and so on, until some node\\'s predecessor is the start node.As an example, when searching for the shortest route on a map, h(x) might represent the straight-line distance to the goal, since that is physically the smallest possible distance between any two points. For a grid map from a video game, using the Manhattan distance or the octile distance becomes better depending on the set of movements available (4-way or 8-way).If the heuristic h satisfies the additional condition h(x) ≤ d(x, y) + h(y) for every edge (x, y) of the graph (where d denotes the length of that edge), then h is called monotone, or consistent. With a consistent heuristic, A* is guaranteed to find an optimal path without processing any node more than once and A* is equivalent to running Dijkstra\\'s algorithm with the reduced cost d\\'(x, y) = d(x, y) + h(y) − h(x).PseudocodeThe following pseudocode describes the algorithm:Remark: In this pseudocode, if a node is reached by one path, removed from openSet, and subsequently reached by a cheaper path, it will be added to openSet again. This is essential to guarantee that the path returned is optimal if the heuristic function is admissible but not  consistent.   If the heuristic is consistent, when a node is removed from openSet the path to it is guaranteed to be optimal so the test ‘tentative_gScore < gScore[neighbor]’ will always fail if the node is reached again.ExampleAn example of an A* algorithm in action where nodes are cities connected with roads and h(x) is the     straight-line distance to target point:Key: green: start; blue: goal; orange: visitedThe A* algorithm also has real-world applications. In this example, edges are railroads and h(x) is the great-circle distance (the shortest possible distance on a sphere) to the target. The algorithm is searching for a path between Washington, D.C. and Los Angeles.Implementation detailsThere are a number of simple optimizations or implementation details that can significantly affect the performance of an A* implementation.  The first detail to note is that the way the priority queue handles ties can have a significant effect on performance in some situations.  If ties are broken so the queue behaves in a LIFO manner, A* will behave like depth-first search among equal cost paths (avoiding exploring more than one equally optimal solution).When a path is required at the end of the search, it is common to keep with each node a reference to that node\\'s parent.  At the end of the search these references can be used to recover the optimal path.  If these references are being kept then it can be important that the same node doesn\\'t appear in the priority queue more than once (each entry corresponding to a different path to the node, and each with a different cost).  A standard approach here is to check if a node about to be added already appears in the priority queue.  If it does, then the priority and parent pointers are changed to correspond to the lower cost path. A standard binary heap based priority queue does not directly support the operation of searching for one of its elements, but it can be augmented with a hash table that maps elements to their position in the heap, allowing this decrease-priority operation to be performed in logarithmic time. Alternatively, a Fibonacci heap can perform the same decrease-priority operations in constant amortized time.Special casesDijkstra\\'s algorithm, as another example of a uniform-cost search algorithm, can be viewed as a special case of A* whereh(x)=0{\\\\displaystyle h(x)=0}for all x. General depth-first search can be implemented using A* by considering that there is a global counter C initialized with a very large value. Every time we process a node we assign C to all of its newly discovered neighbors. After each single assignment, we decrease the counter C by one. Thus the earlier a node is discovered, the higher itsh(x){\\\\displaystyle h(x)}value. Both Dijkstra\\'s algorithm and depth-first search can be implemented more efficiently without including anh(x){\\\\displaystyle h(x)}value at each node.PropertiesTermination and CompletenessOn finite graphs with non-negative edge weights A* is guaranteed to terminate and is complete, i.e. it will always find a solution (a path from start to goal) if one exists. On infinite graphs with a finite branching factor and edge costs that are bounded away from zero (d(x,y)>ε>0{\\\\textstyle d(x,y)>\\\\varepsilon >0}for some fixedε{\\\\displaystyle \\\\varepsilon }), A* is guaranteed to terminate only if there exists a solution.AdmissibilityA search algorithm is said to be admissible if it is guaranteed to return an optimal solution. If the heuristic function used by A* is admissible, then A* is admissible. An intuitive ″proof″ of this  is as follows:When A* terminates its search, it has found a path from start to goal whose actual cost is lower than the estimated cost of any path from start to goal through any open node (the node\\'sf{\\\\displaystyle f}value). When the heuristic is admissible, those estimates are optimistic (not quite—see the next paragraph), so A* can safely ignore those nodes because they cannot possibly lead to a cheaper solution than the one it already has. In other words, A* will never overlook the possibility of a lower-cost path from start to goal and so it will continue to search until no such possibilities exist.The actual proof is a bit more involved because thef{\\\\displaystyle f}values of open nodes are not guaranteed to be optimistic even if the heuristic is admissible. This is because theg{\\\\displaystyle g}values of open nodes are not guaranteed to be optimal, so the sumg+h{\\\\displaystyle g+h}is not guaranteed to be optimistic.Optimality and ConsistencyAlgorithm A is optimally efficient with respect to a set of alternative algorithms Alts on a set of problems P if for every problem P in P and every algorithm A′ in Alts, the set of nodes expanded by A in solving P is a subset (possibly equal) of the set of nodes expanded by A′ in solving P. The definitive study of the optimal efficiency of A* is due to Rina Dechter and Judea Pearl.They considered a variety of  definitions of Alts and P  in combination with A*\\'s heuristic being merely admissible or being both consistent and admissible.  The most interesting positive result they proved is that A*, with a consistent heuristic, is optimally efficient with respect to all admissible A*-like search algorithms on all ″non-pathological″ search problems.  Roughly speaking, their notion of non-pathological problem is what we now mean by ″up to tie-breaking″.  This result does not hold if A*\\'s heuristic is admissible but not consistent. In that case, Dechter and Pearl showed there exist admissible A*-like algorithms that can expand arbitrarily fewer nodes than A* on some non-pathological problems.Optimal efficiency is about the set of nodes expanded, not the number of node expansions (the number of iterations of A*\\'s main loop).  When the heuristic being used is admissible but not consistent, it is possible for a node to be expanded by A* many times, an exponential number of times in the worst case.In such circumstances Dijkstra\\'s algorithm could outperform A* by a large margin. However, more recent research found that this pathological case only occurs in certain contrived situations where the edge weight of the search graph is exponential in the size of the graph, and that certain inconsistent (but admissible) heuristics can lead to a reduced number of node expansions in A* searches.Bounded relaxationWhile the admissibility criterion guarantees an optimal solution path, it also means that A* must examine all equally meritorious paths to find the optimal path. To compute approximate shortest paths, it is possible to speed up the search at the expense of optimality by relaxing the admissibility criterion. Oftentimes we want to bound this relaxation, so that we can guarantee that the solution path is no worse than (1 + ε) times the optimal solution path. This new guarantee is referred to as ε-admissible.There are a number of ε-admissible algorithms:Weighted A*/Static Weighting\\'s. If ha(n) is an admissible heuristic function, in the weighted version of the A* search one uses hw(n) = ε ha(n), ε > 1 as the heuristic function, and perform the A* search as usual (which eventually happens faster than using ha since fewer nodes are expanded). The path hence found by the search algorithm can have a cost of at most ε times that of the least cost path in the graph.Dynamic Weighting uses the cost functionf(n)=g(n)+(1+εw(n))h(n){\\\\displaystyle f(n)=g(n)+(1+\\\\varepsilon w(n))h(n)}, wherew(n)={1−d(n)Nd(n)≤N0otherwise{\\\\displaystyle w(n)={\\\\begin{cases}1-{\\\\frac {d(n)}{N}}&d(n)\\\\leq N\\\\\\\\0&{\\\\text{otherwise}}\\\\end{cases}}}, and whered(n){\\\\displaystyle d(n)}is the depth of the search and N is the anticipated length of the solution path.Sampled Dynamic Weighting uses sampling of nodes to better estimate and debias the heuristic error.Aε∗{\\\\displaystyle A_{\\\\varepsilon }^{*}}. uses two heuristic functions. The first is the FOCAL list, which is used to select candidate nodes, and the second hF is used to select the most promising node from the FOCAL list.Aε selects nodes with the functionAf(n)+BhF(n){\\\\displaystyle Af(n)+Bh_{F}(n)}, where A and B are constants. If no nodes can be selected, the algorithm will backtrack with the functionCf(n)+DhF(n){\\\\displaystyle Cf(n)+Dh_{F}(n)}, where C and D are constants.AlphA* attempts to promote depth-first exploitation by preferring recently expanded nodes. AlphA* uses the cost functionfα(n)=(1+wα(n))f(n){\\\\displaystyle f_{\\\\alpha }(n)=(1+w_{\\\\alpha }(n))f(n)}, wherewα(n)={λg(π(n))≤g(n~)Λotherwise{\\\\displaystyle w_{\\\\alpha }(n)={\\\\begin{cases}\\\\lambda &g(\\\\pi (n))\\\\leq g({\\\\tilde {n}})\\\\\\\\\\\\Lambda &{\\\\text{otherwise}}\\\\end{cases}}}, where λ and Λ are constants withλ≤Λ{\\\\displaystyle \\\\lambda \\\\leq \\\\Lambda }, π(n) is the parent of n, and ñ is the most recently expanded node.ComplexityThe time complexity of A* depends on the heuristic. In the worst case of an unbounded search space, the number of nodes expanded is exponential in the depth of the solution (the shortest path) d: O(bd), where b is the branching factor (the average number of successors per state). This assumes that a goal state exists at all, and is reachable from the start state; if it is not, and the state space is infinite, the algorithm will not terminate.The heuristic function has a major effect on the practical performance of A* search, since a good heuristic allows A* to prune away many of the bd nodes that an uninformed search would expand. Its quality can be expressed in terms of the effective branching factor b*, which can be determined empirically for a problem instance by measuring the number of nodes generated by expansion, N, and the depth of the solution, then solvingN+1=1+b∗+(b∗)2+⋯+(b∗)d.{\\\\displaystyle N+1=1+b^{*}+(b^{*})^{2}+\\\\dots +(b^{*})^{d}.}Good heuristics are those with low effective branching factor (the optimal being b* = 1).The time complexity is polynomial when the search space is a tree, there is a single goal state, and the heuristic function h meets the following condition:|h(x)−h∗(x)|=O(log\\u2061h∗(x)){\\\\displaystyle |h(x)-h^{*}(x)|=O(\\\\log h^{*}(x))}where h* is the optimal heuristic, the exact cost to get from x to the goal. In other words, the error of h will not grow faster than the logarithm of the \"perfect heuristic\" h* that returns the true distance from x to the goal.The space complexity of A* is roughly the same as that of all other graph search algorithms, as it keeps all generated nodes in memory. In practice, this turns out to be the biggest drawback of A* search, leading to the development of memory-bounded heuristic searches, such as Iterative deepening A*, memory bounded A*, and SMA*.ApplicationsA* is often used for the common pathfinding problem in applications such as video games, but was originally designed as a general graph traversal algorithm.It finds applications in diverse problems, including the problem of parsing using stochastic grammars in NLP.Other cases include an Informational search with online learning.Relations to other algorithmsWhat sets A* apart from a greedy best-first search algorithm is that it takes the cost/distance already traveled, g(n), into account.Some common variants of Dijkstra\\'s algorithm can be viewed as a special case of A* where the heuristich(n)=0{\\\\displaystyle h(n)=0}for all nodes; in turn, both Dijkstra and A* are special cases of dynamic programming.A* itself is a special case of a generalization of branch and bound.VariantsAnytime A*Block A*D*Field D*FringeFringe Saving A* (FSA*)Generalized Adaptive A* (GAA*)Incremental heuristic searchReduced A*Iterative deepening A* (IDA*)Jump point searchLifelong Planning A* (LPA*)New Bidirectional A* (NBA*)Simplified Memory bounded A* (SMA*)Theta*A* can also be adapted to a bidirectional search algorithm. Special care needs to be taken for the stopping criterion.See alsoBreadth-first searchDepth-first searchAny-angle path planning, search for paths that are not limited to move along graph edges but rather can take on any angleNotesReferencesFurther readingNilsson, N. J. (1980). Principles of Artificial Intelligence. Palo Alto, California: Tioga Publishing Company. ISBN 978-0-935382-01-3.External linksClear visual A* explanation, with advice and thoughts on path-findingVariation on A* called Hierarchical Path-Finding A* (HPA*)Brian Grinstead. \"A* Search Algorithm in JavaScript (Updated)\". Archived from the original on 15 February 2020. Retrieved 8 February 2021.',\n",
              " '= Bellman–Ford algorithm =The Bellman–Ford algorithm is an algorithm that computes shortest paths from a single source vertex to all of the other vertices in a weighted digraph.It is slower than Dijkstra\\'s algorithm for the same problem, but more versatile, as it is capable of handling graphs in which some of the edge weights are negative numbers.The algorithm was first proposed by Alfonso Shimbel (1955), but is instead named after Richard Bellman and Lester Ford Jr., who published it in 1958 and 1956, respectively. Edward F. Moore also published a variation of the algorithm in 1959, and for this reason it is also sometimes called the Bellman–Ford–Moore algorithm.Negative edge weights are found in various applications of graphs, hence the usefulness of this algorithm.If a graph contains a \"negative cycle\" (i.e. a cycle whose edges sum to a negative value) that is reachable from the source, then there is no cheapest path: any path that has a point on the negative cycle can be made cheaper by one more walk around the negative cycle. In such a case, the Bellman–Ford algorithm can detect and report the negative cycle.AlgorithmLike Dijkstra\\'s algorithm, Bellman–Ford proceeds by relaxation, in which approximations to the correct distance are replaced by better ones until they eventually reach the solution. In both algorithms, the approximate distance to each vertex is always an overestimate of the true distance, and is replaced by the minimum of its old value and the length of a newly found path.However, Dijkstra\\'s algorithm uses a priority queue to greedily select the closest vertex that has not yet been processed, and performs this relaxation process on all of its outgoing edges; by contrast, the Bellman–Ford algorithm simply relaxes all the edges, and does this|V|−1{\\\\displaystyle |V|-1}times, where|V|{\\\\displaystyle |V|}is the number of vertices in the graph. In each of these repetitions, the number of vertices with correctly calculated distances grows, from which it follows that eventually all vertices will have their correct distances. This method allows the Bellman–Ford algorithm to be applied to a wider class of inputs than Dijkstra. The intermediate answers depend on the order of edges relaxed, but the final answer remains the same.Bellman–Ford runs inO(|V|⋅|E|){\\\\displaystyle O(|V|\\\\cdot |E|)}time, where|V|{\\\\displaystyle |V|}and|E|{\\\\displaystyle |E|}are the number of vertices and edges respectively.function BellmanFord(list vertices, list edges, vertex source) is// This implementation takes in a graph, represented as// lists of vertices (represented as integers [0..n-1]) and edges,// and fills two arrays (distance and predecessor) holding// the shortest path from the source to each vertexdistance := list of size npredecessor := list of size n// Step 1: initialize graphfor each vertex v in vertices dodistance[v] := inf             // Initialize the distance to all vertices to infinitypredecessor[v] := null         // And having a null predecessordistance[source] := 0              // The distance from the source to itself is, of course, zero// Step 2: relax edges repeatedlyrepeat |V|−1 times:for each edge (u, v) with weight w in edges doif distance[u] + w < distance[v] thendistance[v] := distance[u] + wpredecessor[v] := u// Step 3: check for negative-weight cyclesfor each edge (u, v) with weight w in edges doif distance[u] + w < distance[v] thenerror \"Graph contains a negative-weight cycle\"return distance, predecessorSimply put, the algorithm initializes the distance to the source to 0 and all other nodes to infinity. Then for all edges, if the distance to the destination can be shortened by taking the edge, the distance is updated to the new lower value. At each iteration i that the edges are scanned, the algorithm finds all shortest paths of at most length i edges (and possibly some paths longer than i edges). Since the longest possible path without a cycle can be|V|−1{\\\\displaystyle |V|-1}edges, the edges must be scanned|V|−1{\\\\displaystyle |V|-1}times to ensure the shortest path has been found for all nodes. A final scan of all the edges is performed and if any distance is updated, then a path of length|V|{\\\\displaystyle |V|}edges has been found which can only occur if at least one negative cycle exists in the graph.Proof of correctnessThe correctness of the algorithm can be shown by induction:Lemma. After i repetitions of for loop,if Distance(u) is not infinity, it is equal to the length of some path from s to u; andif there is a path from s to u with at most i edges, then Distance(u) is at most the length of the shortest path from s to u with at most i edges.Proof. For the base case of induction, consider i=0 and the moment before for loop is executed for the first time. Then, for the source vertex, source.distance = 0, which is correct. For other vertices u, u.distance = infinity, which is also correct because there is no path from source to u with 0 edges.For the inductive case, we first prove the first part. Consider a moment when a vertex\\'s distance is updated byv.distance := u.distance + uv.weight. By inductive assumption, u.distance is the length of some path from source to u. Then u.distance + uv.weight is the length of the path from source to v that follows the path from  source to u and then goes to v.For the second part, consider a shortest path P (there may be more than one) from source to v with at most i edges. Let u be the last vertex before v on this path. Then, the part of the path from source to u is a shortest path from source to u with at most i-1 edges, since if it were not, then there must be some strictly shorter path from source to u with at most i-1 edges, and we could then append the edge uv to this path to obtain a path with at most i edges that is strictly shorter than P—a contradiction. By inductive assumption, u.distance after i−1 iterations is at most the length of this path from source to u. Therefore, uv.weight + u.distance is at most the length of P. In the ith iteration, v.distance gets compared with uv.weight + u.distance, and is set equal to it if uv.weight + u.distance is smaller. Therefore, after i iterations, v.distance is at most the length of P, i.e., the length of the shortest path from source to v that uses at most i edges.If there are no negative-weight cycles, then every shortest path visits each vertex at most once, so at step 3 no further improvements can be made. Conversely, suppose no improvement can be made. Then for any cycle with vertices v[0], ..., v[k−1],v[i].distance <= v[i-1 (mod k)].distance + v[i-1 (mod k)]v[i].weightSumming around the cycle, the v[i].distance and v[i−1 (mod k)].distance terms cancel, leaving0 <= sum from 1 to k of v[i-1 (mod k)]v[i].weightI.e., every cycle has nonnegative weight.Finding negative cyclesWhen the algorithm is used to find shortest paths, the existence of negative cycles is a problem, preventing the algorithm from finding a correct answer. However, since it terminates upon finding a negative cycle, the Bellman–Ford algorithm can be used for applications in which this is the target to be sought – for example in cycle-cancelling techniques in network flow analysis.Applications in routingA distributed variant of the Bellman–Ford algorithm is used in distance-vector routing protocols, for example the Routing Information Protocol (RIP). The algorithm is distributed because it involves a number of nodes (routers) within an Autonomous system (AS), a collection of IP networks typically owned by an ISP.It consists of the following steps:Each node calculates the distances between itself and all other nodes within the AS and stores this information as a table.Each node sends its table to all neighboring nodes.When a node receives distance tables from its neighbors, it calculates the shortest routes to all other nodes and updates its own table to reflect any changes.The main disadvantages of the Bellman–Ford algorithm in this setting are as follows:It does not scale well.Changes in network topology are not reflected quickly since updates are spread node-by-node.Count to infinity if link or node failures render a node unreachable from some set of other nodes, those nodes may spend forever gradually increasing their estimates of the distance to it, and in the meantime there may be routing loops.ImprovementsThe Bellman–Ford algorithm may be improved in practice (although not in the worst case) by the observation that, if an iteration of the main loop of the algorithm terminates without making any changes, the algorithm can be immediately terminated, as subsequent iterations will not make any more changes. With this early termination condition, the main loop may in some cases use many fewer than |V| − 1 iterations, even though the worst case of the algorithm remains unchanged. The following improvements all maintain theO(|V|⋅|E|){\\\\displaystyle O(|V|\\\\cdot |E|)}worst-case time complexity.A variation of the Bellman-Ford algorithm known as Shortest Path Faster Algorithm, first described by Moore (1959), reduces the number of relaxation steps that need to be performed within each iteration of the algorithm. If a vertex v has a distance value that has not changed since the last time the edges out of v were relaxed, then there is no need to relax the edges out of v a second time. In this way, as the number of vertices with correct distance values grows, the number whose outgoing edges that need to be relaxed in each iteration shrinks, leading to a constant-factor savings in time for dense graphs.Yen (1970) described another improvement to the Bellman–Ford algorithm. His improvement first assigns some arbitrary linear order on all vertices and then partitions the set of all edges into two subsets. The first subset, Ef, contains all edges (vi, vj) such that i < j; the second, Eb, contains edges (vi, vj) such that i > j. Each vertex is visited in the order v1, v2, ..., v|V|, relaxing each outgoing edge from that vertex in Ef. Each vertex is then visited in the order v|V|, v|V|−1, ..., v1, relaxing each outgoing edge from that vertex in Eb. Each iteration of the main loop of the algorithm, after the first one, adds at least two edges to the set of edges whose relaxed distances match the correct shortest path distances: one from Ef and one from Eb. This modification reduces the worst-case number of iterations of the main loop of the algorithm from |V| − 1 to|V|/2{\\\\displaystyle |V|/2}.Another improvement, by Bannister & Eppstein (2012), replaces the arbitrary linear order of the vertices used in Yen\\'s second improvement by a random permutation. This change makes the worst case for Yen\\'s improvement (in which the edges of a shortest path strictly alternate between the two subsets Ef and Eb) very unlikely to happen. With a randomly permuted vertex ordering, the expected number of iterations needed in the main loop is at most|V|/3{\\\\displaystyle |V|/3}.NotesReferencesOriginal sourcesShimbel, A. (1955). Structure in communication nets. Proceedings of the Symposium on Information Networks. New York, New York: Polytechnic Press of the Polytechnic Institute of Brooklyn. pp. 199–203.Bellman, Richard (1958). \"On a routing problem\". Quarterly of Applied Mathematics. 16: 87–90. doi:10.1090/qam/102435. MR 0102435.Ford, Lester R. Jr. (August 14, 1956). Network Flow Theory. Paper P-923. Santa Monica, California: RAND Corporation.Moore, Edward F. (1959). The shortest path through a maze. Proc. Internat. Sympos. Switching Theory 1957, Part II. Cambridge, Massachusetts: Harvard Univ. Press. pp. 285–292. MR 0114710.Yen, Jin Y. (1970). \"An algorithm for finding shortest routes from all source nodes to a given destination in general networks\". Quarterly of Applied Mathematics. 27 (4): 526–530. doi:10.1090/qam/253822. MR 0253822.Bannister, M. J.; Eppstein, D. (2012). Randomized speedup of the Bellman–Ford algorithm. Analytic Algorithmics and Combinatorics (ANALCO12), Kyoto, Japan. pp. 41–47. arXiv:1111.5414. doi:10.1137/1.9781611973020.6.Secondary sourcesBang-Jensen, Jørgen; Gutin, Gregory (2000). \"Section 2.3.4: The Bellman-Ford-Moore algorithm\". Digraphs: Theory, Algorithms and Applications (First ed.). ISBN 978-1-84800-997-4.Schrijver, Alexander (2005). \"On the history of combinatorial optimization (till 1960)\" (PDF). Handbook of Discrete Optimization. Elsevier: 1–68.Cormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L. Introduction to Algorithms. MIT Press and McGraw-Hill., Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7. Section 24.1: The Bellman–Ford algorithm, pp. 588–592. Problem 24-1, pp. 614–615. Third Edition. MIT Press, 2009. ISBN 978-0-262-53305-8. Section 24.1: The Bellman–Ford algorithm, pp. 651–655.Heineman, George T.; Pollice, Gary; Selkow, Stanley (2008). \"Chapter 6: Graph Algorithms\". Algorithms in a Nutshell. O\\'Reilly Media. pp. 160–164. ISBN 978-0-596-51624-6.Kleinberg, Jon; Tardos, Éva (2006). Algorithm Design. New York: Pearson Education, Inc.Sedgewick, Robert (2002). \"Section 21.7: Negative Edge Weights\". Algorithms in Java (3rd ed.). ISBN 0-201-36121-3. Archived from the original on 2008-05-31. Retrieved 2007-05-28.',\n",
              " '= K shortest path routing =The k shortest path routing problem is a generalization of the shortest path routing problem in a given network. It asks not only about a shortest path but also about next k−1 shortest paths (which may be longer than the shortest path). A variation of the problem is the loopless k shortest paths.Finding k shortest paths is possible by extending Dijkstra algorithm or Bellman-Ford algorithm and extend them to find more than one path.HistorySince 1957 many papers were published on the k shortest path routing problem. Most of the fundamental works were done between 1960s and 2001. Since then, most of the research has been on the problem\\'s applications and its variants. In 2010, Michael Günther et al. published a book on Symbolic calculation of k-shortest paths and related measures with the stochastic process algebra tool CASPA.AlgorithmThe Dijkstra algorithm can be generalized to find the k shortest paths.VariationsThere are two main variations of the k shortest path routing problem.  In one variation, paths are allowed to visit the same node more than once, thus creating loops.  In another variation, paths are required to be simple and loopless.  The loopy version is solvable using Eppstein\\'s algorithm and the loopless variation is solvable by Yen\\'s algorithm.Loopy variantIn this variant, the problem is simplified by not requiring paths to be loopless.  A solution was given by B. L. Fox in 1975 in which the k-shortest paths are determined in O(m + kn log n) asymptotic time complexity (using big O notation.  In 1998, David Eppstein reported an approach that maintains an asymptotic complexity of O(m + n log n + k) by computing an implicit representation of the paths, each of which can be output in O(n) extra time. In 2015, Akuba et al. devised an indexing method as a significantly faster alternative for Eppstein\\'s algorithm, in which a data structure called an index is constructed from a graph and then top-k distances between arbitrary pairs of vertices can be rapidly obtained.Loopless variantIn the loopless variant, the paths are forbidden to contain loops which adds an additional level of complexity. It can be solved using Yen\\'s algorithm to find the lengths of all shortest paths from a fixed node to all other nodes in an n-node non negative-distance network, a technique requiring only 2n2 additions and n2 comparison, fewer than other available shortest path algorithms need.  The running time complexity is pseudo-polynomial, being O(kn(m + n log n)) (where m and n represent the number of edges and vertices, respectively). In 2007, John Hershberger and Subhash Suri proposed a replacement paths algorithm, a more efficient implementation of Lawler\\'s  and Yen\\'s algorithm with O(n) improvement in time.Some examples and descriptionExample #1The following example makes use of Yen’s model to find k shortest paths between communicating end nodes. That is, it finds a shortest path, second shortest path, etc. up to the Kth shortest path. More details can be found here.The code provided in this example attempts to solve the k shortest path routing problem for a 15-nodes network containing a combination of unidirectional and bidirectional links:Example #2Another example is the use of k shortest paths algorithm to track multiple objects. The technique implements a multiple object tracker based on the k shortest paths routing algorithm. A set of probabilistic occupancy maps is used as input. An object detector provides the input.The complete details can be found at \"Computer Vision Laboratory – CVLAB\".Example #3Another use of k shortest paths algorithms is to design a transit network that enhances passengers\\' experience in public transportation systems. Such an example of a transit network can be constructed by putting traveling time under consideration. In addition to traveling time, other conditions may be taken depending upon economical and geographical limitations. Despite variations in parameters, the k shortest path algorithms finds the most optimal solutions that satisfies almost all user needs. Such applications of k shortest path algorithms are becoming common, recently Xu, He, Song, and Chaudry (2012) studied the k shortest path problems in transit network systems.ApplicationsThe k shortest path routing is a good alternative for:Geographic path planningNetwork routing, especially in optical mesh network where there are additional constraints that cannot be solved by using ordinary shortest path algorithms.Hypothesis generation in computational linguisticsSequence alignment and metabolic pathway finding in bioinformaticsMultiple object tracking as described aboveRoad Networks: road junctions are the nodes (vertices) and each  edge (link) of the graph is associated with a road segment between two junctions.Related problemsThe breadth-first search algorithm is used when the search is only limited to two operations.The Floyd–Warshall algorithm solves all pairs shortest paths.Johnson\\'s algorithm solves all pairs\\' shortest paths, and may be faster than Floyd–Warshall on sparse graphs.Perturbation theory finds (at worst) the locally shortest path.Cherkassky et al. provide more algorithms and associated evaluations.See alsoConstrained shortest path routingNotesExternal linksImplementation of Yen\\'s algorithmImplementation of Yen\\'s and fastest k shortest simple paths algorithmshttp://www.technical-recipes.com/2012/the-k-shortest-paths-algorithm-in-c/#more-2432Multiple objects tracking technique using K-shortest path algorithm: http://cvlab.epfl.ch/software/ksp/Computer Vision Laboratory: http://cvlab.epfl.ch/software/ksp/',\n",
              " \"= Widest path problem =In graph algorithms, the widest path problem is the problem of finding a path between two designated vertices in a weighted graph, maximizing the weight of the minimum-weight edge in the path. The widest path problem is also known as the maximum capacity path problem. It is possible to adapt most shortest path algorithms to compute widest paths, by modifying them to use the bottleneck distance instead of path length. However, in many cases even faster algorithms are possible.For instance, in a graph that represents connections between routers in the Internet, where the weight of an edge represents the bandwidth of a connection between two routers, the widest path problem is the problem of finding an end-to-end path between two Internet nodes that has the maximum possible bandwidth.  The smallest edge weight on this path is known as the capacity or bandwidth of the path. As well as its applications in network routing, the widest path problem is also an important component of the Schulze method for deciding the winner of a multiway election, and has been applied to digital compositing, metabolic pathway analysis, and the computation of maximum flows.A closely related problem, the minimax path problem or bottleneck shortest path problem asks for the path that minimizes the maximum weight of any of its edges. It has applications that include transportation planning. Any algorithm for the widest path problem can be transformed into an algorithm for the minimax path problem, or vice versa, by reversing the sense of all the weight comparisons performed by the algorithm, or equivalently by replacing every edge weight by its negation.Undirected graphsIn an undirected graph, a widest path may be found as the path between the two vertices in the maximum spanning tree of the graph, and a minimax path may be found as the path between the two vertices in the minimum spanning tree.In any graph, directed or undirected, there is a straightforward algorithm for finding a widest path once the weight of its minimum-weight edge is known: simply delete all smaller edges and search for any path among the remaining edges using breadth first search or depth first search. Based on this test, there also exists a linear time algorithm for finding a widest s-t path in an undirected graph, that does not use the maximum spanning tree. The main idea of the algorithm is to apply the linear-time path-finding algorithm to the median edge weight in the graph, and then either to delete all smaller edges or contract all larger edges according to whether a path does or does not exist, and recurse in the resulting smaller graph.Fernández, Garfinkel & Arbiol (1998) use undirected bottleneck shortest paths in order to form composite aerial photographs that combine multiple images of overlapping areas. In the subproblem to which the widest path problem applies, two images have already been transformed into a common coordinate system; the remaining task is to select a seam, a curve that passes through the region of overlap and divides one of the two images from the other. Pixels on one side of the seam will be copied from one of the images, and pixels on the other side of the seam will be copied from the other image. Unlike other compositing methods that average pixels from both images, this produces a valid photographic image of every part of the region being photographed. They weight the edges of a grid graph by a numeric estimate of how visually apparent a seam across that edge would be, and find a bottleneck shortest path for these weights. Using this path as the seam, rather than a more conventional shortest path, causes their system to find a seam that is difficult to discern at all of its points, rather than allowing it to trade off greater visibility in one part of the image for lesser visibility elsewhere.A solution to the minimax path problem between the two opposite corners of a grid graph can be used to find the weak Fréchet distance between two polygonal chains. Here, each grid graph vertex represents a pair of line segments, one from each chain, and the weight of an edge represents the Fréchet distance needed to pass from one pair of segments to another.If all edge weights of an undirected graph are positive, then the minimax distances between pairs of points (the maximum edge weights of minimax paths) form an ultrametric; conversely every finite ultrametric space comes from minimax distances in this way. A data structure constructed from the minimum spanning tree allows the minimax distance between any pair of vertices to be queried in constant time per query, using lowest common ancestor queries in a Cartesian tree. The root of the Cartesian tree represents the heaviest minimum spanning tree edge, and the children of the root are Cartesian trees recursively constructed from the subtrees of the minimum spanning tree formed by removing the heaviest edge. The leaves of the Cartesian tree represent the vertices of the input graph, and the minimax distance between two vertices equals the weight of the Cartesian tree node that is their lowest common ancestor. Once the minimum spanning tree edges have been sorted, this Cartesian tree can be constructed in linear time.Directed graphsIn directed graphs, the maximum spanning tree solution cannot be used. Instead, several different algorithms are known; the choice of which algorithm to use depends on whether a start or destination vertex for the path is fixed, or whether paths for many start or destination vertices must be found simultaneously.All pairsThe all-pairs widest path problem has applications in the Schulze method for choosing a winner in multiway elections in which voters rank the candidates in preference order. The Schulze method constructs a complete directed graph in which the vertices represent the candidates and every two vertices are connected by an edge. Each edge is directed from the winner to the loser of a pairwise contest between the two candidates it connects, and is labeled with the margin of victory of that contest. Then the method computes widest paths between all pairs of vertices, and the winner is the candidate whose vertex has wider paths to each opponent than vice versa. The results of an election using this method are consistent with the Condorcet method – a candidate who wins all pairwise contests automatically wins the whole election – but it generally allows a winner to be selected, even in situations where the Concorcet method itself fails. The Schulze method has been used by several organizations including the Wikimedia Foundation.To compute the widest path widths for all pairs of nodes in a dense directed graph, such as the ones that arise in the voting application, the asymptotically fastest known approach takes time O(n(3+ω)/2) where ω is the exponent for fast matrix multiplication. Using the best known algorithms for matrix multiplication, this time bound becomes  O(n2.688). Instead, the reference implementation for the Schulze method uses a modified version of the simpler Floyd–Warshall algorithm, which takes O(n3) time. For sparse graphs, it may be more efficient to repeatedly apply a single-source widest path algorithm.Single sourceIf the edges are sorted by their weights, then a modified version of Dijkstra's algorithm can compute the bottlenecks between a designated start vertex and every other vertex in the graph, in linear time. The key idea behind the speedup over a conventional version of Dijkstra's algorithm is that the sequence of bottleneck distances to each vertex, in the order that the vertices are considered by this algorithm, is a monotonic subsequence of the sorted sequence of edge weights; therefore, the priority queue of Dijkstra's algorithm can be implemented as a bucket queue: an array indexed by the numbers from 1 to m (the number of edges in the graph), where array cell i contains the vertices whose bottleneck distance is the weight of the edge with position i in the sorted order. This method allows the widest path problem to be solved as quickly as sorting; for instance, if the edge weights are represented as integers, then the time bounds for integer sorting a list of m integers would apply also to this problem.Single source and single destinationBerman & Handler (1987) suggest that service vehicles and emergency vehicles should use minimax paths when returning from a service call to their base. In this application, the time to return is less important than the response time if another service call occurs while the vehicle is in the process of returning. By using a minimax path, where the weight of an edge is the maximum travel time from a point on the edge to the farthest possible service call, one can plan a route that minimizes the maximum possible delay between receipt of a service call and arrival of a responding vehicle. Ullah, Lee & Hassoun (2009) use maximin paths to model the dominant reaction chains in metabolic networks; in their model, the weight of an edge is the free energy of the metabolic reaction represented by the edge.Another application of widest paths arises in the Ford–Fulkerson algorithm for the maximum flow problem. Repeatedly augmenting a flow along a maximum capacity path in the residual network of the flow leads to a small bound, O(m log U), on the number of augmentations needed to find a maximum flow; here, the edge capacities are assumed to be integers that are at most U. However, this analysis does not depend on finding a path that has the exact maximum of capacity; any path whose capacity is within a constant factor of the maximum suffices. Combining this approximation idea with the shortest path augmentation method of the Edmonds–Karp algorithm leads to a maximum flow algorithm with running time O(mn log U).It is possible to find maximum-capacity paths and minimax paths with a single source and single destination very efficiently even in models of computation that allow only comparisons of the input graph's edge weights and not arithmetic on them. The algorithm maintains a set S of edges that are known to contain the bottleneck edge of the optimal path; initially, S is just the set of all m edges of the graph. At each iteration of the algorithm, it splits S into an ordered sequence of subsets S1, S2, ... of approximately equal size; the number of subsets in this partition is chosen in such a way that all of the split points between subsets can be found by repeated median-finding in time O(m). The algorithm then reweights each edge of the graph by the index of the subset containing the edge, and uses the modified Dijkstra algorithm on the reweighted graph; based on the results of this computation, it can determine in linear time which of the subsets contains the bottleneck edge weight. It then replaces S by the subset Si that it has determined to contain the bottleneck weight, and starts the next iteration with this new set S. The number of subsets into which S can be split increases exponentially with each step, so the number of iterations is proportional to the iterated logarithm function, O(log*n), and the total time is O(m log*n). In a model of computation where each edge weight is a machine integer, the use of repeated bisection in this algorithm can be replaced by a list-splitting technique of Han & Thorup (2002), allowing S to be split into O(√m) smaller sets Si in a single step and leading to a linear overall time bound.Euclidean point setsA variant of the minimax path problem has also been considered for sets of points in the Euclidean plane. As in the undirected graph problem, this Euclidean minimax path problem can be solved efficiently by finding a Euclidean minimum spanning tree: every path in the tree is a minimax path. However, the problem becomes more complicated when a path is desired that not only minimizes the hop length but also, among paths with the same hop length, minimizes or approximately minimizes the total length of the path. The solution can be approximated using geometric spanners.In number theory, the unsolved Gaussian moat problem asks whether or not minimax paths in the Gaussian prime numbers have bounded or unbounded minimax length. That is, does there exist a constant B such that, for every pair of points p and q in the infinite Euclidean point set defined by the Gaussian primes, the minimax path in the Gaussian primes between p and q has minimax edge length at most B?== References ==\"]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "do_search('Bellman-Ford')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GjY-G0OZT-i",
        "outputId": "b4f44e77-62e6-43b0-a441-141feb81d186"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['= Bellman–Ford algorithm =The Bellman–Ford algorithm is an algorithm that computes shortest paths from a single source vertex to all of the other vertices in a weighted digraph.It is slower than Dijkstra\\'s algorithm for the same problem, but more versatile, as it is capable of handling graphs in which some of the edge weights are negative numbers.The algorithm was first proposed by Alfonso Shimbel (1955), but is instead named after Richard Bellman and Lester Ford Jr., who published it in 1958 and 1956, respectively. Edward F. Moore also published a variation of the algorithm in 1959, and for this reason it is also sometimes called the Bellman–Ford–Moore algorithm.Negative edge weights are found in various applications of graphs, hence the usefulness of this algorithm.If a graph contains a \"negative cycle\" (i.e. a cycle whose edges sum to a negative value) that is reachable from the source, then there is no cheapest path: any path that has a point on the negative cycle can be made cheaper by one more walk around the negative cycle. In such a case, the Bellman–Ford algorithm can detect and report the negative cycle.AlgorithmLike Dijkstra\\'s algorithm, Bellman–Ford proceeds by relaxation, in which approximations to the correct distance are replaced by better ones until they eventually reach the solution. In both algorithms, the approximate distance to each vertex is always an overestimate of the true distance, and is replaced by the minimum of its old value and the length of a newly found path.However, Dijkstra\\'s algorithm uses a priority queue to greedily select the closest vertex that has not yet been processed, and performs this relaxation process on all of its outgoing edges; by contrast, the Bellman–Ford algorithm simply relaxes all the edges, and does this|V|−1{\\\\displaystyle |V|-1}times, where|V|{\\\\displaystyle |V|}is the number of vertices in the graph. In each of these repetitions, the number of vertices with correctly calculated distances grows, from which it follows that eventually all vertices will have their correct distances. This method allows the Bellman–Ford algorithm to be applied to a wider class of inputs than Dijkstra. The intermediate answers depend on the order of edges relaxed, but the final answer remains the same.Bellman–Ford runs inO(|V|⋅|E|){\\\\displaystyle O(|V|\\\\cdot |E|)}time, where|V|{\\\\displaystyle |V|}and|E|{\\\\displaystyle |E|}are the number of vertices and edges respectively.function BellmanFord(list vertices, list edges, vertex source) is// This implementation takes in a graph, represented as// lists of vertices (represented as integers [0..n-1]) and edges,// and fills two arrays (distance and predecessor) holding// the shortest path from the source to each vertexdistance := list of size npredecessor := list of size n// Step 1: initialize graphfor each vertex v in vertices dodistance[v] := inf             // Initialize the distance to all vertices to infinitypredecessor[v] := null         // And having a null predecessordistance[source] := 0              // The distance from the source to itself is, of course, zero// Step 2: relax edges repeatedlyrepeat |V|−1 times:for each edge (u, v) with weight w in edges doif distance[u] + w < distance[v] thendistance[v] := distance[u] + wpredecessor[v] := u// Step 3: check for negative-weight cyclesfor each edge (u, v) with weight w in edges doif distance[u] + w < distance[v] thenerror \"Graph contains a negative-weight cycle\"return distance, predecessorSimply put, the algorithm initializes the distance to the source to 0 and all other nodes to infinity. Then for all edges, if the distance to the destination can be shortened by taking the edge, the distance is updated to the new lower value. At each iteration i that the edges are scanned, the algorithm finds all shortest paths of at most length i edges (and possibly some paths longer than i edges). Since the longest possible path without a cycle can be|V|−1{\\\\displaystyle |V|-1}edges, the edges must be scanned|V|−1{\\\\displaystyle |V|-1}times to ensure the shortest path has been found for all nodes. A final scan of all the edges is performed and if any distance is updated, then a path of length|V|{\\\\displaystyle |V|}edges has been found which can only occur if at least one negative cycle exists in the graph.Proof of correctnessThe correctness of the algorithm can be shown by induction:Lemma. After i repetitions of for loop,if Distance(u) is not infinity, it is equal to the length of some path from s to u; andif there is a path from s to u with at most i edges, then Distance(u) is at most the length of the shortest path from s to u with at most i edges.Proof. For the base case of induction, consider i=0 and the moment before for loop is executed for the first time. Then, for the source vertex, source.distance = 0, which is correct. For other vertices u, u.distance = infinity, which is also correct because there is no path from source to u with 0 edges.For the inductive case, we first prove the first part. Consider a moment when a vertex\\'s distance is updated byv.distance := u.distance + uv.weight. By inductive assumption, u.distance is the length of some path from source to u. Then u.distance + uv.weight is the length of the path from source to v that follows the path from  source to u and then goes to v.For the second part, consider a shortest path P (there may be more than one) from source to v with at most i edges. Let u be the last vertex before v on this path. Then, the part of the path from source to u is a shortest path from source to u with at most i-1 edges, since if it were not, then there must be some strictly shorter path from source to u with at most i-1 edges, and we could then append the edge uv to this path to obtain a path with at most i edges that is strictly shorter than P—a contradiction. By inductive assumption, u.distance after i−1 iterations is at most the length of this path from source to u. Therefore, uv.weight + u.distance is at most the length of P. In the ith iteration, v.distance gets compared with uv.weight + u.distance, and is set equal to it if uv.weight + u.distance is smaller. Therefore, after i iterations, v.distance is at most the length of P, i.e., the length of the shortest path from source to v that uses at most i edges.If there are no negative-weight cycles, then every shortest path visits each vertex at most once, so at step 3 no further improvements can be made. Conversely, suppose no improvement can be made. Then for any cycle with vertices v[0], ..., v[k−1],v[i].distance <= v[i-1 (mod k)].distance + v[i-1 (mod k)]v[i].weightSumming around the cycle, the v[i].distance and v[i−1 (mod k)].distance terms cancel, leaving0 <= sum from 1 to k of v[i-1 (mod k)]v[i].weightI.e., every cycle has nonnegative weight.Finding negative cyclesWhen the algorithm is used to find shortest paths, the existence of negative cycles is a problem, preventing the algorithm from finding a correct answer. However, since it terminates upon finding a negative cycle, the Bellman–Ford algorithm can be used for applications in which this is the target to be sought – for example in cycle-cancelling techniques in network flow analysis.Applications in routingA distributed variant of the Bellman–Ford algorithm is used in distance-vector routing protocols, for example the Routing Information Protocol (RIP). The algorithm is distributed because it involves a number of nodes (routers) within an Autonomous system (AS), a collection of IP networks typically owned by an ISP.It consists of the following steps:Each node calculates the distances between itself and all other nodes within the AS and stores this information as a table.Each node sends its table to all neighboring nodes.When a node receives distance tables from its neighbors, it calculates the shortest routes to all other nodes and updates its own table to reflect any changes.The main disadvantages of the Bellman–Ford algorithm in this setting are as follows:It does not scale well.Changes in network topology are not reflected quickly since updates are spread node-by-node.Count to infinity if link or node failures render a node unreachable from some set of other nodes, those nodes may spend forever gradually increasing their estimates of the distance to it, and in the meantime there may be routing loops.ImprovementsThe Bellman–Ford algorithm may be improved in practice (although not in the worst case) by the observation that, if an iteration of the main loop of the algorithm terminates without making any changes, the algorithm can be immediately terminated, as subsequent iterations will not make any more changes. With this early termination condition, the main loop may in some cases use many fewer than |V| − 1 iterations, even though the worst case of the algorithm remains unchanged. The following improvements all maintain theO(|V|⋅|E|){\\\\displaystyle O(|V|\\\\cdot |E|)}worst-case time complexity.A variation of the Bellman-Ford algorithm known as Shortest Path Faster Algorithm, first described by Moore (1959), reduces the number of relaxation steps that need to be performed within each iteration of the algorithm. If a vertex v has a distance value that has not changed since the last time the edges out of v were relaxed, then there is no need to relax the edges out of v a second time. In this way, as the number of vertices with correct distance values grows, the number whose outgoing edges that need to be relaxed in each iteration shrinks, leading to a constant-factor savings in time for dense graphs.Yen (1970) described another improvement to the Bellman–Ford algorithm. His improvement first assigns some arbitrary linear order on all vertices and then partitions the set of all edges into two subsets. The first subset, Ef, contains all edges (vi, vj) such that i < j; the second, Eb, contains edges (vi, vj) such that i > j. Each vertex is visited in the order v1, v2, ..., v|V|, relaxing each outgoing edge from that vertex in Ef. Each vertex is then visited in the order v|V|, v|V|−1, ..., v1, relaxing each outgoing edge from that vertex in Eb. Each iteration of the main loop of the algorithm, after the first one, adds at least two edges to the set of edges whose relaxed distances match the correct shortest path distances: one from Ef and one from Eb. This modification reduces the worst-case number of iterations of the main loop of the algorithm from |V| − 1 to|V|/2{\\\\displaystyle |V|/2}.Another improvement, by Bannister & Eppstein (2012), replaces the arbitrary linear order of the vertices used in Yen\\'s second improvement by a random permutation. This change makes the worst case for Yen\\'s improvement (in which the edges of a shortest path strictly alternate between the two subsets Ef and Eb) very unlikely to happen. With a randomly permuted vertex ordering, the expected number of iterations needed in the main loop is at most|V|/3{\\\\displaystyle |V|/3}.NotesReferencesOriginal sourcesShimbel, A. (1955). Structure in communication nets. Proceedings of the Symposium on Information Networks. New York, New York: Polytechnic Press of the Polytechnic Institute of Brooklyn. pp. 199–203.Bellman, Richard (1958). \"On a routing problem\". Quarterly of Applied Mathematics. 16: 87–90. doi:10.1090/qam/102435. MR 0102435.Ford, Lester R. Jr. (August 14, 1956). Network Flow Theory. Paper P-923. Santa Monica, California: RAND Corporation.Moore, Edward F. (1959). The shortest path through a maze. Proc. Internat. Sympos. Switching Theory 1957, Part II. Cambridge, Massachusetts: Harvard Univ. Press. pp. 285–292. MR 0114710.Yen, Jin Y. (1970). \"An algorithm for finding shortest routes from all source nodes to a given destination in general networks\". Quarterly of Applied Mathematics. 27 (4): 526–530. doi:10.1090/qam/253822. MR 0253822.Bannister, M. J.; Eppstein, D. (2012). Randomized speedup of the Bellman–Ford algorithm. Analytic Algorithmics and Combinatorics (ANALCO12), Kyoto, Japan. pp. 41–47. arXiv:1111.5414. doi:10.1137/1.9781611973020.6.Secondary sourcesBang-Jensen, Jørgen; Gutin, Gregory (2000). \"Section 2.3.4: The Bellman-Ford-Moore algorithm\". Digraphs: Theory, Algorithms and Applications (First ed.). ISBN 978-1-84800-997-4.Schrijver, Alexander (2005). \"On the history of combinatorial optimization (till 1960)\" (PDF). Handbook of Discrete Optimization. Elsevier: 1–68.Cormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L. Introduction to Algorithms. MIT Press and McGraw-Hill., Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7. Section 24.1: The Bellman–Ford algorithm, pp. 588–592. Problem 24-1, pp. 614–615. Third Edition. MIT Press, 2009. ISBN 978-0-262-53305-8. Section 24.1: The Bellman–Ford algorithm, pp. 651–655.Heineman, George T.; Pollice, Gary; Selkow, Stanley (2008). \"Chapter 6: Graph Algorithms\". Algorithms in a Nutshell. O\\'Reilly Media. pp. 160–164. ISBN 978-0-596-51624-6.Kleinberg, Jon; Tardos, Éva (2006). Algorithm Design. New York: Pearson Education, Inc.Sedgewick, Robert (2002). \"Section 21.7: Negative Edge Weights\". Algorithms in Java (3rd ed.). ISBN 0-201-36121-3. Archived from the original on 2008-05-31. Retrieved 2007-05-28.',\n",
              " \"= Johnson's algorithm =Johnson's algorithm is a way to find the shortest paths between all pairs of vertices in an edge-weighted directed graph. It allows some of the edge weights to be negative numbers, but no negative-weight cycles may exist. It works by using the Bellman–Ford algorithm to compute a transformation of the input graph that removes all negative weights, allowing Dijkstra's algorithm to be used on the transformed graph. It is named after Donald B. Johnson, who first published the technique in 1977.A similar reweighting technique is also used in Suurballe's algorithm for finding two disjoint paths of minimum total length between the same two vertices in a graph with non-negative edge weights.Algorithm descriptionJohnson's algorithm consists of the following steps:First, a new node q is added to the graph, connected by zero-weight edges to each of the other nodes.Second, the Bellman–Ford algorithm is used, starting from the new vertex q, to find for each vertex v the minimum weight h(v) of a path from q to v. If this step detects a negative cycle, the algorithm is terminated.Next the edges of the original graph are reweighted using the values computed by the Bellman–Ford algorithm: an edge from u to v, having lengthw(u,v){\\\\displaystyle w(u,v)}, is given the new length w(u,v) + h(u) − h(v).Finally, q is removed, and Dijkstra's algorithm is used to find the shortest paths from each node s to every other vertex in the reweighted graph. The distance in the original graph is then computed for each distance D(u , v), by adding h(v) − h(u) to the distance returned by Dijkstra's algorithm.ExampleThe first three stages of Johnson's algorithm are depicted in the illustration below.The graph on the left of the illustration has two negative edges, but no negative cycles. The center graph shows the new vertex q, a shortest path tree as computed by the Bellman–Ford algorithm with q as starting vertex, and the values h(v) computed at each other node as the length of the shortest path from q to that node. Note that these values are all non-positive, because q has a length-zero edge to each vertex and the shortest path can be no longer than that edge. On the right is shown the reweighted graph, formed by replacing each edge weightw(u,v){\\\\displaystyle w(u,v)}by w(u,v) + h(u) − h(v). In this reweighted graph, all edge weights are non-negative, but the shortest path between any two nodes uses the same sequence of edges as the shortest path between the same two nodes in the original graph. The algorithm concludes by applying Dijkstra's algorithm to each of the four starting nodes in the reweighted graph.CorrectnessIn the reweighted graph, all paths between a pair s and t of nodes have the same quantity h(s) − h(t) added to them. The previous statement can be proven as follows: Let p be ans−t{\\\\displaystyle s-t}path. Its weight W in the reweighted graph is given by the following expression:(w(s,p1)+h(s)−h(p1))+(w(p1,p2)+h(p1)−h(p2))+...+(w(pn,t)+h(pn)−h(t)).{\\\\displaystyle {\\\\bigl (}w(s,p_{1})+h(s)-h(p_{1}){\\\\bigr )}+{\\\\bigl (}w(p_{1},p_{2})+h(p_{1})-h(p_{2}){\\\\bigr )}+...+{\\\\bigl (}w(p_{n},t)+h(p_{n})-h(t){\\\\bigr )}.}Every+h(pi){\\\\displaystyle +h(p_{i})}is cancelled by−h(pi){\\\\displaystyle -h(p_{i})}in the previous bracketed expression; therefore, we are left with the following expression for W:(w(s,p1)+w(p1,p2)+...+w(pn,t))+h(s)−h(t){\\\\displaystyle {\\\\bigl (}w(s,p_{1})+w(p_{1},p_{2})+...+w(p_{n},t){\\\\bigr )}+h(s)-h(t)}The bracketed expression is the weight of p in the original weighting.Since the reweighting adds the same amount to the weight of everys−t{\\\\displaystyle s-t}path, a path is a shortest path in the original weighting if and only if it is a shortest path after reweighting. The weight of edges that belong to a shortest path from q to any node is zero, and therefore the lengths of the shortest paths from q to every node become zero in the reweighted graph; however, they still remain shortest paths. Therefore, there can be no negative edges: if edge uv had a negative weight after the reweighting, then the zero-length path from q to u together with this edge would form a negative-length path from q to v, contradicting the fact that all vertices have zero distance from q. The non-existence of negative edges ensures the optimality of the paths found by Dijkstra's algorithm. The distances in the original graph may be calculated from the distances calculated by Dijkstra's algorithm in the reweighted graph by reversing the reweighting transformation.AnalysisThe time complexity of this algorithm, using Fibonacci heaps in the implementation of Dijkstra's algorithm, isO(|V|2log\\u2061|V|+|V||E|){\\\\displaystyle O(|V|^{2}\\\\log |V|+|V||E|)}: the algorithm usesO(|V||E|){\\\\displaystyle O(|V||E|)}time for the Bellman–Ford stage of the algorithm, andO(|V|log\\u2061|V|+|E|){\\\\displaystyle O(|V|\\\\log |V|+|E|)}for each of the|V|{\\\\displaystyle |V|}instantiations of Dijkstra's algorithm. Thus, when the graph is sparse, the total time can be faster than the Floyd–Warshall algorithm, which solves the same problem in timeO(|V|3){\\\\displaystyle O(|V|^{3})}.ReferencesExternal linksBoost: All Pairs Shortest Paths\",\n",
              " \"= Shortest Path Faster Algorithm =The Shortest Path Faster Algorithm (SPFA) is an improvement of the Bellman–Ford algorithm which computes single-source shortest paths in a weighted directed graph. The algorithm is believed to work well on random sparse graphs and is particularly suitable for graphs that contain negative-weight edges. However, the worst-case complexity of SPFA is the same as that of Bellman–Ford, so for graphs with nonnegative edge weights Dijkstra's algorithm is preferred.  The SPFA algorithm was first published by Edward F. Moore in 1959, as a generalization of breadth first search; SPFA is Moore's “Algorithm D.” The name, “Shortest Path Faster Algorithm (SPFA),” was given by FanDing Duan, a Chinese researcher who rediscovered the algorithm in 1994.AlgorithmGiven a weighted directed graphG=(V,E){\\\\displaystyle G=(V,E)}and a source vertexs{\\\\displaystyle s}, the SPFA algorithm finds the shortest path froms{\\\\displaystyle s},  to each vertexv{\\\\displaystyle v},  in the graph. The length of the shortest path froms{\\\\displaystyle s}tov{\\\\displaystyle v}is stored ind(v){\\\\displaystyle d(v)}for each vertexv{\\\\displaystyle v}.The basic idea of SPFA is the same as the Bellman-Ford algorithm in that each vertex is used as a candidate to relax its adjacent vertices. The improvement over the latter is that instead of trying all vertices blindly, SPFA maintains a queue of candidate vertices and adds a vertex to the queue only if that vertex is relaxed. This process repeats until no more vertex can be relaxed.Below is the pseudo-code of the algorithm. HereQ{\\\\displaystyle Q}is a first-in, first-out queue of candidate vertices, andw(u,v){\\\\displaystyle w(u,v)}is the edge weight of(u,v){\\\\displaystyle (u,v)}.procedure Shortest-Path-Faster-Algorithm(G, s)1    for each vertex v ≠ s in V(G)2        d(v) := ∞3    d(s) := 04    push s into Q5    while Q is not empty do6        u := poll Q7        for each edge (u, v) in E(G) do8            if d(u) + w(u, v) < d(v) then9                d(v) := d(u) + w(u, v)10                if v is not in Q then11                    push v into QThe algorithm can also be applied to an undirected graph by replacing each undirected edge with two directed edges of opposite directions.Proof of correctnessWe will prove that the algorithm never computes incorrect shortest path lengths.Lemma: Whenever the queue is checked for emptiness, any vertex currently capable of causing relaxation is in the queue.Proof: We want to show that ifdist[w]>dist[u]+w(u,w){\\\\displaystyle dist[w]>dist[u]+w(u,w)}for any two verticesu{\\\\displaystyle u}andw{\\\\displaystyle w}at the time the condition is checked,u{\\\\displaystyle u}is in the queue. We do so by induction on the number of iterations of the loop that have already occurred. First we note that this certainly holds before the loop is entered: ifu≠s{\\\\displaystyle u\\\\not =s}, then relaxation is not possible; relaxation is possible fromu=s{\\\\displaystyle u=s}, and this is added to the queue immediately before the while loop is entered. Now, consider what happens inside the loop. A vertexu{\\\\displaystyle u}is popped, and is used to relax all its neighbors, if possible. Therefore, immediately after that iteration of the loop,u{\\\\displaystyle u}is not capable of causing any more relaxations (and does not have to be in the queue anymore). However, the relaxation byx{\\\\displaystyle x}might cause some other vertices to become capable of causing relaxation. If there exists some vertexx{\\\\displaystyle x}such thatdist[x]>dist[w]+w(w,x){\\\\displaystyle dist[x]>dist[w]+w(w,x)}before the current loop iteration, thenw{\\\\displaystyle w}is already in the queue. If this condition becomes true during the current loop iteration, then eitherdist[x]{\\\\displaystyle dist[x]}increased, which is impossible, ordist[w]{\\\\displaystyle dist[w]}decreased, implying thatw{\\\\displaystyle w}was relaxed. But afterw{\\\\displaystyle w}is relaxed, it is added to the queue if it is not already present.Corollary: The algorithm terminates when and only when no further relaxations are possible.Proof: If no further relaxations are possible, the algorithm continues to remove vertices from the queue, but does not add any more into the queue, because vertices are added only upon successful relaxations. Therefore the queue becomes empty and the algorithm terminates. If any further relaxations are possible, the queue is not empty, and the algorithm continues to run.The algorithm fails to terminate if negative-weight cycles are reachable from the source. See here for a proof that relaxations are always possible when negative-weight cycles exist. In a graph with no cycles of negative weight, when no more relaxations are possible, the correct shortest paths have been computed (proof). Therefore in graphs containing no cycles of negative weight, the algorithm will never terminate with incorrect shortest path lengths.Running timeThe worst-case running time of the algorithm isO(|V|⋅|E|){\\\\displaystyle O(|V|\\\\cdot |E|)}, just like the standard Bellman-Ford algorithm.  Experiments suggest that the average running time isO(|E|){\\\\displaystyle O(|E|)}, and indeed this is true on random graphs, but it is possible to construct sparse graphs where SPFA runs in timeΩ(|V|2){\\\\displaystyle \\\\Omega (|V|^{2})}like the usual Bellman-Ford algorithm.Optimization techniquesThe performance of the algorithm is strongly determined by the order in which candidate vertices are used to relax other vertices. In fact, ifQ{\\\\displaystyle Q}is a priority queue, then the algorithm pretty much resembles Dijkstra's. However, since a priority queue is not used here, two techniques are sometimes employed to improve the quality of the queue, which in turn improves the average-case performance (but not the worst-case performance). Both techniques rearrange the order of elements inQ{\\\\displaystyle Q}so that vertices closer to the source are processed first. Therefore, when implementing these techniques,Q{\\\\displaystyle Q}is no longer a first-in, first-out queue, but rather a normal doubly linked list or double-ended queue.Small Label First (SLF) technique. In line 11, instead of always pushing vertexv{\\\\displaystyle v}to the end of the queue, we compared(v){\\\\displaystyle d(v)}tod(front(Q)){\\\\displaystyle d{\\\\big (}{\\\\text{front}}(Q){\\\\big )}}, and insertv{\\\\displaystyle v}to the front of the queue ifd(v){\\\\displaystyle d(v)}is smaller. The pseudo-code for this technique is (after pushingv{\\\\displaystyle v}to the end of the queue in line 11):procedure Small-Label-First(G, Q)if d(back(Q)) < d(front(Q)) thenu := pop back of Qpush u into front of QLarge Label Last (LLL) technique. After line 11, we update the queue so that the first element is smaller than the average, and any element larger than the average is moved to the end of the queue. The pseudo-code is:procedure Large-Label-Last(G, Q)x := average of d(v) for all v in Qwhile d(front(Q)) > xu := pop front of Qpush u to back of Q== References ==\",\n",
              " '= Zero-weight cycle problem =In computer science and graph theory, the zero-weight cycle problem is the problem of deciding whether a directed graph with weights on the edges (which may be positive or negative or zero) has a cycle in which the sum of weights is 0.A related problem is to decide whether the graph has a cycle in which the sum of weights is less than 0. This related problem can be solved in polynomial time using the Bellman–Ford algorithm. In contrast, detecting a cycle of weight exactly 0 is an NP-complete problem.The problem is in NP since, given a cycle, it is easy to verify that its weight is 0.The proof of NP-hardness is by reduction from the subset sum problem. In this problem we are given a set of numbers, positive and some negative, and have to decide whether there exists a subset whose sum is exactly 0. Given an instance of subset-sum with n numbers, construct an instance of zero-weight-cycle as follows. Construct a graph with 2n vertices. For each number ai the graph contains two vertices: ui and vi. From each ui, there is only one outgoing edge, which goes to vi and has weight ai. From each vi, there are n outgoing edges, which go to each uj and have weights 0. Any cycle in this graph have the form u1-v1-u2-v2-...-uk-vk.   The weight of a cycle is 0, iff the sum of all weights between each ui and its corresponding vi is 0, iff the sum of all corresponding ai is 0, iff there is a subset with a sum of 0.ReferencesExternal linksWinston (2017-06-06). \"Reducing subset sum to zero weight cycle\". Math StackExchange. Retrieved 2019-04-11.D.W. (2017-06-06). \"Detecting a zero-weight cycle in a graph without negative cycles\". Computer Science StackExchange. Retrieved 2019-04-11.',\n",
              " '= K shortest path routing =The k shortest path routing problem is a generalization of the shortest path routing problem in a given network. It asks not only about a shortest path but also about next k−1 shortest paths (which may be longer than the shortest path). A variation of the problem is the loopless k shortest paths.Finding k shortest paths is possible by extending Dijkstra algorithm or Bellman-Ford algorithm and extend them to find more than one path.HistorySince 1957 many papers were published on the k shortest path routing problem. Most of the fundamental works were done between 1960s and 2001. Since then, most of the research has been on the problem\\'s applications and its variants. In 2010, Michael Günther et al. published a book on Symbolic calculation of k-shortest paths and related measures with the stochastic process algebra tool CASPA.AlgorithmThe Dijkstra algorithm can be generalized to find the k shortest paths.VariationsThere are two main variations of the k shortest path routing problem.  In one variation, paths are allowed to visit the same node more than once, thus creating loops.  In another variation, paths are required to be simple and loopless.  The loopy version is solvable using Eppstein\\'s algorithm and the loopless variation is solvable by Yen\\'s algorithm.Loopy variantIn this variant, the problem is simplified by not requiring paths to be loopless.  A solution was given by B. L. Fox in 1975 in which the k-shortest paths are determined in O(m + kn log n) asymptotic time complexity (using big O notation.  In 1998, David Eppstein reported an approach that maintains an asymptotic complexity of O(m + n log n + k) by computing an implicit representation of the paths, each of which can be output in O(n) extra time. In 2015, Akuba et al. devised an indexing method as a significantly faster alternative for Eppstein\\'s algorithm, in which a data structure called an index is constructed from a graph and then top-k distances between arbitrary pairs of vertices can be rapidly obtained.Loopless variantIn the loopless variant, the paths are forbidden to contain loops which adds an additional level of complexity. It can be solved using Yen\\'s algorithm to find the lengths of all shortest paths from a fixed node to all other nodes in an n-node non negative-distance network, a technique requiring only 2n2 additions and n2 comparison, fewer than other available shortest path algorithms need.  The running time complexity is pseudo-polynomial, being O(kn(m + n log n)) (where m and n represent the number of edges and vertices, respectively). In 2007, John Hershberger and Subhash Suri proposed a replacement paths algorithm, a more efficient implementation of Lawler\\'s  and Yen\\'s algorithm with O(n) improvement in time.Some examples and descriptionExample #1The following example makes use of Yen’s model to find k shortest paths between communicating end nodes. That is, it finds a shortest path, second shortest path, etc. up to the Kth shortest path. More details can be found here.The code provided in this example attempts to solve the k shortest path routing problem for a 15-nodes network containing a combination of unidirectional and bidirectional links:Example #2Another example is the use of k shortest paths algorithm to track multiple objects. The technique implements a multiple object tracker based on the k shortest paths routing algorithm. A set of probabilistic occupancy maps is used as input. An object detector provides the input.The complete details can be found at \"Computer Vision Laboratory – CVLAB\".Example #3Another use of k shortest paths algorithms is to design a transit network that enhances passengers\\' experience in public transportation systems. Such an example of a transit network can be constructed by putting traveling time under consideration. In addition to traveling time, other conditions may be taken depending upon economical and geographical limitations. Despite variations in parameters, the k shortest path algorithms finds the most optimal solutions that satisfies almost all user needs. Such applications of k shortest path algorithms are becoming common, recently Xu, He, Song, and Chaudry (2012) studied the k shortest path problems in transit network systems.ApplicationsThe k shortest path routing is a good alternative for:Geographic path planningNetwork routing, especially in optical mesh network where there are additional constraints that cannot be solved by using ordinary shortest path algorithms.Hypothesis generation in computational linguisticsSequence alignment and metabolic pathway finding in bioinformaticsMultiple object tracking as described aboveRoad Networks: road junctions are the nodes (vertices) and each  edge (link) of the graph is associated with a road segment between two junctions.Related problemsThe breadth-first search algorithm is used when the search is only limited to two operations.The Floyd–Warshall algorithm solves all pairs shortest paths.Johnson\\'s algorithm solves all pairs\\' shortest paths, and may be faster than Floyd–Warshall on sparse graphs.Perturbation theory finds (at worst) the locally shortest path.Cherkassky et al. provide more algorithms and associated evaluations.See alsoConstrained shortest path routingNotesExternal linksImplementation of Yen\\'s algorithmImplementation of Yen\\'s and fastest k shortest simple paths algorithmshttp://www.technical-recipes.com/2012/the-k-shortest-paths-algorithm-in-c/#more-2432Multiple objects tracking technique using K-shortest path algorithm: http://cvlab.epfl.ch/software/ksp/Computer Vision Laboratory: http://cvlab.epfl.ch/software/ksp/',\n",
              " '= Dijkstra\\'s algorithm =Dijkstra\\'s algorithm ( DYKE-strəz) is an algorithm for finding the shortest paths between nodes in a graph, which may represent, for example, road networks.  It was conceived by computer scientist Edsger W. Dijkstra in 1956 and published three years later.The algorithm exists in many variants. Dijkstra\\'s original algorithm found the shortest path between two given nodes, but a more common variant fixes a single node as the \"source\" node and finds shortest paths from the source to all other nodes in the graph, producing a shortest-path tree.For a given source node in the graph, the algorithm finds the shortest path between that node and every other.:\\u200a196–206\\u200a It can also be used for finding the shortest paths from a single node to a single destination node by stopping the algorithm once the shortest path to the destination node has been determined. For example, if the nodes of the graph represent cities and edge path costs represent driving distances between pairs of cities connected by a direct road (for simplicity, ignore red lights, stop signs, toll roads and other obstructions), Dijkstra\\'s algorithm can be used to find the shortest route between one city and all other cities. A widely used application of shortest path algorithms is network routing protocols, most notably IS-IS (Intermediate System to Intermediate System) and Open Shortest Path First (OSPF). It is also employed as a subroutine in other algorithms such as Johnson\\'s.The Dijkstra algorithm uses labels that are positive integers or real numbers, which are totally ordered.  It can be generalized to use any labels that are partially ordered, provided the subsequent labels (a subsequent label is produced when traversing an edge) are monotonically non-decreasing.  This generalization is called the generic Dijkstra shortest-path algorithm.Dijkstra\\'s algorithm uses a data structure for storing and querying partial solutions sorted by distance from the start. While the original algorithm uses a min-priority queue and runs in timeΘ((|V|+|E|)log\\u2061|V|){\\\\displaystyle \\\\Theta ((|V|+|E|)\\\\log |V|)}(where|V|{\\\\displaystyle |V|}is the number of nodes and|E|{\\\\displaystyle |E|}is the number of edges), it can also be implemented inΘ(|V|2){\\\\displaystyle \\\\Theta (|V|^{2})}using an array. The idea of this algorithm is also given in Leyzorek et al. 1957. Fredman & Tarjan 1984 propose using a Fibonacci heap min-priority queue to optimize the running time complexity toΘ(|E|+|V|log\\u2061|V|){\\\\displaystyle \\\\Theta (|E|+|V|\\\\log |V|)}. This is asymptotically the fastest known single-source shortest-path algorithm for arbitrary directed graphs with unbounded non-negative weights. However, specialized cases (such as bounded/integer weights, directed acyclic graphs etc.) can indeed be improved further as detailed in Specialized variants. Additionally, if preprocessing is allowed algorithms such as contraction hierarchies can be up to seven orders of magnitude faster.In some fields, artificial intelligence in particular, Dijkstra\\'s algorithm or a variant of it is known as uniform cost search and formulated as an instance of the more general idea of best-first search.HistoryWhat is the shortest way to travel from Rotterdam to Groningen, in general: from given city to given city. It is the algorithm for the shortest path, which I designed in about twenty minutes. One morning I was shopping in Amsterdam with my young fiancée, and tired, we sat down on the café terrace to drink a cup of coffee and I was just thinking about whether I could do this, and I then designed the algorithm for the shortest path. As I said, it was a twenty-minute invention. In fact, it was published in \\'59, three years later. The publication is still readable, it is, in fact, quite nice. One of the reasons that it is so nice was that I designed it without pencil and paper. I learned later that one of the advantages of designing without pencil and paper is that you are almost forced to avoid all avoidable complexities. Eventually, that algorithm became to my great amazement, one of the cornerstones of my fame.Dijkstra thought about the shortest path problem when working at the Mathematical Center in Amsterdam in 1956 as a programmer to demonstrate the capabilities of a new computer called ARMAC. His objective was to choose both a problem and a solution (that would be produced by computer) that non-computing people could understand. He designed the shortest path algorithm and later implemented it for ARMAC for a slightly simplified transportation map of 64 cities in the Netherlands (64, so that 6 bits would be sufficient to encode the city number). A year later, he came across another problem from hardware engineers working on the institute\\'s next computer: minimize the amount of wire needed to connect the pins on the back panel of the machine. As a solution, he re-discovered the algorithm known as Prim\\'s minimal spanning tree algorithm (known earlier to Jarník, and also rediscovered by Prim). Dijkstra published the algorithm in 1959, two years after Prim and 29 years after Jarník.AlgorithmLet the node at which we are starting be called the initial node. Let the distance of node Y be the distance from the initial node to Y. Dijkstra\\'s algorithm will initially start with infinite distances and will try to improve them step by step.Mark all nodes unvisited. Create a set of all the unvisited nodes called the unvisited set.Assign to every node a tentative distance value: set it to zero for our initial node and to infinity for all other nodes. The tentative distance of a node v is the length of the shortest path discovered so far between the node v and the starting node. Since initially no path is known to any other vertex than the source itself (which is a path of length zero), all other tentative distances are initially set to infinity. Set the initial node as current.For the current node, consider all of its unvisited neighbors and calculate their tentative distances through the current node. Compare the newly calculated tentative distance to the current assigned value and assign the smaller one. For example, if the current node A is marked with a  distance of 6, and the edge connecting it with a neighbor B has length 2, then the distance to B through A will be 6 + 2 = 8. If B was previously marked with a distance greater than 8 then change it to 8. Otherwise, the current value will be kept.When we are done considering all of the unvisited neighbors of the current node, mark the current node as visited and remove it from the unvisited set. A visited node will never be checked again.If the destination node has been marked visited (when planning a route between two specific nodes) or if the smallest tentative distance among the nodes in the unvisited set is infinity (when planning a complete traversal; occurs when there is no connection between the initial node and remaining unvisited nodes), then stop. The algorithm has finished.Otherwise, select the unvisited node that is marked with the smallest tentative distance, set it as the new current node, and go back to step 3.When planning a route, it is actually not necessary to wait until the destination node is \"visited\" as above: the algorithm can stop once the destination node has the smallest tentative distance among all \"unvisited\" nodes (and thus could be selected as the next \"current\").DescriptionSuppose you would like to find the shortest path between two intersections on a city map: a starting point and a destination. Dijkstra\\'s algorithm initially marks the distance (from the starting point) to every other intersection on the map with infinity. This is done not to imply that there is an infinite distance, but to note that those intersections have not been visited yet. Some variants of this method leave the intersections\\' distances unlabeled. Now select the current intersection at each iteration.  For the first iteration, the current intersection will be the starting point, and the distance to it (the intersection\\'s label) will be zero. For subsequent iterations (after the first), the current intersection will be a closest unvisited intersection to the starting point (this will be easy to find).From the current intersection, update the distance to every unvisited intersection that is directly connected to it. This is done by determining the sum of the distance between an unvisited intersection and the value of the current intersection and then relabeling the unvisited intersection with this value (the sum) if it is less than the unvisited intersection\\'s current value. In effect, the intersection is relabeled if the path to it through the current intersection is shorter than the previously known paths.  To facilitate shortest path identification, in pencil, mark the road with an arrow pointing to the relabeled intersection if you label/relabel it, and erase all others pointing to it.  After you have updated the distances to each neighboring intersection, mark the current intersection as visited and select an unvisited intersection with minimal distance (from the starting point) – or the lowest label—as the current intersection. Intersections marked as visited are labeled with the shortest path from the starting point to it and will not be revisited or returned to.Continue this process of updating the neighboring intersections with the shortest distances, marking the current intersection as visited, and moving onto a closest unvisited intersection until you have marked the destination as visited. Once you have marked the destination as visited (as is the case with any visited intersection), you have determined the shortest path to it from the starting point and can trace your way back following the arrows in reverse. In the algorithm\\'s implementations, this is usually done (after the algorithm has reached the destination node) by following the nodes\\' parents from the destination node up to the starting node; that\\'s why we also keep track of each node\\'s parent.This algorithm makes no attempt of direct \"exploration\" towards the destination as one might expect. Rather, the sole consideration in determining the next \"current\" intersection is its distance from the starting point. This algorithm therefore expands outward from the starting point, interactively considering every node that is closer in terms of shortest path distance until it reaches the destination. When understood in this way, it is clear how the algorithm necessarily finds the shortest path. However, it may also reveal one of the algorithm\\'s weaknesses: its relative slowness in some topologies.PseudocodeIn the following pseudocode algorithm, dist is an array that contains the current distances from the source to other vertices, i.e. dist[u] is the current distance from the source to the vertex u. The prev array contains pointers to previous-hop nodes on the shortest path from source to the given vertex (equivalently, it is the next-hop on the path from the given vertex to the source). The code u ← vertex in Q with min dist[u], searches for the vertex u in the vertex set Q that has the least dist[u] value. Graph.Edges(u, v) returns the length of the edge joining (i.e. the distance between) the two neighbor-nodes u and v. The variable alt on line 14 is the length of the path from the root node to the neighbor node v if it were to go through u. If this path is shorter than the current shortest path recorded for v, that current path is replaced with this alt path.1  function Dijkstra(Graph, source):23      for each vertex v in Graph.Vertices:4          dist[v] ← INFINITY5          prev[v] ← UNDEFINED6          add v to Q7      dist[source] ← 089      while Q is not empty:10          u ← vertex in Q with min dist[u]1112          for each neighbor v of u still in Q:13              alt ← dist[u] + Graph.Edges(u, v)14              if alt < dist[v]:15                  dist[v] ← alt16                  prev[v] ← u1718      return dist[], prev[]If we are only interested in a shortest path between vertices source and target, we can terminate the search after line 10 if u = target.Now we can read the shortest path from source to target by reverse iteration:1  S ← empty sequence2  u ← target3  if prev[u] is defined or u = source:          // Do something only if the vertex is reachable4      while u is defined:                       // Construct the shortest path with a stack S5          insert u at the beginning of S        // Push the vertex onto the stack6          u ← prev[u]                           // Traverse from target to sourceNow sequence S is the list of vertices constituting one of the shortest paths from source to target, or the empty sequence if no path exists.A more general problem would be to find all the shortest paths between source and target (there might be several different ones of the same length). Then instead of storing only a single node in each entry of prev[] we would store all nodes satisfying the relaxation condition. For example, if both r and source connect to target and both of them lie on different shortest paths through target (because the edge cost is the same in both cases), then we would add both r and source to prev[target]. When the algorithm completes, prev[] data structure will actually describe a graph that is a subset of the original graph with some edges removed. Its key property will be that if the algorithm was run with some starting node, then every path from that node to any other node in the new graph will be the shortest path between those nodes in the original graph, and all paths of that length from the original graph will be present in the new graph. Then to actually find all these shortest paths between two given nodes we would use a path finding algorithm on the new graph, such as depth-first search.Using a priority queueA min-priority queue is an abstract data type that provides 3 basic operations: add_with_priority(), decrease_priority() and extract_min(). As mentioned earlier, using such a data structure can lead to faster computing times than using a basic queue. Notably, Fibonacci heap (Fredman & Tarjan 1984) or Brodal queue offer optimal implementations for those 3 operations. As the algorithm is slightly different, we mention it here, in pseudo-code as well :1  function Dijkstra(Graph, source):2      dist[source] ← 0                           // Initialization34      create vertex priority queue Q56      for each vertex v in Graph.Vertices:7          if v ≠ source8              dist[v] ← INFINITY                 // Unknown distance from source to v9              prev[v] ← UNDEFINED                // Predecessor of v1011         Q.add_with_priority(v, dist[v])121314     while Q is not empty:                      // The main loop15         u ← Q.extract_min()                    // Remove and return best vertex16         for each neighbor v of u:              // only v that are still in Q17             alt ← dist[u] + Graph.Edges(u, v)18             if alt < dist[v]19                 dist[v] ← alt20                 prev[v] ← u21                 Q.decrease_priority(v, alt)2223     return dist, prevInstead of filling the priority queue with all nodes in the initialization phase, it is also possible to initialize it to contain only source; then, inside the if alt < dist[v] block, the decrease_priority() becomes an add_with_priority() operation if the node is not already in the queue.:\\u200a198\\u200aYet another alternative is to add nodes unconditionally to the priority queue and to instead check after extraction that no shorter connection was found yet. This can be done by additionally extracting the associated priority p from the queue and only processing further if p == dist[u] inside the while Q is not empty loop. These alternatives can use entirely array-based priority queues without decrease-key functionality, which have been found to achieve even faster computing times in practice. However, the difference in performance was found to be narrower for denser graphs.Proof of correctnessProof of Dijkstra\\'s algorithm is constructed by induction on the number of visited nodes.Invariant hypothesis: For each node v, dist[v] is the shortest distance from source to v when traveling via visited nodes only, or infinity if no such path exists. (Note: we do not assume dist[v] is the actual shortest distance for unvisited nodes.)The base case is when there is just one visited node, namely the initial node source, in which case the hypothesis is trivial.Otherwise, assume the hypothesis for n-1 visited nodes. In which case, we choose an edge vu where u has the least dist[u] of any unvisited nodes such that dist[u] = dist[v] + Graph.Edges[v,u]. dist[u] is considered to be the shortest distance from source to u because if there were a shorter path, and if w was the first unvisited node on that path then by the original hypothesis dist[w] > dist[u] which creates a contradiction. Similarly if there were a shorter path to u without using unvisited nodes, and if the last but one node on that path were w, then we would have had dist[u] = dist[w] + Graph.Edges[w,u], also a contradiction.After processing u it will still be true that for each unvisited node w, dist[w] will be the shortest distance from source to w using visited nodes only, because if there were a shorter path that doesn\\'t go by u we would have found it previously, and if there were a shorter path using u we would have updated it when processing u.After all nodes are visited, the shortest path from source to any node v consists only of visited nodes, therefore dist[v] is the shortest distance.Running timeBounds of the running time of Dijkstra\\'s algorithm on a graph with edges E and vertices V can be expressed as a function of the number of edges, denoted|E|{\\\\displaystyle |E|}, and the number of vertices, denoted|V|{\\\\displaystyle |V|}, using big-O notation. The complexity bound depends mainly on the data structure used to represent the set Q. In the following, upper bounds can be simplified because|E|{\\\\displaystyle |E|}isO(|V|2){\\\\displaystyle O(|V|^{2})}for any graph, but that simplification disregards the fact that in some problems, other upper bounds on|E|{\\\\displaystyle |E|}may hold.For any data structure for the vertex set Q, the running time is inΘ(|E|⋅Tdk+|V|⋅Tem),{\\\\displaystyle \\\\Theta (|E|\\\\cdot T_{\\\\mathrm {dk} }+|V|\\\\cdot T_{\\\\mathrm {em} }),}whereTdk{\\\\displaystyle T_{\\\\mathrm {dk} }}andTem{\\\\displaystyle T_{\\\\mathrm {em} }}are the complexities of the decrease-key and extract-minimum operations in Q, respectively.The simplest version of Dijkstra\\'s algorithm stores the vertex set Q as an linked list or array, and edges as an adjacency list or matrix. In this case, extract-minimum is simply a linear search through all vertices in Q, so the running time isΘ(|E|+|V|2)=Θ(|V|2){\\\\displaystyle \\\\Theta (|E|+|V|^{2})=\\\\Theta (|V|^{2})}.For sparse graphs, that is, graphs with far fewer than|V|2{\\\\displaystyle |V|^{2}}edges, Dijkstra\\'s algorithm can be implemented more efficiently by storing the graph in the form of adjacency lists and using a self-balancing binary search tree, binary heap, pairing heap, or Fibonacci heap as a priority queue to implement extracting minimum efficiently. To perform decrease-key steps in a binary heap efficiently, it is necessary to use an auxiliary data structure that maps each vertex to its position in the heap, and to keep this structure up to date as the priority queue Q changes. With a self-balancing binary search tree or binary heap, the algorithm requiresΘ((|E|+|V|)log\\u2061|V|){\\\\displaystyle \\\\Theta ((|E|+|V|)\\\\log |V|)}time in the worst case (wherelog{\\\\displaystyle \\\\log }denotes the binary logarithmlog2{\\\\displaystyle \\\\log _{2}}); for connected graphs this time bound can be simplified toΘ(|E|log\\u2061|V|){\\\\displaystyle \\\\Theta (|E|\\\\log |V|)}.  The Fibonacci heap improves this toΘ(|E|+|V|log\\u2061|V|).{\\\\displaystyle \\\\Theta (|E|+|V|\\\\log |V|).}When using binary heaps, the average case time complexity is lower than the worst-case: assuming edge costs are drawn independently from a common probability distribution, the expected number of decrease-key operations is bounded byΘ(|V|log\\u2061(|E|/|V|)){\\\\displaystyle \\\\Theta (|V|\\\\log(|E|/|V|))}, giving a total running time of:\\u200a199–200O(|E|+|V|log\\u2061|E||V|log\\u2061|V|).{\\\\displaystyle O\\\\left(|E|+|V|\\\\log {\\\\frac {|E|}{|V|}}\\\\log |V|\\\\right).}Practical optimizations and infinite graphsIn common presentations of Dijkstra\\'s algorithm, initially all nodes are entered into the priority queue. This is, however, not necessary: the algorithm can start with a priority queue that contains only one item, and insert new items as they are discovered (instead of doing a decrease-key, check whether the key is in the queue; if it is, decrease its key, otherwise insert it).:\\u200a198\\u200a This variant has the same worst-case bounds as the common variant, but maintains a smaller priority queue in practice, speeding up the queue operations.Moreover, not inserting all nodes in a graph makes it possible to extend the algorithm to find the shortest path from a single source to the closest of a set of target nodes on infinite graphs or those too large to represent in memory. The resulting algorithm is called uniform-cost search (UCS) in the artificial intelligence literature and can be expressed in pseudocode asprocedure uniform_cost_search(start) isnode ← startfrontier ← priority queue containing node onlyexplored ← empty setdoif frontier is empty thenreturn failurenode ← frontier.pop()if node is a goal state thenreturn solution(node)explored.add(node)for each of node\\'s neighbors n doif n is not in explored and not in frontier thenfrontier.add(n)else if n is in frontier with higher costreplace existing node with nThe complexity of this algorithm can be expressed in an alternative way for very large graphs: when C* is the length of the shortest path from the start node to any node satisfying the \"goal\" predicate, each edge has cost at least ε, and the number of neighbors per node is bounded by b, then the algorithm\\'s worst-case time and space complexity are both in O(b1+⌊C* ⁄ ε⌋).Further optimizations of Dijkstra\\'s algorithm for the single-target case include bidirectional variants, goal-directed variants such as the A* algorithm (see § Related problems and algorithms), graph pruning to determine which nodes are likely to form the middle segment of shortest paths (reach-based routing), and hierarchical decompositions of the input graph that reduce s–t routing to connecting s and t to their respective \"transit nodes\" followed by shortest-path computation between these transit nodes using a \"highway\".Combinations of such techniques may be needed for optimal practical performance on specific problems.Specialized variantsWhen arc weights are small integers (bounded by a parameterC{\\\\displaystyle C}), specialized queues which take advantage of this fact can be used to speed up Dijkstra\\'s algorithm. The first algorithm of this type was Dial\\'s algorithm (Dial 1969) for graphs with positive integer edge weights, which uses a bucket queue to obtain a running timeO(|E|+|V|C){\\\\displaystyle O(|E|+|V|C)}. The use of a Van Emde Boas tree as the priority queue brings the complexity toO(|E|log\\u2061log\\u2061C){\\\\displaystyle O(|E|\\\\log \\\\log C)}(Ahuja et al. 1990). Another interesting variant based on a combination of a new radix heap and the well-known Fibonacci heap runs in timeO(|E|+|V|log\\u2061C){\\\\displaystyle O(|E|+|V|{\\\\sqrt {\\\\log C}})}(Ahuja et al. 1990). Finally, the best algorithms in this special case are as follows. The algorithm given by (Thorup 2000) runs inO(|E|log\\u2061log\\u2061|V|){\\\\displaystyle O(|E|\\\\log \\\\log |V|)}time and the algorithm given by (Raman 1997) runs inO(|E|+|V|min{(log\\u2061|V|)1/3+ε,(log\\u2061C)1/4+ε}){\\\\displaystyle O(|E|+|V|\\\\min\\\\{(\\\\log |V|)^{1/3+\\\\varepsilon },(\\\\log C)^{1/4+\\\\varepsilon }\\\\})}time.Related problems and algorithmsThe functionality of Dijkstra\\'s original algorithm can be extended with a variety of modifications. For example, sometimes it is desirable to present solutions which are less than mathematically optimal. To obtain a ranked list of less-than-optimal solutions, the optimal solution is first calculated. A single edge appearing in the optimal solution is removed from the graph, and the optimum solution to this new graph is calculated. Each edge of the original solution is suppressed in turn and a new shortest-path calculated. The secondary solutions are then ranked and presented after the first optimal solution.Dijkstra\\'s algorithm is usually the working principle behind link-state routing protocols, OSPF and IS-IS being the most common ones.Unlike Dijkstra\\'s algorithm, the Bellman–Ford algorithm can be used on graphs with negative edge weights, as long as the graph contains no negative cycle reachable from the source vertex s. The presence of such cycles means there is no shortest path, since the total weight becomes lower each time the cycle is traversed. (This statement assumes that a \"path\" is allowed to repeat vertices. In graph theory that is normally not allowed.  In theoretical computer science it often is allowed.) It is possible to adapt Dijkstra\\'s algorithm to handle negative weight edges by combining it with the Bellman-Ford algorithm (to remove negative edges and detect negative cycles), such an algorithm is called Johnson\\'s algorithm.The A* algorithm is a generalization of Dijkstra\\'s algorithm that cuts down on the size of the subgraph that must be explored, if additional information is available that provides a lower bound on the \"distance\" to the target. This approach can be viewed from the perspective of linear programming: there is a natural linear program for computing shortest paths, and solutions to its dual linear program are feasible if and only if they form a consistent heuristic (speaking roughly, since the sign conventions differ from place to place in the literature). This feasible dual / consistent heuristic defines a non-negative reduced cost and A* is essentially running Dijkstra\\'s algorithm with these reduced costs. If the dual satisfies the weaker condition of admissibility, then A* is instead more akin to the Bellman–Ford algorithm.The process that underlies Dijkstra\\'s algorithm is similar to the greedy process used in Prim\\'s algorithm.  Prim\\'s purpose is to find a minimum spanning tree that connects all nodes in the graph; Dijkstra is concerned with only two nodes. Prim\\'s does not evaluate the total weight of the path from the starting node, only the individual edges.Breadth-first search can be viewed as a special-case of Dijkstra\\'s algorithm on unweighted graphs, where the priority queue degenerates into a FIFO queue.The fast marching method can be viewed as a continuous version of Dijkstra\\'s algorithm which computes the geodesic distance on a triangle mesh.Dynamic programming perspectiveFrom a dynamic programming point of view, Dijkstra\\'s algorithm is a successive approximation scheme that solves the dynamic programming functional equation for the shortest path problem by the Reaching method.In fact, Dijkstra\\'s explanation of the logic behind the algorithm, namelyProblem 2. Find the path of minimum total length between two given nodesP{\\\\displaystyle P}andQ{\\\\displaystyle Q}.We use the fact that, ifR{\\\\displaystyle R}is a node on the minimal path fromP{\\\\displaystyle P}toQ{\\\\displaystyle Q}, knowledge of the latter implies the knowledge of the minimal path fromP{\\\\displaystyle P}toR{\\\\displaystyle R}.is a paraphrasing of Bellman\\'s famous Principle of Optimality in the context of the shortest path problem.ApplicationsLeast-cost paths are calculated for instance to establish tracks of electricity lines or oil pipelines. The algorithm has also been used to calculate optimal long-distance footpaths in Ethiopia and contrast them with the situation on the ground.See alsoA* search algorithmBellman–Ford algorithmEuclidean shortest pathFloyd–Warshall algorithmJohnson\\'s algorithmLongest path problemParallel all-pairs shortest path algorithmNotesReferencesCormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L.; Stein, Clifford (2001). \"Section 24.3: Dijkstra\\'s algorithm\". Introduction to Algorithms (Second ed.). MIT Press and McGraw–Hill. pp. 595–601. ISBN 0-262-03293-7.Dial, Robert B. (1969). \"Algorithm 360: Shortest-path forest with topological ordering [H]\". Communications of the ACM. 12 (11): 632–633. doi:10.1145/363269.363610. S2CID 6754003.Fredman, Michael Lawrence; Tarjan, Robert E. (1984). Fibonacci heaps and their uses in improved network optimization algorithms. 25th Annual Symposium on Foundations of Computer Science. IEEE. pp. 338–346. doi:10.1109/SFCS.1984.715934.Fredman, Michael Lawrence; Tarjan, Robert E. (1987). \"Fibonacci heaps and their uses in improved network optimization algorithms\". Journal of the Association for Computing Machinery. 34 (3): 596–615. doi:10.1145/28869.28874. S2CID 7904683.Zhan, F. Benjamin; Noon, Charles E. (February 1998). \"Shortest Path Algorithms: An Evaluation Using Real Road Networks\". Transportation Science. 32 (1): 65–73. doi:10.1287/trsc.32.1.65. S2CID 14986297.Leyzorek, M.; Gray, R. S.; Johnson, A. A.; Ladew, W. C.; Meaker, Jr., S. R.; Petry, R. M.; Seitz, R. N. (1957). Investigation of Model Techniques – First Annual Report – 6 June 1956 – 1 July 1957 – A Study of Model Techniques for Communication Systems. Cleveland, Ohio: Case Institute of Technology.Knuth, D.E. (1977). \"A Generalization of Dijkstra\\'s Algorithm\". Information Processing Letters. 6 (1): 1–5. doi:10.1016/0020-0190(77)90002-3.Ahuja, Ravindra K.; Mehlhorn, Kurt; Orlin, James B.; Tarjan, Robert E. (April 1990). \"Faster Algorithms for the Shortest Path Problem\" (PDF). Journal of the ACM. 37 (2): 213–223. doi:10.1145/77600.77615. hdl:1721.1/47994. S2CID 5499589.Raman, Rajeev (1997). \"Recent results on the single-source shortest paths problem\". SIGACT News. 28 (2): 81–87. doi:10.1145/261342.261352. S2CID 18031586.Thorup, Mikkel (2000). \"On RAM priority Queues\". SIAM Journal on Computing. 30 (1): 86–109. doi:10.1137/S0097539795288246. S2CID 5221089.Thorup, Mikkel (1999). \"Undirected single-source shortest paths with positive integer weights in linear time\". Journal of the ACM. 46 (3): 362–394. doi:10.1145/316542.316548. S2CID 207654795.External linksOral history interview with Edsger W. Dijkstra, Charles Babbage Institute, University of Minnesota, MinneapolisImplementation of Dijkstra\\'s algorithm using TDD, Robert Cecil Martin, The Clean Code Blog',\n",
              " '= Parsing expression grammar =In computer science, a parsing expression grammar (PEG), is a type of analytic formal grammar, i.e. it describes a formal language in terms of a set of rules for recognizing strings in the language. The formalism was introduced by Bryan Ford in 2004 and is closely related to the family of top-down parsing languages introduced in the early 1970s.Syntactically, PEGs also look similar to context-free grammars (CFGs), but they have a different interpretation: the choice operator selects the first match in PEG, while it is ambiguous in CFG. This is closer to how string recognition tends to be done in practice, e.g. by a recursive descent parser.Unlike CFGs, PEGs cannot be ambiguous; if a string parses, it has exactly one valid parse tree. It is conjectured that there exist context-free languages that cannot be recognized by a PEG, but this is not yet proven. PEGs are well-suited to parsing computer languages (and artificial human languages such as Lojban), but not natural languages where the performance of PEG algorithms is comparable to general CFG algorithms such as the Earley algorithm.DefinitionSyntaxFormally, a parsing expression grammar consists of:A finite set N of nonterminal symbols.A finite set Σ of terminal symbols that is disjoint from N.A finite set P of parsing rules.An expression eS termed the starting expression.Each parsing rule in P has the form A ← e, where A is a nonterminal symbol and e is a parsing expression. A parsing expression is a hierarchical expression similar to a regular expression, which is constructed in the following fashion:An atomic parsing expression consists of:any terminal symbol,any nonterminal symbol, orthe empty string ε.Given any existing parsing expressions e, e1, and e2, a new parsing expression can be constructed using the following operators:Sequence: e1 e2Ordered choice: e1 / e2Zero-or-more: e*One-or-more: e+Optional: e?And-predicate: &eNot-predicate: !eSemanticsThe fundamental difference between context-free grammars and parsing expression grammars is that the PEG\\'s choice operator is ordered. If the first alternative succeeds, the second alternative is ignored. Thus ordered choice is not commutative, unlike unordered choice as in context-free grammars. Ordered choice is analogous to soft cut operators available in some logic programming languages.The consequence is that if a CFG is transliterated directly to a PEG, any ambiguity in the former is resolved by deterministically picking one parse tree from the possible parses. By carefully choosing the order in which the grammar alternatives are specified, a programmer has a great deal of control over which parse tree is selected.Like boolean context-free grammars, parsing expression grammars also add the and- and not- syntactic predicates. Because they can use an arbitrarily complex sub-expression to \"look ahead\" into the input string without actually consuming it, they provide a powerful syntactic lookahead and disambiguation facility, in particular when reordering the alternatives cannot specify the exact parse tree desired.Operational interpretation of parsing expressionsEach nonterminal in a parsing expression grammar essentially represents a parsing function in a recursive descent parser, and the corresponding parsing expression represents the \"code\" comprising the function. Each parsing function conceptually takes an input string as its argument, and yields one of the following results:success, in which the function may optionally move forward or consume one or more characters of the input string supplied to it, orfailure, in which case no input is consumed.An atomic parsing expression consisting of a single terminal (i.e. literal) succeeds if the first character of the input string matches that terminal, and in that case consumes the input character; otherwise the expression yields a failure result. An atomic parsing expression consisting of the empty string always trivially succeeds without consuming any input.An atomic parsing expression consisting of a nonterminal A represents a recursive call to the nonterminal-function A. A nonterminal may succeed without actually consuming any input, and this is considered an outcome distinct from failure.The sequence operator e1 e2 first invokes e1, and if e1 succeeds, subsequently invokes e2 on the remainder of the input string left unconsumed by e1, and returns the result. If either e1 or e2 fails, then the sequence expression e1 e2 fails (consuming no input).The choice operator e1 / e2 first invokes e1, and if e1 succeeds, returns its result immediately. Otherwise, if e1 fails, then the choice operator backtracks to the original input position at which it invoked e1, but then calls e2 instead, returning e2\\'s result.The zero-or-more, one-or-more, and optional operators consume zero or more, one or more, or zero or one consecutive repetitions of their sub-expression e, respectively. Unlike in context-free grammars and regular expressions, however, these operators always behave greedily, consuming as much input as possible and never backtracking. (Regular expression matchers may start by matching greedily, but will then backtrack and try shorter matches if they fail to match.) For example, the expression a* will always consume as many a\\'s as are consecutively available in the input string, and the expression (a* a) will always fail because the first part (a*) will never leave any a\\'s for the second part to match.The and-predicate expression &e invokes the sub-expression e, and then succeeds if e succeeds and fails if e fails, but in either case never consumes any input.The not-predicate expression !e succeeds if e fails and fails if e succeeds, again consuming no input in either case.ExamplesThis is a PEG that recognizes mathematical formulas that apply the basic five operations to non-negative integers.In the above example, the terminal symbols are characters of text, represented by characters in single quotes, such as \\'(\\' and \\')\\'. The range [0-9] is also a shortcut for ten characters, indicating any one of the digits 0 through 9. (This range syntax is the same as the syntax used by regular expressions.) The nonterminal symbols are the ones that expand to other rules: Value, Power, Product, Sum, and Expr. Note that rules Sum and Product don\\'t lead to desired left-associativity of these operations (they don\\'t deal with associativity at all, and it has to be handled in post-processing step after parsing), and the Power rule (by referring to itself on the right) results in desired right-associativity of exponent. Also note that a rule like Sum ← Sum ((\\'+\\' / \\'-\\') Product)? (with intention to achieve left-associativity) would cause infinite recursion, so it cannot be used in practice even though it can be expressed in the grammar.The following recursive rule matches standard C-style if/then/else statements in such a way that the optional \"else\" clause always binds to the innermost \"if\", because of the implicit prioritization of the \\'/\\' operator. (In a context-free grammar, this construct yields the classic dangling else ambiguity.)The following recursive rule matches Pascal-style nested comment syntax, (* which can (* nest *) like this *). The comment symbols appear in single quotes to distinguish them from PEG operators.The parsing expression foo &(bar) matches and consumes the text \"foo\" but only if it is followed by the text \"bar\". The parsing expression foo !(bar) matches the text \"foo\" but only if it is not followed by the text \"bar\".  The expression !(a+ b) a matches a single \"a\" but only if it is not part of an arbitrarily long sequence of a\\'s followed by a b.The parsing expression (\\'a\\'/\\'b\\')* matches and consumes an arbitrary-length sequence of a\\'s and b\\'s. The production rule S ← \\'a\\' \\'\\'S\\'\\'? \\'b\\' describes the simple context-free \"matching language\"{anbn:n≥1}{\\\\displaystyle \\\\{a^{n}b^{n}:n\\\\geq 1\\\\}}.The following parsing expression grammar describes the classic non-context-free language{anbncn:n≥1}{\\\\displaystyle \\\\{a^{n}b^{n}c^{n}:n\\\\geq 1\\\\}}:Implementing parsers from parsing expression grammarsAny parsing expression grammar can be converted directly into a recursive descent parser. Due to the unlimited lookahead capability that the grammar formalism provides, however, the resulting parser could exhibit exponential time performance in the worst case.It is possible to obtain better performance for any parsing expression grammar by converting its recursive descent parser into a packrat parser, which always runs in linear time, at the cost of substantially greater storage space requirements. A packrat parseris a form of parser similar to a recursive descent parser in construction, except that during the parsing process it memoizes the intermediate results of all invocations of the mutually recursive parsing functions, ensuring that each parsing function is only invoked at most once at a given input position. Because of this memoization, a packrat parser has the ability to parse many context-free grammars and any parsing expression grammar (including some that do not represent context-free languages) in linear time. Examples of memoized recursive descent parsers are known from at least as early as 1993.This analysis of the performance of a packrat parser assumes that enough memory is available to hold all of the memoized results; in practice, if there is not enough memory, some parsing functions might have to be invoked more than once at the same input position, and consequently the parser could take more than linear time.It is also possible to build LL parsers and LR parsers from parsing expression grammars, with better worst-case performance than a recursive descent parser, but the unlimited lookahead capability of the grammar formalism is then lost.  Therefore, not all languages that can be expressed using parsing expression grammars can be parsed by LL or LR parsers.AdvantagesCompared to pure regular expressions (i.e. without back-references), PEGs are strictly more powerful, but require significantly more memory. For example, a regular expression inherently cannot find an arbitrary number of matched pairs of parentheses, because it is not recursive, but a PEG can.  However, a PEG will require an amount of memory proportional to the length of the input, while a regular expression matcher will require only a constant amount of memory.Any PEG can be parsed in linear time by using a packrat parser, as described above.Many CFGs contain ambiguities, even when they\\'re intended to describe unambiguous languages. The \"dangling else\" problem in C, C++, and Java is one example. These problems are often resolved by applying a rule outside of the grammar. In a PEG, these ambiguities never arise, because of prioritization.DisadvantagesMemory consumptionPEG parsing is typically carried out via packrat parsing, which uses memoization to eliminate redundant parsing steps. Packrat parsing requires storage proportional to the total input size, rather than the depth of the parse tree as with LR parsers. This is a significant difference in many domains: for example, hand-written source code has an effectively constant expression nesting depth independent of the length of the program—expressions nested beyond a certain depth tend to get refactored.For some grammars and some inputs, the depth of the parse tree can be proportional to the input size,so both an LR parser and a packrat parser will appear to have the same worst-case asymptotic performance. A more accurate analysis would take the depth of the parse tree into account separately from the input size. This is similar to a situation which arises in graph algorithms: the Bellman–Ford algorithm and Floyd–Warshall algorithm appear to have the same running time (O(|V|3){\\\\displaystyle O(|V|^{3})}) if only the number of vertices is considered. However, a more precise analysis which accounts for the number of edges as a separate parameter assigns the Bellman–Ford algorithm a time ofO(|V|∗|E|){\\\\displaystyle O(|V|*|E|)}, which is quadratic for sparse graphs with|E|∈O(|V|){\\\\displaystyle |E|\\\\in O(|V|)}.Indirect left recursionA PEG is called well-formed if it contains no left-recursive rules, i.e., rules that allow a nonterminal to expand to an expression in which the same nonterminal occurs as the leftmost symbol. For a left-to-right top-down parser, such rules cause infinite regress: parsing will continually expand the same nonterminal without moving forward in the string.Therefore, to allow packrat parsing, left recursion must be eliminated. For example, in the arithmetic grammar above, it would be tempting to move some rules around so that the precedence order of products and sums could be expressed in one line:In this new grammar, matching an Expr requires testing if a Product matches while matching a Product requires testing if an Expr matches. Because the term appears in the leftmost position, these rules make up a circular definition that cannot be resolved.  (Circular definitions that can be resolved exist—such as in the original formulation from the first example—but such definitions are required not to exhibit pathological recursion.)  However, left-recursive rules can always be rewritten to eliminate left-recursion. For example, the following left-recursive CFG rule:can be rewritten in a PEG using the plus operator:The process of rewriting indirectly left-recursive rules is complex in some packrat parsers, especially when semantic actions are involved.With some modification, traditional packrat parsing can support direct left recursion, but doing so results in a loss of the linear-time parsing property which is generally the justification for using PEGs and packrat parsing in the first place.  Only the OMeta parsing algorithm supports full direct and indirect left recursion without additional attendant complexity (but again, at a loss of the linear time complexity), whereas all GLR parsers support left recursion.Expressive powerPEG packrat parsers cannot recognize some unambiguous nondeterministic CFG rules, such as the following:Neither LL(k) nor LR(k) parsing algorithms are capable of recognizing this example. However, this grammar can be used by a general CFG parser like the CYK algorithm. However, the language in question can be recognised by all these types of parser, since it is in fact a regular language (that of strings of an odd number of x\\'s).It is an open problem to give a concrete example of a context-free language which cannot be recognized by a parsing expression grammar.Ambiguity detection and influence of rule order on language that is matchedLL(k) and LR(k) parser generators will fail to complete when the input grammar is ambiguous. This is a feature in the common case that the grammar is intended to be unambiguous but is defective. A PEG parser generator will resolve unintended ambiguities earliest-match-first, which may be arbitrary and lead to surprising parses.The ordering of productions in a PEG grammar affects not only the resolution of ambiguity, but also the language matched. For example, consider the first PEG example in Ford\\'s paper(example rewritten in pegjs.org/online notation, and labelledG1{\\\\displaystyle G_{1}}andG2{\\\\displaystyle G_{2}}):G1{\\\\displaystyle G_{1}}:  A = \"a\" \"b\" / \"a\"G2{\\\\displaystyle G_{2}}:  A = \"a\" / \"a\" \"b\"Ford notes that The second alternative in the latter PEG rule will never succeed because the first choice is always taken if the input string ... begins with \\'a\\'..Specifically,L(G1){\\\\displaystyle L(G_{1})}(i.e., the language matched byG1{\\\\displaystyle G_{1}}) includes the input \"ab\", butL(G2){\\\\displaystyle L(G_{2})}does not.Thus, adding a new option to a PEG grammar can remove strings from the language matched, e.g.G2{\\\\displaystyle G_{2}}is the addition of a rule to the single-production grammar A = \"a\" \"b\", which contains a string not matched byG2{\\\\displaystyle G_{2}}.Furthermore, constructing a grammar to matchL(G1)∪L(G2){\\\\displaystyle L(G_{1})\\\\cup L(G_{2})}from PEG grammarsG1{\\\\displaystyle G_{1}}andG2{\\\\displaystyle G_{2}}is not always a trivial task.This is in stark contrast to CFG\\'s, in which the addition of a new production cannot remove strings (though, it can introduce problems in the form of ambiguity),and a (potentially ambiguous) grammar forL(G1)∪L(G2){\\\\displaystyle L(G_{1})\\\\cup L(G_{2})}can be constructedBottom-up PEG parsingA pika parser uses dynamic programming to apply PEG rules bottom-up and right to left, which is the inverse of the normal recursive descent order of top-down, left to right. Parsing in reverse order solves the left recursion problem, allowing left-recursive rules to be used directly in the grammar without being rewritten into non-left-recursive form, and also conveys optimal error recovery capabilities upon the parser, which historically proved difficult to achieve for recursive descent parsers.See alsoCompiler Description Language (CDL)Formal grammarRegular expressionTop-down parsing languageComparison of parser generatorsParser combinatorPythonReferencesExternal linksConverting a string expression into a lambda expression using an expression parserThe Packrat Parsing and Parsing Expression Grammars PageThe constructed language Lojban has a fairly large PEG grammar allowing unambiguous parsing of Lojban text.An illustrative implementation of a PEG in Guile scheme',\n",
              " '= Yen\\'s algorithm =Yen\\'s algorithm computes single-source K-shortest loopless paths for a graph with non-negative edge cost.  The algorithm was published by Jin Y. Yen in 1971 and employs any shortest path algorithm to find the best path, then proceeds to find K − 1 deviations of the best path.AlgorithmTerminology and notationDescriptionThe algorithm can be broken down into two parts, determining the first k-shortest path,A1{\\\\displaystyle A^{1}}, and then determining all other k-shortest paths. It is assumed that the containerA{\\\\displaystyle A}will hold the k-shortest path, whereas the containerB{\\\\displaystyle B}, will hold the potential k-shortest paths. To determineA1{\\\\displaystyle A^{1}}, the shortest path from the source to the sink, any efficient shortest path algorithm can be used.To find theAk{\\\\displaystyle A^{k}}, wherek{\\\\displaystyle k}ranges from2{\\\\displaystyle 2}toK{\\\\displaystyle K}, the algorithm assumes that all paths fromA1{\\\\displaystyle A^{1}}toAk−1{\\\\displaystyle A^{k-1}}have previously been found. Thek{\\\\displaystyle k}iteration can be divided into two processes, finding all the deviationsAki{\\\\displaystyle {A^{k}}_{i}}and choosing a minimum length path to becomeAk{\\\\displaystyle A^{k}}. Note that in this iteration,i{\\\\displaystyle i}ranges from1{\\\\displaystyle 1}toQkk{\\\\displaystyle {Q^{k}}_{k}}.The first process can be further subdivided into three operations, choosing theRki{\\\\displaystyle {R^{k}}_{i}}, findingSki{\\\\displaystyle {S^{k}}_{i}}, and then addingAki{\\\\displaystyle {A^{k}}_{i}}to the containerB{\\\\displaystyle B}. The root path,Rki{\\\\displaystyle {R^{k}}_{i}}, is chosen by finding the subpath inAk−1{\\\\displaystyle A^{k-1}}that follows the firsti{\\\\displaystyle i}nodes ofAj{\\\\displaystyle A^{j}}, wherej{\\\\displaystyle j}ranges from1{\\\\displaystyle 1}tok−1{\\\\displaystyle k-1}. Then, if a path is found, the cost of edgedi(i+1){\\\\displaystyle d_{i(i+1)}}ofAj{\\\\displaystyle A^{j}}is set to infinity. Next, the spur path,Ski{\\\\displaystyle {S^{k}}_{i}}, is found by computing the shortest path from the spur node, nodei{\\\\displaystyle i}, to the sink. The removal of previous used edges from(i){\\\\displaystyle (i)}to(i+1){\\\\displaystyle (i+1)}ensures that the spur path is different.Aki=Rki+Ski{\\\\displaystyle {A^{k}}_{i}={R^{k}}_{i}+{S^{k}}_{i}}, the addition of the root path and the spur path, is added toB{\\\\displaystyle B}. Next, the edges that were removed, i.e. had their cost set to infinity, are restored to their initial values.The second process determines a suitable path forAk{\\\\displaystyle A^{k}}by finding the path in containerB{\\\\displaystyle B}with the lowest cost. This path is removed from containerB{\\\\displaystyle B}and inserted into containerA{\\\\displaystyle A}and the algorithm continues to the next iteration.PseudocodeThe algorithm assumes that the Dijkstra algorithm is used to find the shortest path between two nodes, but any shortest path algorithm can be used in its place.function YenKSP(Graph, source, sink, K):// Determine the shortest path from the source to the sink.A[0] = Dijkstra(Graph, source, sink);// Initialize the set to store the potential kth shortest path.B = [];for k from 1 to K:// The spur node ranges from the first node to the next to last node in the previous k-shortest path.for i from 0 to size(A[k − 1]) − 2:// Spur node is retrieved from the previous k-shortest path, k − 1.spurNode = A[k-1].node(i);// The sequence of nodes from the source to the spur node of the previous k-shortest path.rootPath = A[k-1].nodes(0, i);for each path p in A:if rootPath == p.nodes(0, i):// Remove the links that are part of the previous shortest paths which share the same root path.remove p.edge(i,i + 1) from Graph;for each node rootPathNode in rootPath except spurNode:remove rootPathNode from Graph;// Calculate the spur path from the spur node to the sink.// Consider also checking if any spurPath foundspurPath = Dijkstra(Graph, spurNode, sink);// Entire path is made up of the root path and spur path.totalPath = rootPath + spurPath;// Add the potential k-shortest path to the heap.if (totalPath not in B):B.append(totalPath);// Add back the edges and nodes that were removed from the graph.restore edges to Graph;restore nodes in rootPath to Graph;if B is empty:// This handles the case of there being no spur paths, or no spur paths left.// This could happen if the spur paths have already been exhausted (added to A),// or there are no spur paths at all - such as when both the source and sink vertices// lie along a \"dead end\".break;// Sort the potential k-shortest paths by cost.B.sort();// Add the lowest cost path becomes the k-shortest path.A[k] = B[0];// In fact we should rather use shift since we are removing the first elementB.pop();return A;ExampleThe example uses Yen\\'s K-Shortest Path Algorithm to compute three paths from(C){\\\\displaystyle (C)}to(H){\\\\displaystyle (H)}. Dijkstra\\'s algorithm is used to calculate the best path from(C){\\\\displaystyle (C)}to(H){\\\\displaystyle (H)}, which is(C)−(E)−(F)−(H){\\\\displaystyle (C)-(E)-(F)-(H)}with cost 5. This path is appended to containerA{\\\\displaystyle A}and becomes the first k-shortest path,A1{\\\\displaystyle A^{1}}.Node(C){\\\\displaystyle (C)}ofA1{\\\\displaystyle A^{1}}becomes the spur node with a root path of itself,R21=(C){\\\\displaystyle {R^{2}}_{1}=(C)}. The edge,(C)−(E){\\\\displaystyle (C)-(E)}, is removed because it coincides with the root path and a path in containerA{\\\\displaystyle A}. Dijkstra\\'s algorithm is used to compute the spur pathS21{\\\\displaystyle {S^{2}}_{1}}, which is(C)−(D)−(F)−(H){\\\\displaystyle (C)-(D)-(F)-(H)}, with a cost of 8.A21=R21+S21=(C)−(D)−(F)−(H){\\\\displaystyle {A^{2}}_{1}={R^{2}}_{1}+{S^{2}}_{1}=(C)-(D)-(F)-(H)}is added to containerB{\\\\displaystyle B}as a potential k-shortest path.Node(E){\\\\displaystyle (E)}ofA1{\\\\displaystyle A^{1}}becomes the spur node withR22=(C)−(E){\\\\displaystyle {R^{2}}_{2}=(C)-(E)}. The edge,(E)−(F){\\\\displaystyle (E)-(F)}, is removed because it coincides with the root path and a path in containerA{\\\\displaystyle A}. Dijkstra\\'s algorithm is used to compute the spur pathS22{\\\\displaystyle {S^{2}}_{2}}, which is(E)−(G)−(H){\\\\displaystyle (E)-(G)-(H)}, with a cost of 7.A22=R22+S22=(C)−(E)−(G)−(H){\\\\displaystyle {A^{2}}_{2}={R^{2}}_{2}+{S^{2}}_{2}=(C)-(E)-(G)-(H)}is added to containerB{\\\\displaystyle B}as a potential k-shortest path.Node(F){\\\\displaystyle (F)}ofA1{\\\\displaystyle A^{1}}becomes the spur node with a root path,R23=(C)−(E)−(F){\\\\displaystyle {R^{2}}_{3}=(C)-(E)-(F)}. The edge,(F)−(H){\\\\displaystyle (F)-(H)}, is removed because it coincides with the root path and a path in containerA{\\\\displaystyle A}. Dijkstra\\'s algorithm is used to compute the spur pathS23{\\\\displaystyle {S^{2}}_{3}}, which is(F)−(G)−(H){\\\\displaystyle (F)-(G)-(H)}, with a cost of 8.A23=R23+S23=(C)−(E)−(F)−(G)−(H){\\\\displaystyle {A^{2}}_{3}={R^{2}}_{3}+{S^{2}}_{3}=(C)-(E)-(F)-(G)-(H)}is added to containerB{\\\\displaystyle B}as a potential k-shortest path.Of the three paths in container B,A22{\\\\displaystyle {A^{2}}_{2}}is chosen to becomeA2{\\\\displaystyle A^{2}}because it has the lowest cost of 7. This process is continued to the 3rd k-shortest path. However, within this 3rd iteration, note that some spur paths do not exist. And the path that is chosen to becomeA3{\\\\displaystyle A^{3}}is(C)−(D)−(F)−(H){\\\\displaystyle (C)-(D)-(F)-(H)}.FeaturesSpace complexityTo store the edges of the graph, the shortest path listA{\\\\displaystyle A}, and the potential shortest path listB{\\\\displaystyle B},N2+KN{\\\\displaystyle N^{2}+KN}memory addresses are required. At worse case, the every node in the graph has an edge to every other node in the graph, thusN2{\\\\displaystyle N^{2}}addresses are needed. OnlyKN{\\\\displaystyle KN}addresses are need for both listA{\\\\displaystyle A}andB{\\\\displaystyle B}because at most onlyK{\\\\displaystyle K}paths will be stored, where it is possible for each path to haveN{\\\\displaystyle N}nodes.Time complexityThe time complexity of Yen\\'s algorithm is dependent on the shortest path algorithm used in the computation of the spur paths, so the Dijkstra algorithm is assumed. Dijkstra\\'s algorithm has a worse case time complexity ofO(N2){\\\\displaystyle O(N^{2})}, but using a Fibonacci heap it becomesO(M+Nlog\\u2061N){\\\\displaystyle O(M+N\\\\log N)}, whereM{\\\\displaystyle M}is the amount of edges in the graph. Since Yen\\'s algorithm makesKl{\\\\displaystyle Kl}calls to the Dijkstra in computing the spur paths, wherel{\\\\displaystyle l}is the length of spur paths. In a condensed graph, the expected value ofl{\\\\displaystyle l}isO(log\\u2061N){\\\\displaystyle O(\\\\log N)}, while the worst case isN{\\\\displaystyle N}., the time complexity becomesO(KN(M+Nlog\\u2061N)){\\\\displaystyle O(KN(M+N\\\\log N))}.ImprovementsYen\\'s algorithm can be improved by using a heap to storeB{\\\\displaystyle B}, the set of potential k-shortest paths. Using a heap instead of a list will improve the performance of the algorithm, but not the complexity.  One method to slightly decrease complexity is to skip the nodes where there are non-existent spur paths. This case is produced when all the spur paths from a spur node have been used in the previousAk{\\\\displaystyle A^{k}}. Also, if containerB{\\\\displaystyle B}hasK−k{\\\\displaystyle K-k}paths of minimum length, in reference to those in containerA{\\\\displaystyle A}, then they can be extract and inserted into containerA{\\\\displaystyle A}since no shorter paths will be found.Lawler\\'s modificationEugene Lawler proposed a modification to Yen\\'s algorithm in which duplicates path are not calculated as opposed to the original algorithm where they are calculated and then discarded when they are found to be duplicates. These duplicates paths result from calculating spur paths of nodes in the root ofAk{\\\\displaystyle A^{k}}. For instance,Ak{\\\\displaystyle A^{k}}deviates fromAk−1{\\\\displaystyle A^{k-1}}at some node(i){\\\\displaystyle (i)}. Any spur path,Skj{\\\\displaystyle {S^{k}}_{j}}wherej=0,…,i{\\\\displaystyle j=0,\\\\ldots ,i}, that is calculated will be a duplicate because they have already been calculated during thek−1{\\\\displaystyle k-1}iteration. Therefore, only spur paths for nodes that were on the spur path ofAk−1{\\\\displaystyle A^{k-1}}must be calculated, i.e. onlySkh{\\\\displaystyle {S^{k}}_{h}}whereh{\\\\displaystyle h}ranges from(i+1)k−1{\\\\displaystyle (i+1)^{k-1}}to(Qk)k−1{\\\\displaystyle (Q_{k})^{k-1}}. To perform this operation forAk{\\\\displaystyle A^{k}}, a record is needed to identify the node whereAk−1{\\\\displaystyle A^{k-1}}branched fromAk−2{\\\\displaystyle A^{k-2}}.See alsoYen\\'s improvement to the Bellman–Ford algorithmReferencesExternal linksOpen Source Python Implementation on GitHubOpen Source C++ ImplementationOpen Source C++ Implementation using Boost Graph Library',\n",
              " '= Tamam Shud case =The Tamám Shud case, also known as the Mystery of the Somerton Man, is an unsolved case of an unidentified man found dead in 1948 on the Somerton Park beach, just south of Adelaide, South Australia, Australia. The case is named after the Persian phrase tamám shud, meaning \"is over\" or \"is finished\", which was printed on a scrap of paper found months later in the fob pocket of the man\\'s trousers. The scrap had been torn from the final page of a copy of Rubaiyat of Omar Khayyám, authored by 12th century poet Omar Khayyám. Tamám was misspelt as Tamán in many early reports, and this error has often been repeated, leading to confusion about the name in the media.Following a public appeal by police, the book from which the page had been torn was located. On the inside back cover, detectives read through indentations left from previous handwriting – a local telephone number, another unidentified number, and text that resembled an encrypted message. The text has not been deciphered or interpreted in a way that satisfies authorities on the case.The case has been considered, since the early stages of the police investigation, \"one of Australia\\'s most profound mysteries\". There has been intense speculation ever since regarding the identity of the victim, the cause of his death, and the events leading up to it. Public interest in the case remains significant for several reasons: the death occurred at a time of heightened international tensions following the beginning of the Cold War; the apparent involvement of a secret code; the possible use of an undetectable poison; and the inability of authorities to identify the dead man.In addition to intense public interest in Australia during the late 1940s and early 1950s, the case also attracted international attention. South Australian Police consulted their counterparts overseas and distributed information about the dead man internationally, in an effort to identify him. International circulation of a photograph of the man and details of his fingerprints yielded no positive identification. For example, in the United States, the Federal Bureau of Investigation was unable to match the dead man\\'s fingerprint with prints taken from files of domestic criminals. Scotland Yard was also asked to assist with the case, but could not offer any insights.In recent years, additional evidence has emerged, including an old identification card possibly identifying the Somerton Man as one H. C. Reynolds, and an ongoing DNA analysis of hair roots found on the plaster bust. On 19 May 2021, after a series of requests, the body was exhumed for analysis. Police stated that the remains were in \"reasonable\" condition and were optimistic about the prospect of DNA recovery.Discovery of bodyOn 1 December 1948 at 6:30 am, the police were contacted after the body of a man was discovered on Somerton Park beach near Glenelg, about 11 km (7 mi) southwest of Adelaide, South Australia. The man was found lying in the sand across from the Crippled Children\\'s Home, which was on the corner of The Esplanade and Bickford Terrace. He was lying back with his head resting against the seawall, with his legs extended and his feet crossed. It was believed the man had died while sleeping. An unlit cigarette was on the right collar of his coat. A search of his pockets revealed an unused second-class rail ticket from Adelaide to Henley Beach, a bus ticket from the city that may not have been used, a narrow aluminium comb that had been manufactured in the USA, a half-empty packet of Juicy Fruit chewing gum, an Army Club cigarette packet which contained seven cigarettes of a different brand, Kensitas, and a quarter-full box of Bryant & May matches.Witnesses who came forward said that on the evening of 30 November, they had seen an individual resembling the dead man lying on his back in the same spot and position near the Crippled Children\\'s Home where the corpse was later found. A couple who saw him at around 7 pm noted that they saw him extend his right arm to its fullest extent and then drop it limply. Another couple who saw him from 7:30 pm to 8 pm, during which time the street lights had come on, recounted that they did not see him move during the half an hour in which he was in view, although they did have the impression that his position had changed. Although they commented between themselves that it was odd that he was not reacting to the mosquitoes, they had thought it more likely that he was drunk or asleep, and thus did not investigate further. One of the witnesses told the police she observed a man looking down at the sleeping man from the top of the steps that led to the beach. Witnesses said the body was in the same position when the police viewed it.Another witness came forward in 1959 and reported to the police that he and three others had seen a well-dressed man carrying another man on his shoulders along Somerton Park beach the night before the body was found. A police report was made by Detective Don O\\'Doherty.According to the pathologist, John Burton Cleland, the man was of \"Britisher\" appearance and thought to be aged about 40–45; he was in \"top physical condition\". He was:180 centimetres (5 ft 11 in) tall, with grey eyes, fair to ginger-coloured hair, slightly grey around the temples, with broad shoulders and a narrow waist, hands and nails that showed no signs of manual labour, big and little toes that met in a wedge shape, like those of a dancer or someone who wore boots with pointed toes; and pronounced high calf muscles consistent with people who regularly wore boots or shoes with high heels or performed ballet.He was dressed in a white shirt; a red, white and blue tie; brown trousers; socks and shoes; a brown knitted pullover and fashionable grey and brown double-breasted jacket of reportedly \"American\" tailoring. All labels on his clothes had been removed, and he had no hat (unusual for 1948) or wallet. He was clean-shaven and carried no identification, which led police to believe he had committed suicide. Finally, his dental records were not able to be matched to any known person.An autopsy was conducted, and the pathologist estimated the time of death at around 2 am on 1 December. The heart was of normal size, and normal in every way ...small vessels not commonly observed in the brain were easily discernible with congestion. There was congestion of the pharynx, and the gullet was covered with whitening of superficial layers of the mucosa with a patch of ulceration in the middle of it. The stomach was deeply congested... There was congestion in the second half of the duodenum. There was blood mixed with the food in the stomach. Both kidneys were congested, and the liver contained a great excess of blood in its vessels. ...The spleen was strikingly large ... about 3 times normal size ... there was destruction of the centre of the liver lobules revealed under the microscope. ... acute gastritis hemorrhage, extensive congestion of the liver and spleen, and the congestion to the brain.The autopsy also showed that the man\\'s last meal was a pasty eaten three to four hours before death, but tests failed to reveal any foreign substance in the body. The pathologist, Dr. Dwyer, concluded: \"I am quite convinced the death could not have been natural ... the poison I suggested was a barbiturate or a soluble hypnotic\". Although poisoning remained a prime suspicion, the pasty was not believed to be the source. Other than that, the coroner was unable to reach a conclusion as to the man\\'s identity, cause of death, or whether the man seen alive at Somerton Beach on the evening of 30 November was the same man, as nobody had seen his face at that time. The body was then embalmed on 10 December 1948 after the police were unable to get a positive identification. The police said this was the first time they knew that such action was needed.Discovery of suitcaseOn 14 January 1949, staff at the Adelaide railway station discovered a brown suitcase with its label removed, which had been checked into the station cloakroom after 11:00 am on 30 November 1948. It was believed that the suitcase was owned by the man found on the beach. In the case were a red checked dressing gown; a size-seven, red felt pair of slippers; four pairs of underpants; pyjamas; shaving items; a light brown pair of trousers with sand in the cuffs; an electrician\\'s screwdriver; a table knife cut down into a short sharp instrument; a pair of scissors with sharpened points; a small square of zinc thought to have been used as a protective sheath for the knife and scissors; and a stencilling brush, as used by third officers on merchant ships for stencilling cargo.Also in the suitcase was a thread card of Barbour brand orange waxed thread of \"an unusual type\" not available in Australia—it was the same as that used to repair the lining in a pocket of the trousers the dead man was wearing. All identification marks on the clothes had been removed but police found the name \"T. Keane\" on a tie, \"Keane\" on a laundry bag and \"Kean\" on a singlet, along with three dry-cleaning marks; 1171/7, 4393/7 and 3053/7. Police believed that whoever removed the clothing tags either overlooked these three items or purposely left the \"Keane\" tags on the clothes, knowing Keane was not the dead man\\'s name. With wartime rationing still enforced, clothing was difficult to acquire at that time. Although it was a very common practice to use name tags, it was also common when buying secondhand clothing to remove the tags of the previous owners. What was unusual was that there were no spare socks found in the case, and no correspondence, although the police found pencils and unused letter stationery.A search concluded that no T. Keane was missing in any English-speaking country. A nationwide circulation of the dry-cleaning marks also proved fruitless. All that could be garnered from the suitcase was that the front gusset and featherstitching on a coat found in the case indicated it had been manufactured in the United States. The coat had not been imported, indicating the man had been to the United States or bought the coat from someone of similar size who had been.Police checked incoming train records and believed the man had arrived at the Adelaide railway station by overnight train from either Melbourne, Sydney or Port Augusta. They speculated he had showered and shaved at the adjacent City Baths (there was no Baths ticket on his body) before returning to the train station to purchase a ticket for the 10:50 a.m. train to Henley Beach, which, for whatever reason, he missed or did not catch. He immediately checked his suitcase at the station cloak room before leaving the station and catching a city bus to Glenelg. Although named \"City Baths\", the centre was not a public bathing facility, but rather a public swimming pool. The railway station bathing facilities were adjacent to the station cloak room, which itself was adjacent to the station\\'s southern exit onto North Terrace. The City Baths on King William St. were accessed from the station\\'s northern exit via a lane way. There is no record of the station\\'s bathroom facilities being unavailable on the day he arrived.InquestAn inquest into the man\\'s death conducted by coroner Thomas Erskine Cleland, commenced a few days following the discovery of the body, but was adjourned until 17 June 1949. Cleland, as the investigating pathologist, re-examined the body and made a number of discoveries. He noted that the man\\'s shoes were remarkably clean and appeared to have been recently polished, rather than in the condition expected of a man who had apparently been wandering around Glenelg all day. He added that this evidence fit in with the theory that the body may have been brought to Somerton Park beach after the man\\'s death, accounting for the lack of evidence of vomiting and convulsions, which are the two main physiological reactions to poison.Cleland speculated that, as none of the witnesses could positively identify the man they saw the previous night as the same person discovered the next morning, there remained the possibility the man had died elsewhere and had been dumped. He stressed that this was purely speculation as all the witnesses believed it was, \"definitely the same person\", as the body was in the same place and lying in the same distinctive position. He also found no evidence indicating the identity of the deceased.Cedric Stanton Hicks, professor of physiology and pharmacology at the University of Adelaide, testified that of a group of drugs, variants of a drug in that group he called \"number 1\" and in particular \"number 2\" were extremely toxic in a relatively small oral dose that would be extremely difficult if not impossible to identify even if it had been suspected in the first instance. He gave Cleland a piece of paper with the names of the two drugs which was entered as Exhibit C.18. The names were not released to the public until the 1980s as at the time they were \"quite easily procurable by the ordinary individual\" from a chemist without the need to give a reason for the purchase. (The drugs were later publicly identified as digitalis and ouabain, both cardenolide-type cardiac glycosides.) Hicks noted the only \"fact\" not found in relation to the body was evidence of vomiting. He then stated its absence was not unknown but that he could not make a \"frank conclusion\" without it. Hicks stated that if death had occurred seven hours after the man was last seen to move, it would imply a massive dose that could still have been undetectable. It was noted that the movement seen by witnesses at 7 p.m. could have been the last convulsion preceding death.Early in the inquiry, Cleland stated, \"I would be prepared to find that he died from poison, that the poison was probably a glucoside and that it was not accidentally administered; but I cannot say whether it was administered by the deceased himself or by some other person.\" Despite these findings, he could not determine the cause of death of the unidentified man. Cleland remarked that if the body had been carried to its final resting place then \"all the difficulties would disappear\".After the inquest, a plaster cast was made of the man\\'s head and shoulders. The lack of success in determining the identity and cause of death of the man had led authorities to call it an \"unparalleled mystery\" and believe that the cause of death might never be known.Connection to Rubaiyat of Omar KhayyamAround the same time as the inquest, a tiny piece of rolled-up paper with the words \"Tamám Shud\" printed on it was found in a fob pocket sewn within the dead man\\'s trouser pocket. Public library officials called in to translate the text identified it as a phrase meaning \"ended\" or \"finished\" found on the last page of Rubaiyat of Omar Khayyam. The paper\\'s verso side was blank. Police conducted an Australia-wide search to find a copy of the book that had a similarly blank verso. A photograph of the scrap of paper was released to the press.Following a public appeal by police, the copy of Rubaiyat from which the page had been torn was located. A man showed police a 1941 edition of Edward FitzGerald\\'s (1859) translation of Rubaiyat, published by Whitcombe and Tombs in Christchurch, New Zealand. Detective Sergeant Lionel Leane, who led the initial investigation, often protected the privacy of witnesses in public statements by using pseudonyms; Leane referred to the man who found the book by the pseudonym \"Ronald Francis\" and he has never been officially identified. \"Francis\" had not considered that the book might be connected to the case until he had seen an article in the previous day\\'s newspaper.There is some uncertainty about the circumstances under which the book was found. One newspaper article refers to the book being found about a week or two before the body was found. Former South Australian Police detective Gerry Feltus (who dealt with the matter as a cold case) reports that the book was found \"just after that man was found on the beach at Somerton\". The timing is significant as the man is presumed, based on the suitcase, to have arrived in Adelaide the day before he was found on the beach. If the book was found one or two weeks before, it suggests that the man had visited previously or had been in Adelaide for a longer period. Most accounts state that the book was found in an unlocked car parked in Jetty Road, Glenelg - either in the rear floor well, or on the back seat.The theme of Rubaiyat is that one should live life to the fullest and have no regrets when it ends. The poem\\'s subject led police to theorise that the man had committed suicide by poison, although no other evidence corroborated the theory. The book was missing the words \"Tamám Shud\" on the last page, which had a blank reverse, and microscopic tests indicated that the piece of paper was from the page torn from the book. Also, in the back of the book were faint indentations representing five lines of text, in capital letters. The second line has been struck out – a fact considered significant due to its similarities to the fourth line and the possibility that it represents an error in encryption.In the book it is unclear whether the first line begins with an \"M\" or \"W\", but it is widely believed to be the letter W, owing to the distinctive difference when compared to the stricken letter M. There appears to be a deleted or underlined line of text that reads \"MLIAOI\". Although the last character in this line of text looks like an \"L\", it is fairly clear on closer inspection of the image that this is formed from an \"I\" and the extension of the line used to delete or underline that line of text. Also, the other \"L\" has a curve to the bottom part of the character. There is also an \"X\" above the last \"O\" in the code, and it is not known if this is significant to the code or not. Initially, the letters were thought to be words in a foreign language before it was realised it was a code. Code experts were called in at the time to decipher the lines but were unsuccessful. In 1978, following a request from ABC Television\\'s journalist Stuart Littlemore, Department of Defence cryptographers analysed the handwritten text. The cryptographers reported that it would be impossible to provide \"a satisfactory answer\": if the text were an encrypted message, its brevity meant that it had \"insufficient symbols\" from which a clear meaning could be extracted, and the text could be the \"meaningless\" product of a \"disturbed mind\".Jessica ThomsonA telephone number was also found in the back of the book, belonging to a nurse named Jessica Ellen \"Jo\" Thomson (1921–2007)–born Jessie Harkness in the Sydney suburb of Marrickville, New South Wales–who lived in Moseley St, Glenelg, about 400 metres (1,300 ft) north of the location where the body was found. When she was interviewed by police, Thomson said that she did not know the dead man or why he would have her phone number and choose to visit her suburb on the night of his death. However, she also reported that, at some time in late 1948, an unidentified man had attempted to visit her and asked a next door neighbour about her. In his book on the case, Gerry Feltus stated that when he interviewed Thomson in 2002, he found that she was either being \"evasive\" or she \"just did not wish to talk about it\". Feltus believed Thomson knew the Somerton man\\'s identity. Thomson\\'s daughter Kate, in a television interview in 2014 with Channel Nine\\'s 60 Minutes, also said that she believed her mother knew the dead man.In 1949, Jessica Thomson requested that police not keep a permanent record of her name or release her details to third parties, as it would be embarrassing and harmful to her reputation to be linked to such a case. The police agreed – a decision that hampered later investigations. In news media, books and other discussions of the case, Thomson was frequently referred to by various pseudonyms, including the nickname \"Jestyn\" and names such as \"Teresa Johnson née Powell\". Feltus in 2010 claimed he was given permission by Thomson\\'s family to disclose her names and that of her husband, Prosper Thomson. Nevertheless, the names Feltus used in his book were pseudonyms. Feltus also stated that her family did not know of her connection with the case, and he agreed not to disclose her identity or anything that might reveal it. Thomson\\'s real name was considered important because it may be the decryption key for the purported code.When she was shown the plaster cast bust of the dead man by DS Leane, Thomson said she could not identify the person depicted. According to Leane, he described her reaction upon seeing the cast as \"completely taken aback, to the point of giving the appearance that she was about to faint\". In an interview many years later, Paul Lawson–the technician who made the cast and was present when Thomson viewed it–noted that after looking at the bust she immediately looked away and would not look at it again.Thomson also said that while she was working at Royal North Shore Hospital in Sydney during World War II, she had owned a copy of Rubaiyat. In 1945, at the Clifton Gardens Hotel in Sydney, she had given it to an Australian Army lieutenant named Alf Boxall, who was serving at the time in the Water Transport Section of the Royal Australian Engineers. Thomson told police that, after the war ended, she had moved to Melbourne and married. She said that she had received a letter from Boxall and had replied, telling him that she was now married. (Subsequent research suggests that her future husband, Prosper Thomson, was in the process of obtaining a divorce from his first wife in 1949, and that he did not marry Jessica until mid-1950.) There is no evidence that Boxall had any contact with Jessica Thomson after 1945.As a result of their conversations with Thomson, police suspected that Boxall was the dead man. However, in July 1949, Boxall was found in Sydney and the final page of his copy of Rubaiyat (reportedly a 1924 edition published in Sydney) was intact, with the words \"Tamam Shud\" still in place. Boxall was now working in the maintenance section at the Randwick Bus Depot (where he had worked before the war) and was unaware of any link between the dead man and himself. In the front of the copy of Rubaiyat that was given to Boxall, Jessica Harkness had signed herself \"JEstyn\"  [sic] and written out verse 70:Media reactionThe two daily Adelaide newspapers, The Advertiser and The News, covered the death in separate ways. The Advertiser first mentioned the case in a small article on page three of its morning edition of 2 December 1948. Titled \"Body found on Beach\", it read:A body, believed to be of E.C. Johnson, about 45, of Arthur St, Payneham, was found on Somerton Beach, opposite the Crippled Children\\'s Home yesterday morning. The discovery was made by Mr J. Lyons, of Whyte Rd, Somerton. Detective H. Strangway and Constable J. Moss are enquiring.The News featured their story on its first page, giving more details of the dead man.As one journalist wrote in June 1949, alluding to the line in Rubaiyat, \"the Somerton Man seems to have made certain that the glass would be empty, save for speculation\". An editorial called the case \"one of Australia\\'s most profound mysteries\" and noted that if he died by poison so rare and obscure it could not be identified by toxicology experts, then surely the culprit\\'s advanced knowledge of toxic substances pointed to something more serious than a mere domestic poisoning.Spy theoriesThere has been persistent speculation that the dead man was a spy, due to the circumstances and historical context of his death. At least two sites relatively close to Adelaide were of interest to spies: the Radium Hill uranium mine and the Woomera Test Range, an Anglo-Australian military research facility. The man\\'s death also coincided with a reorganisation of Australian security agencies, which would culminate the following year with the founding of the Australian Security Intelligence Organisation (ASIO). This would be followed by a crackdown on Soviet espionage in Australia, which was revealed by intercepts of Soviet communications under the Venona project.Another theory concerns Boxall, who was reportedly involved in intelligence work during and immediately after World War II. In a 1978 television interview Stuart Littlemore asks: \"Mr Boxall, you had been working, hadn\\'t you, in an intelligence unit, before you met this young woman [Jessica Harkness]. Did you talk to her about that at all?\" In reply, Boxall says \"no\", and when asked if Harkness could have known, Boxall replies: \"Not unless somebody else told her.\" When Littlemore suggests in the interview that there may have been an espionage connection to the dead man in Adelaide, Boxall replies: \"It\\'s quite a melodramatic thesis, isn\\'t it?\" Boxall\\'s army service record suggests that he served initially in the 4th Water Transport Company, before being seconded to the North Australia Observer Unit (NAOU)–a special operations unit–and that during his time with NAOU, Boxall rose rapidly in rank, being promoted from lance corporal to lieutenant within three months.Post-inquestIn 1949, the body of the unknown man was buried in Adelaide\\'s West Terrace Cemetery, where the Salvation Army conducted the service. The South Australian Grandstand Bookmakers Association paid for the service to save the man from a pauper\\'s burial.Years after the burial, flowers began appearing on the grave. Police questioned a woman seen leaving the cemetery but she claimed she knew nothing of the man. About the same time, Ina Harvey, the receptionist from the Strathmore Hotel opposite Adelaide railway station, revealed that a strange man had stayed in Room 21 or 23 for a few days around the time of the death, checking out on 30 November 1948. She recalled that he was English speaking and only carrying a small black case, not unlike one a musician or a doctor might carry. When an employee looked inside the case he told Harvey he had found an object inside the case he described as looking like a \"needle\". On 22 November 1959 it was reported that one E.B. Collins, an inmate of New Zealand\\'s Whanganui Prison, claimed to know the identity of the dead man.There have been numerous unsuccessful attempts in the seventy-plus years since its discovery to crack the letters found at the rear of the book, including efforts by military and naval intelligence, mathematicians, and amateur code crackers. In 2004, retired detective Gerry Feltus suggested in a Sunday Mail article that the final line \"ITTMTSAMSTGAB\" could stand for the initials of \"It\\'s Time To Move To South Australia Moseley Street...\" (Jessica Thomson lived in Moseley Street which is the main road through Glenelg). A 2014 analysis by computational linguist John Rehling strongly supports the theory that the letters consist of the initials of some English text, but finds no match for these in a large survey of literature, and concludes that the letters were likely written as a form of shorthand, not as a code, and that the original text can likely never be determined.In 1978, ABC-TV, in its documentary series Inside Story, produced a programme on the Tamám Shud case, titled \"The Somerton Beach Mystery\", where reporter Stuart Littlemore investigated the case, including interviewing Boxall, who could add no new information, and Paul Lawson, who made the plaster cast of the body and who refused to answer a question about whether anyone had positively identified the body.In 1994, John Harber Phillips, Chief Justice of Victoria and Chairman of the Victorian Institute of Forensic Medicine, reviewed the case to determine the cause of death and concluded that, \"There seems little doubt it was digitalis.\" Phillips supported his conclusion by pointing out that the organs were engorged, consistent with digitalis, the lack of evidence of natural disease and \"the absence of anything seen macroscopically which could account for the death\".Former South Australian Chief Superintendent Len Brown, who worked on the case in the 1940s, stated that he believed that the man was from a country in the Warsaw Pact, which led to the police\\'s inability to confirm the man\\'s identity.The South Australian Police Historical Society holds the plaster bust, which contains strands of the man\\'s hair. Any further attempts to identify the body have been hampered by the embalming formaldehyde having destroyed much of the man\\'s DNA. Other key evidence no longer exists, such as the brown suitcase, which was destroyed in 1986. In addition, witness statements have disappeared from the police file over the years.Abbott investigationIn March 2009 a University of Adelaide team led by Professor Derek Abbott began an attempt to solve the case through cracking the code and proposing to exhume the body to test for DNA. His investigations have led to questions concerning the assumptions police had made on the case. Abbott also tracked down the Barbour waxed cotton of the period and found packaging variations. This may provide clues to the country where it was purchased.Decryption of the \"code\" was being started from scratch. It had been determined the letter frequency was considerably different from letters written down randomly; the frequency was to be further tested to determine if the alcohol level of the writer could alter random distribution. The format of the code also appeared to follow the quatrain format of Rubaiyat, supporting the theory that the code was a one-time pad encryption algorithm. Copies of Rubaiyat, as well as the Talmud and Bible, were being compared to the code using computers to get a statistical base for letter frequencies. However, the code\\'s short length meant the investigators would require the exact edition of the book used. With the original copy lost in the 1950s, researchers have been looking for a FitzGerald edition.An investigation had shown that the Somerton man\\'s autopsy reports of 1948 and 1949 are now missing and the Barr Smith Library\\'s collection of Cleland\\'s notes do not contain anything on the case. Maciej Henneberg, professor of anatomy at the University of Adelaide, examined images of the Somerton man\\'s ears and found that his cymba (upper ear hollow) is larger than his cavum (lower ear hollow), a feature possessed by only 1–2% of the Caucasian population. In May 2009, Abbott consulted with dental experts who concluded that the Somerton Man had hypodontia (a rare genetic disorder) of both lateral incisors, a feature present in only 2% of the general population. In June 2010, Abbott obtained a photograph of Jessica Thomson\\'s eldest son Robin, which clearly showed that he–like the unknown man–had not only a larger cymba than cavum but also hypodontia. The chance that this was a coincidence has been estimated as between one in 10,000,000 and one in 20,000,000.The media have suggested that Robin Thomson, who was sixteen months old in 1948 and died in 2009, may have been a child of either Boxall or the Somerton man and passed off as Prosper Thomson\\'s son. DNA testing would confirm or eliminate this speculation. Abbott believes an exhumation and an autosomal DNA test could link the Somerton man to a shortlist of surnames which, along with existing clues to the man\\'s identity, would be the \"final piece of the puzzle\".In July 2013, Abbott released an artistic impression he commissioned of the Somerton man, believing this might finally lead to an identification. \"All this time we\\'ve been publishing the autopsy photo, and it\\'s hard to tell what something looks like from that\", Abbott said.According to a 2015 feature in California Sunday, Abbott married Rachel, the daughter of Roma Egan and Robin Thomson, in 2010.In December 2017, Abbott announced three \"excellent\" hairs \"at the right development stage for extracting DNA\" had been found on the plaster cast of the corpse, and had been submitted for analysis to the Australian Centre for Ancient DNA at the University of Adelaide. Processing the results could reportedly take up to a year. While much of the DNA is degraded, in February 2018, the University of Adelaide team obtained a high definition analysis of the mitochondrial DNA from the hair sample from Somerton Man. They found that the Somerton Man belonged to haplogroup H4a1a1a, possessed by only 1% of Europeans. However, mitochondrial DNA is only inherited through the maternal line, and therefore cannot be used to investigate a hereditary link between Rachel Egan, Abbott\\'s wife, and the Somerton Man.60 Minutes investigationIn November 2013 relatives of \"Jestyn\" gave interviews to the Channel Nine current affairs program 60 Minutes. Kate Thomson, the daughter of Jessica and Prosper Thomson, said that her mother was the woman interviewed by the police and that her mother had told her she had lied to them – Jessica did know the identity of the Somerton man and his identity was also \"known to a level higher than the police force\". Thomson\\'s father had died in 1995 and mother had died in 2007. She suggested that her mother and the Somerton man may both have been spies, noting that Jessica Thomson taught English to migrants, was interested in communism, and could speak Russian, although she would not disclose to her daughter where she had learned it or why.Robin Thomson\\'s widow, Roma Egan, and their daughter Rachel Egan, also appeared on 60 Minutes suggesting that the Somerton man was Robin\\'s father and, therefore, Rachel\\'s grandfather. The Egans reported lodging a new application with the Attorney-General John Rau to have the Somerton man\\'s body exhumed and DNA tested. Abbott also subsequently wrote to Rau in support of the Egans, saying that exhumation for DNA testing would be consistent with federal government policy of identifying soldiers in war graves, to bring closure to their families. Kate Thomson opposed the exhumation as being disrespectful to her brother.ExhumationIn October 2011, as interest in the case resurfaced, Attorney-General John Rau refused to exhume the body, stating: \"There needs to be public interest reasons that go well beyond public curiosity or broad scientific interest.\" Feltus said he was still contacted by people in Europe who believed the man was a missing relative but did not believe an exhumation and finding the man\\'s family grouping would provide answers to relatives, as \"during that period so many war criminals changed their names and came to different countries\".In October 2019, however, Attorney-General Vickie Chapman granted approval for his body to be exhumed to extract DNA for analysis. The parties interested in the analysis agreed to cover the costs. A potential granddaughter\\'s DNA is planned to be compared to the unknown man\\'s to see if it is a match.An exhumation was carried out on 19 May 2021. The remains were deeper in the ground than previously thought. It was reported that the body was exhumed as part of Operation Persevere and Operation Persist, which are investigating historical unidentified remains in South Australia. The authorities have said that they intend to take DNA from the remains if possible. Dr. Anne Coxon of Forensic Science South Australia said: \"The technology available to us now is clearly light years ahead of the techniques available when this body was discovered in the late 1940s,\" and that tests would use \"every method at our disposal to try and bring closure to this enduring mystery\".Reported identificationsA number of possible identifications have been proposed over the years. The Advertiser, on reporting the discovery of the body, gave the possible identification as \"E.C. Johnson, about 45, of Arthur St, Payneham\". The following day, 3 December 1948, Johnson identified himself at a police station. That same day, The News published a photograph of the dead man on its front page, leading to additional calls from members of the public about his possible identity. By 4 December, police had announced that the man\\'s fingerprints were not on South Australian police records, forcing them to look further afield. On 5 December, The Advertiser reported that police were searching through military records after a man claimed to have had a drink with a person resembling the dead man at a hotel in Glenelg on 13 November. During their drinking session, the mystery man supposedly produced a military pension card bearing the name \"Solomonson\".In early January 1949, two people identified the body as that of 63-year-old former wood cutter Robert Walsh. A third person, James Mack, also viewed the body, initially could not identify it, but an hour later he contacted police to claim it was Walsh. Mack stated that the reason he did not confirm this at the viewing was a difference in the colour of the hair. Walsh had left Adelaide several months earlier to buy sheep in Queensland but had failed to return at Christmas as planned. Police were skeptical, believing Walsh to be too old to be the dead man. However, the police did state that the body was consistent with that of a man who had been a wood cutter, although the state of the man\\'s hands indicated he had not cut wood for at least eighteen months. Any thoughts that a positive identification had been made were quashed, however, when Elizabeth Thompson, one of the people who had earlier positively identified the body as Walsh, retracted her statement after a second viewing of the body, where the absence of a particular scar on the body, as well as the size of the dead man\\'s legs, led her to realise the body was not Walsh.By early February 1949, there had been eight different \"positive\" identifications of the body, including two Darwin men who thought the body was of a friend of theirs, and others who thought it was a missing station worker, a worker on a steamship or a Swedish man. Detectives from Victoria initially believed the man was from there because of the similarity of the laundry marks to those used by several dry-cleaning firms in Melbourne. Following publication of the man\\'s photograph in Victoria, twenty-eight people claimed to know his identity. Victoria detectives disproved all the claims and said that \"other investigations\" indicated it was unlikely that he was from Victoria. A seaman named Tommy Reade from the SS Cycle, in port at the time, was thought to be the dead man, but after some of his shipmates viewed the body at the morgue, they stated categorically that the corpse was not that of Reade. By November 1953, police announced they had recently received the 251st \"solution\" to the identity of the body from members of the public who claimed to have met or known him. But, they said that the \"only clue of any value\" remained the clothing the man wore.In 2011, an Adelaide woman contacted biological anthropologist Maciej Henneberg about an identification card of an H. C. Reynolds that she had found in her father\\'s possessions. The card, a document issued in the United States to foreign seamen during World War I, was given to Henneberg in October 2011 for comparison of the ID photograph to that of the Somerton man. While Henneberg found anatomical similarities in features such as the nose, lips and eyes, he believed they were not as reliable as the close similarity of the ear. The ear shapes shared by both men were a \"very good\" match, although Henneberg also found what he called a \"unique identifier\"; a mole on the cheek that was the same shape and in the same position in both photographs. \"Together with the similarity of the ear characteristics, this mole, in a forensic case, would allow me to make a rare statement positively identifying the Somerton man.\"The ID card, numbered 58757, was issued in the United States on 28 February 1918 to H. C. Reynolds, giving his nationality as \"British\" and age as 18. Searches conducted by the US National Archives, the UK National Archives and the Australian War Memorial Research Centre have failed to find any records relating to H. C. Reynolds. The South Australia Police Major Crime Branch, who still have the case listed as open, will investigate the new information. Some independent researchers believe the ID card belonged to Horace Charles Reynolds, a Tasmanian man who died in 1953 and therefore could not have been the Somerton man.Timelinecirca 1905: Somerton Man is born, according to the coroner\\'s report.April 1906: Alfred Boxall born in London, England.16 October 1912: Prosper Thomson is born in central Queensland.28 February 1918: H. C. Reynolds identity card issued.1921: Jessie Harkness is born in Marrickville, New South Wales.1936: Prosper Thomson moves from Blacktown, New South Wales, to Melbourne, Victoria, marries and lives in Mentone, a south east Melbourne suburb.August 1945: Jessica Harkness gives Alf Boxall an inscribed copy of Rubaiyat over drinks at the Clifton Gardens Hotel, Sydney, prior to his being posted overseas on active service. The inscription is signed \"JEstyn\".circa October 1946: Jessica Harkness\\'s son Robin is conceived (assuming a normal duration pregnancy).Late 1946: Harkness moves to Mentone to temporarily live with her parents. (The same Melbourne suburb in which Prosper Thomson had established himself and his then new wife ten years before.)Early 1947: Harkness moves to a suburb of Adelaide, South Australia, and changes her surname to Thomson, the name of her future husband.July 1947: Robin Thomson is born.15 January 1948: Boxall arrives back in Sydney from his last active duty and is discharged from the army in April 1948.July 1948: \"Prosper McTaggart Thomson, hire car proprietor, of Moseley Street, Glenelg\" appears in Adelaide Local Court as defendant in a car sale dispute, dating from November 1947, establishing Prosper Thomson as active in Adelaide from 1947.30 November 1948. 8:30 a.m. to 10:50 am: The Somerton Man is presumed to have arrived in Adelaide by train. He buys a ticket for the 10:50 a.m. train to Henley Beach but does not use it. This ticket was the first sold of only three issued between 6:15 a.m. and 2 p.m. by this particular ticket clerk for the Henley Beach train.Between 8:30 a.m. to 10:50 am: There is no satisfactory explanation for what The Somerton Man did during these hours. There is no record of the Adelaide railway station\\'s bathroom facilities being unavailable and no ticket in his pocket to suggest he had visited the Public Baths, outside of the station.Between 11:00 a.m. and 11:15 a.m: Checks a brown suitcase into the train station cloak room.after 11:15 am: Buys a 7d bus ticket on a bus that departed at 11:15 a.m. from the south side of North Terrace (in front of the Strathmore Hotel) opposite the railway station. He may have boarded at a later time elsewhere in the city as his ticket was the sixth of nine sold between the railway station and South Terrace; however, he only had a fifteen-minute window from the earliest time he could have checked his suitcase (the luggage room was around sixty metres from the bus stop). It is not known which stop he alighted at; the bus terminated at Somerton Park at 11:44 am and enquiries indicated that he \"must have\" alighted at Glenelg, a short distance from the St. Leonard\\'s hotel. This stop is less than 1 kilometre (3,300 ft) north of the Moseley St address of Jessica Thomson, which was itself 400 metres from where the body was found.7 p.m.–8 p.m.: Various witness sightings.10 p.m.–11 p.m.: Estimated time he had eaten the pasty based on time of death.1 December 2 a.m.: Estimated time of death. The time was estimated by a \"quick opinion\" on the state of rigor mortis while the ambulance was in transit. As a suspected suicide, no attempt to determine the correct time was made. As poisons affect the progression of rigor, 2 a.m. is probably inaccurate.6:30 am: Found dead by John Lyons and two men with a horse.14 January 1949: Adelaide railway station finds the brown suitcase belonging to the man.6–14 June: The piece of paper bearing the inscription \"Tamám Shud\" is found in a concealed fob pocket.17 and 21 June: Coroner\\'s inquest.22 July: A man hands in the copy of Rubaiyat he had found on 30 November (or perhaps a week or two earlier) containing an unlisted phone number and mysterious inscription. Police later match the \"Tamám Shud\" paper to the book.26 July: The unlisted phone number discovered in the book is traced to a woman living in Glenelg (Jessica Thomson, previously Harkness). Shown the plaster cast by Paul Lawson, she did not identify that the man was Alf Boxall, or any other person. Lawson\\'s diary entry for that day names her as \"Mrs Thompson\" and states that she had a \"nice figure\" and was \"very acceptable\" (referring to the level of attractiveness) which allows the possibility of an affair with the Somerton man. She was 27 years old in 1948. In a later interview Lawson described her behaviour as being very odd that day. She appeared as if she was about to faint. Jessica Harkness requests that her real name be withheld because she didn\\'t want her husband to know she knew Alf Boxall. Although she was in fact not married at this time, the name she gave police was Jessica Thomson with her real name not being discovered until 2002.27 July: Sydney detectives locate and interview Boxall.Early 1950: Prosper Thomson\\'s divorce is finalised.May 1950: Jessica and Prosper Thomson are married.1950s: The original Rubaiyat is lost.18 May 1953: death of Horace Charles Reynolds, Tasmanian man born in 1900 and regarded by some investigators as the owner of the \"H. C. Reynolds\" ID card.14 March 1958: The coroner\\'s inquest is continued. The Thomsons and Alf Boxall are not mentioned. No new findings are recorded and the inquest is ended with an adjournment sine die.1986: The Somerton Man\\'s brown suitcase and contents are destroyed as \"no longer required\".1994: The Chief Justice of Victoria, John Harber Phillips, studies the evidence and concludes that poisoning was due to digitalis.26 April 1995: Prosper Thomson dies.17 August 1995: Boxall dies.13 May 2007: Jessica Thomson dies.March 2009: Robin Thomson dies.14 October 2019: Attorney-General of South Australia grants conditional approval for The Somerton Man to be exhumed in order for a DNA sample to be obtained.19 May 2021: Exhumation took place.Similar or possibly related cases, 1945–1949Joseph \"George\" MarshallIn June 1945—three years before the death of the Somerton man—a 34-year-old Singaporean named George Marshall (born Joseph Saul Haim Mashal) was found dead in Ashton Park, Mosman, with an open copy of Rubaiyat on his chest. Ashton Park is directly adjacent to Clifton Gardens. Marshall\\'s death is believed to be a suicide by poisoning and occurred two months before Harkness gave Boxall the inscribed copy of Rubaiyat. Marshall was a brother of David Marshall, who was later to become Singapore\\'s first Chief Minister. An inquest was held on 15 August 1945; Gwenneth Dorothy Graham testified at the inquest and was found dead thirteen days later face down, naked, in a bath with her wrists slit.Mangnoson familyOn 6 June 1949, the body of two-year-old Clive Mangnoson was found in a sack in the Largs Bay sand hills, about 20 kilometres (12 mi) up the coast from Somerton Park. Lying next to him was his unconscious father, Keith Waldemar Mangnoson. The father was taken to a hospital in a very weak condition, suffering from exposure; following a medical examination, he was transferred to a mental hospital. The Mangnosons had been missing for four days. The police believed that Clive had been dead for twenty-four hours when his body was found. The two were found by Neil McRae of Largs Bay, who claimed he had seen the location of the two in a dream the night before. The coroner could not determine the young Mangnoson\\'s cause of death, although it was not believed to be natural causes. The contents of the boy\\'s stomach were sent to a government analyst for further examination.Following the death, the boy\\'s mother, Roma Mangnoson, reported having been threatened by a masked man who, while driving a battered cream car, almost ran her down outside her home in Cheapside Street, Largs North. Mangnoson stated that \"the car stopped and a man with a khaki handkerchief over his face told her to \\'keep away from the police or else\\'\". Additionally a similar-looking man had been recently seen lurking around the house. Mangnoson believed that this situation was related to her husband\\'s attempt to identify the Somerton Man, believing him to be Carl Thompsen, who had worked with him in Renmark in 1939. Soon after being interviewed by police over her harassment, Mangnoson collapsed and required medical treatment.J. M. Gower, secretary of the Largs North Progress Association, received anonymous phone calls threatening that Mrs. Mangnoson would meet with an accident if he interfered while A. H. Curtis, the acting mayor of Port Adelaide, received three anonymous phone calls threatening \"an accident\" if he \"stuck his nose into the Mangnoson affair\". Police suspect the calls may be a hoax and the caller may be the same person who also terrorised a woman in a nearby suburb who had recently lost her husband in tragic circumstances.MediaThe Unknown Man: A Suspicious Death at Somerton Beach by Gerald Michael Feltus was published in 2010.Tamam Shud: The Somerton Man Mystery by Kerry Greenwood was published in 2012.The case was covered by Casefile True Crime Podcast in Case 2: The Somerton Man, aired in January 2016.Episode 50 of the podcast My Favorite Murder discussed this case.Episode 7 of the Ross Bolen Podcast covered the case in its weekly segment on \"Stuff to Wikipedia when You\\'re High\".In 2019, ABC\\'s Radio National released a six-part series titled The Somerton Man Mystery.In popular cultureHolly Throsby\\'s novel Cedar Valley (2018) features a mysterious dead body found in the street of a small town. Characters in the book soon reference similarities to the Somerton Man and the case becomes a feature of the novel\\'s story.Episode 3 of the second series of The Doctor Blake Mysteries, \"A Foreign Field\", draws heavily on the case. The story included a mysterious victim found slumped dead in a public place, a suitcase of clothes found in a railway station locker with all labels removed, a page of a poem used with a secret code and even the victim\\'s last meal being a pasty.The Colorado Kid, a mystery novel by Stephen King, makes reference to a case that mirrors the Tamám Shud case almost exactly, except it is set in Maine.Australian rock band Tamam Shud took its name from this case.The Perth band The Drones\\' 2015 song \"Taman Shud\" frequently refers to the case and the unidentified man. The band\\'s adjoining album Feelin Kinda Free utilises an image of the code found in the back of The Rubaiyat in the album artwork.Australian-European black/thrash metal band Deströyer 666 featured a song about the case, titled \"Tamam Shud\", on their 2016 album Wildfire.Episode 9 of the second series of the dystopian science fiction drama Colony is named after the case.The BBC Radio 4 adaptation of The Shadow Over Innsmouth uses the case as a minor plot point, identifying the dead man\\'s unusually wide mouth and weak chin with the \"Innsmouth Look\".The 2016 album Fever Daydream by band The Black Queen has a song titled \"Taman Shud\".See alsoIsdal WomanList of unsolved murdersPeter Bergmann caseRicky McCormick\\'s encrypted notesNotesReferencesSourcesCleland, Thomas Erskine (1949). Inquest into the Death of a Body Located at Somerton on 1.12.48. GRG 1/27 File 71/1949, 17 and 21 June 1949 (PDF). State Records of South Australia.Cleland, Thomas Erskine (1958). Inquest into the Death of a Body Located at Somerton on 1.12.48. GRG 1/27 File 53/1958, 14 March 1958 (PDF). State Records of South Australia.Coghlan, Robyn (2008). Connections From London to the Antipodes: The Family of John Boxall and Sarah Hugkulstone. Acme Publishing Co., Hawker, ACT. ISBN 978-0-646-50443-8.Feltus, Gerald Michael (2010). The Unknown Man. South Australia: Klemzig. ISBN 978-0-646-54476-2.Further readingTamam Shud caseRuth Balint, \"The Somerton Man: An unsolved history,\" Cultural Studies Review, Vol. 16, no. 2, pp. 159–78, 2010.Ruth Balint, \"Der Somerton Man: Eine dokumentarische Fiktion in drei Dimensionen,\" Book Chapter in Goofy History: Fehler machen Geschichte, (Ed. Butis Butis) Böhlau Verlag, pp. 264–279, 2009, ISBN 9783412204266Omar Khayyam, The Rubaiyat of Omar Khayyam: First and Fifth Editions, translated by Edward FitzGerald Courier Dover Publications, 1990, ISBN 0-486-26467-X.Michael Newton, The Encyclopedia of Unsolved Crimes, Infobase Publishing, 2009, ISBN 0-8160-7818-1.John Pinkney, Great Australian Mysteries: Unsolved, Unexplained, Unknown, Five Mile Press, Rowville, Victoria, 2003. ISBN 1-74124-024-7.Kerry Greenwood, Tamam Shud – The Somerton Man Mystery, University of New South Wales Publishing, 2013 ISBN 978-1742233505Peter Bowes, The Bookmaker From Rabaul – Bennison Books Publishing, 2016 ISBN 978-0-9954302-1-1Scott Philbrook & Forrest Burgess – Astonishing Legends PodcastRoyal North Shore HospitalGeoffrey Sherington, Royal North Shore Hospital, 1888–1988 : A Century of Caring, Horwitz Grahame, Sydney, 1988, ISBN 0-7255-2104-X.Margaret Rice, The Close of an Era: A History of Nursing at the Royal North Shore Hospital, 1887–1987, St. Leonard\\'s, N.S.W: Royal North Shore Hospital Graduate Nurses Association, 1988, ISBN 0-9593811-4-7.Roger Vanderfield, Royal North Shore Hospital: Service in War and Peace, University of Sydney, Sydney, 2001.South Australia police articlesSA Police Historical Society Oct 2007 Newsletter on the caseSA Police Historical Society October 2010 article on Jimmy Durham who worked in the caseExternal linksArchival newspaper articles on the Taman Shud CaseReddit AMA interview with Taman Shud researcher Derek AbbottTaman Shud Case at the Doe Network',\n",
              " '= Fuzzy extractor =Fuzzy extractors are a method that allows biometric data to be used as inputs to standard cryptographic techniques for security.  \"Fuzzy\", in this context, refers to the fact that the fixed values required for cryptography will be extracted from values close to but not identical to the original key, without compromising the security required. One application is to encrypt and authenticate users records, using the biometric inputs of the user as a key.Fuzzy extractors are a biometric tool that allows for user authentication, using a biometric template constructed from the user\\'s biometric data as the key.  They extract a uniform and random stringR{\\\\displaystyle R}from an inputw{\\\\displaystyle w}with a tolerance for noise.  If the input changes tow′{\\\\displaystyle w\\'}but is still close tow{\\\\displaystyle w}, the same stringR{\\\\displaystyle R}will be re-constructed.  To achieve this, during the initial computation ofR{\\\\displaystyle R}the process also outputs a helper stringP{\\\\displaystyle P}which will be stored to recoverR{\\\\displaystyle R}later and can be made public without compromising the security ofR{\\\\displaystyle R}.  The security of the process is ensured also when an adversary modifiesP{\\\\displaystyle P}. Once the fixed stringR{\\\\displaystyle R}has been calculated, it can be used for example for key agreement between a user and a server based only on a biometric input.HistoryOne precursor to fuzzy extractors was the so-called \"Fuzzy Commitment\", as designed by Juels and Wattenberg. Here, the cryptographic key is decommited using biometric data.Later, Juels and Sudan came up with Fuzzy vault schemes. These are order invariant for the fuzzy commitment scheme and use a Reed–Solomon code. The codeword is inserted as the coefficients of a polynomial, and this polynomial is then evaluated with respect to various properties of the biometric data.Both Fuzzy Commitment and Fuzzy Vaults were together precursors to Fuzzy Extractors.MotivationIn order for fuzzy extractors to generate strong keys from biometric and other noisy data, cryptography paradigms will be applied to this biometric data. This means they must be allowed to:(1) Limit the number of assumptions about the content of the biometric data (this data comes from a variety of sources, so in order to avoid exploitation by an adversary, it\\'s best to assume the input is unpredictable)(2) Apply usual cryptographic techniques to the input. (Fuzzy extractors convert biometric data into secret, uniformly random and reliably reproducible random strings).These techniques can also have other broader applications for other type of noisy inputs such as approximative data from human memory, images used as passwords, keys from quantum channel. According to the Differential Privacy paper by Cynthia Dwork (ICALP 2006), fuzzy extractors also have applications in the proof of impossibility of the strong notions of privacy for statistical databases.Basic definitionsPredictabilityPredictability indicates the probability that an adversary can guess a secret key.  Mathematically speaking, the predictability of a random variableA{\\\\displaystyle A}ismaxaP[A=a]{\\\\displaystyle \\\\max _{\\\\mathrm {a} }P[A=a]}.For example, given a pair of random variableA{\\\\displaystyle A}andB{\\\\displaystyle B}, if the adversary knowsb{\\\\displaystyle b}ofB{\\\\displaystyle B}, then the predictability ofA{\\\\displaystyle A}will bemaxaP[A=a|B=b]{\\\\displaystyle \\\\max _{\\\\mathrm {a} }P[A=a|B=b]}.  So, an adversary can predictA{\\\\displaystyle A}withEb←B[maxaP[A=a|B=b]]{\\\\displaystyle E_{b\\\\leftarrow B}[\\\\max _{\\\\mathrm {a} }P[A=a|B=b]]}.  We use the average overB{\\\\displaystyle B}as it is not under adversary control, but since knowingb{\\\\displaystyle b}makes the prediction ofA{\\\\displaystyle A}adversarial, we take the worst case overA{\\\\displaystyle A}.Min-entropyMin-entropy indicates the worst-case entropy.  Mathematically speaking, it is defined asH∞(A)=−log\\u2061(maxaP[A=a]){\\\\displaystyle H_{\\\\infty }(A)=-\\\\log(\\\\max _{\\\\mathrm {a} }P[A=a])}.A random variable with a min-entropy at least ofm{\\\\displaystyle m}is called am{\\\\displaystyle m}-source.Statistical distanceStatistical distance is a measure of distinguishability.  Mathematically speaking, it is expressed for two probability distributionsA{\\\\displaystyle A}andB{\\\\displaystyle B}asSD[A,B]{\\\\displaystyle SD[A,B]}=12∑v|P[A=v]−P[B=v]|{\\\\displaystyle {\\\\frac {1}{2}}\\\\sum _{\\\\mathrm {v} }|P[A=v]-P[B=v]|}.  In any system, ifA{\\\\displaystyle A}is replaced byB{\\\\displaystyle B}, it will behave as the original system with a probability at least of1−SD[A,B]{\\\\displaystyle 1-SD[A,B]}.Definition 1 (strong extractor)SettingM{\\\\displaystyle M}as a strong randomness extractor. The randomized function Ext:M→{0,1}l{\\\\displaystyle M\\\\rightarrow \\\\{0,1\\\\}^{l}}with randomness of lengthr{\\\\displaystyle r}is a(m,l,ϵ){\\\\displaystyle (m,l,\\\\epsilon )}strong extractor if for allm{\\\\displaystyle m}-sourcesW{\\\\displaystyle W}onM(Ext\\u2061(W;I),I)≈ϵ(Ul,Ur),{\\\\displaystyle M(\\\\operatorname {Ext} (W;I),I)\\\\approx _{\\\\epsilon }(U_{l},U_{r}),}whereI=Ur{\\\\displaystyle I=U_{r}}is independent ofW{\\\\displaystyle W}.The output of the extractor is a key generated fromw←W{\\\\displaystyle w\\\\leftarrow W}with the seedi←I{\\\\displaystyle i\\\\leftarrow I}.   It behaves independently of other parts of the system with the probability of1−ϵ{\\\\displaystyle 1-\\\\epsilon }.  Strong extractors can extract at mostl=m−2log\\u20611ϵ+O(1){\\\\displaystyle l=m-2\\\\log {\\\\frac {1}{\\\\epsilon }}+O(1)}bits from an arbitrarym{\\\\displaystyle m}-source.Secure sketchSecure sketch makes it possible to reconstruct noisy input, so that if the input isw{\\\\displaystyle w}and the sketch iss{\\\\displaystyle s}, givens{\\\\displaystyle s}and a valuew′{\\\\displaystyle w\\'}close tow{\\\\displaystyle w},w{\\\\displaystyle w}can be recovered.  But the sketchs{\\\\displaystyle s}must not reveal information aboutw{\\\\displaystyle w}, in order to keep it secure.IfM{\\\\displaystyle \\\\mathbb {M} }is a metric space with the distance function dis, Secure sketch recovers the stringw∈M{\\\\displaystyle w\\\\in \\\\mathbb {M} }from any close stringw′∈M{\\\\displaystyle w\\'\\\\in \\\\mathbb {M} }without disclosingw{\\\\displaystyle w}.Definition 2 (secure sketch)An(m,m~,t){\\\\displaystyle (m,{\\\\tilde {m}},t)}secure sketch is a pair of efficient randomized procedures (the Sketch noted SS, the Recover noted Rec)  such that :(1) The sketching procedure SS applied on inputw∈M{\\\\displaystyle w\\\\in \\\\mathbb {M} }returns a strings∈{0,1}∗{\\\\displaystyle s\\\\in {\\\\{0,1\\\\}^{*}}}.The recovery procedure Rec uses as input two elementsw′∈M{\\\\displaystyle w\\'\\\\in \\\\mathbb {M} }ands∈{0,1}∗{\\\\displaystyle s\\\\in {\\\\{0,1\\\\}^{*}}}.(2) Correctness:  Ifdis(w,w′)≤t{\\\\displaystyle dis(w,w\\')\\\\leq t}thenRec(w′,SS(w))=w{\\\\displaystyle Rec(w\\',SS(w))=w}.(3) Security:  For anym{\\\\displaystyle m}-source overM{\\\\displaystyle M}, the min-entropy ofW{\\\\displaystyle W}givens{\\\\displaystyle s}is high:for any(W,E){\\\\displaystyle (W,E)}, ifH~∞(W|E)≥m{\\\\displaystyle {\\\\tilde {H}}_{\\\\mathrm {\\\\infty } }(W|E)\\\\geq m}, thenH~∞(W|SS(W),E)≥m~{\\\\displaystyle {\\\\tilde {H}}_{\\\\mathrm {\\\\infty } }(W|SS(W),E)\\\\geq {\\\\tilde {m}}}.Fuzzy extractorFuzzy extractors do not recover the original input but generate a stringR{\\\\displaystyle R}(which is close to uniform) fromw{\\\\displaystyle w}and allow its subsequent reproduction (using helper stringP{\\\\displaystyle P}) given anyw′{\\\\displaystyle w\\'}close tow{\\\\displaystyle w}.  Strong extractors are a special case of fuzzy extractors whent{\\\\displaystyle t}= 0 andP=I{\\\\displaystyle P=I}.Definition 3 (fuzzy extractor)An(m,l,t,ϵ){\\\\displaystyle (m,l,t,\\\\epsilon )}fuzzy extractor is a pair of efficient randomized procedures (Gen – Generate and Rep – Reproduce) such that:(1) Gen, givenw∈M{\\\\displaystyle w\\\\in \\\\mathbb {M} }, outputs an extracted stringR∈{0,1}l{\\\\displaystyle R\\\\in {\\\\mathbb {\\\\{} 0,1\\\\}^{l}}}and a helper stringP∈{0,1}∗{\\\\displaystyle P\\\\in {\\\\mathbb {\\\\{} 0,1\\\\}^{*}}}.(2) Correctness: Ifdis(w,w′)≤t{\\\\displaystyle dis(w,w\\')\\\\leq t}and(R,P)←Gen(w){\\\\displaystyle (R,P)\\\\leftarrow Gen(w)}, thenRep(w′,P)=R{\\\\displaystyle Rep(w\\',P)=R}.(3) Security: For all m-sourcesW{\\\\displaystyle W}overM{\\\\displaystyle M}, the stringR{\\\\displaystyle R}is nearly uniform even givenP{\\\\displaystyle P}, SoH~∞(W|E)≥m{\\\\displaystyle {\\\\tilde {H}}_{\\\\mathrm {\\\\infty } }(W|E)\\\\geq m}, then(R,P,E)≈(Ul,P,E){\\\\displaystyle (R,P,E)\\\\approx (U_{\\\\mathrm {l} },P,E)}.So Fuzzy extractors output almost uniform random sequences of bits which are a prerequisite for using cryptographic applications (as secret keys).  Since the output bits are slightly non-uniform, there\\'s a risk of a decreased security,  but the distance from a uniform distribution is no more thanϵ{\\\\displaystyle \\\\epsilon }and as long as this distance is sufficiently small, the security will remain adequate.Secure sketches and fuzzy extractorsSecure sketches can be used to construct fuzzy extractors.  Like applying SS tow{\\\\displaystyle w}to obtains{\\\\displaystyle s}and strong extractor Ext with randomnessx{\\\\displaystyle x}tow{\\\\displaystyle w}to getR{\\\\displaystyle R}.(s,x){\\\\displaystyle (s,x)}can be stored as helper stringP{\\\\displaystyle P}.R{\\\\displaystyle R}can be reproduced byw′{\\\\displaystyle w\\'}andP=(s,x){\\\\displaystyle P=(s,x)}.Rec(w′,s){\\\\displaystyle Rec(w\\',s)}can recoverw{\\\\displaystyle w}andExt(w,x){\\\\displaystyle Ext(w,x)}can reproduceR{\\\\displaystyle R}.The following lemma formalizes this.Lemma 1 (fuzzy extractors from sketches)Assume (SS,Rec) is an(M,m,m~,t){\\\\displaystyle (M,m,{\\\\tilde {m}},t)}secure sketch and let Ext be an average-case(n,m~,l,ϵ){\\\\displaystyle (n,{\\\\tilde {m}},l,\\\\epsilon )}strong extractor.  Then the following (Gen, Rep) is an(M,m,l,t,ϵ){\\\\displaystyle (M,m,l,t,\\\\epsilon )}fuzzy extractor:(1) Gen(w,r,x){\\\\displaystyle (w,r,x)}: setP=(SS(w;r),x),R=Ext(w;x),{\\\\displaystyle P=(SS(w;r),x),R=Ext(w;x),}and output(R,P){\\\\displaystyle (R,P)}.(2) Rep(w′,(s,x)){\\\\displaystyle (w\\',(s,x))}: recoverw=Rec(w′,s){\\\\displaystyle w=Rec(w\\',s)}and outputR=Ext(w;x){\\\\displaystyle R=Ext(w;x)}.Proof: From the definition of secure sketch (Definition 2),H∞(W|SS(W))≥m~{\\\\displaystyle H_{\\\\infty }(W|SS(W))\\\\geq {\\\\tilde {m}}}. And since Ext is an average-case(n,m,l,ϵ){\\\\displaystyle (n,m,l,\\\\epsilon )}-strong extractor.SD((Ext(W;X),SS(W),X),(Ul,SS(W),X))=SD((R,P),(Ul,P))≤ϵ.{\\\\displaystyle SD((Ext(W;X),SS(W),X),(U_{l},SS(W),X))=SD((R,P),(U_{l},P))\\\\leq \\\\epsilon .}Corollary 1If (SS,Rec) is an(M,m,m~,t){\\\\displaystyle (M,m,{\\\\tilde {m}},t)}secure sketch and Ext is an(n,m~−log(1δ),l,ϵ){\\\\displaystyle (n,{\\\\tilde {m}}-log({\\\\frac {1}{\\\\delta }}),l,\\\\epsilon )}strong extractor, then the above construction (Gen,Rep) is a(M,m,l,t,ϵ+δ){\\\\displaystyle (M,m,l,t,\\\\epsilon +\\\\delta )}fuzzy extractor.The reference paper includes many generic combinatorial bounds on secure sketches and fuzzy extractors.Basic constructionsDue to their error tolerant properties, secure sketches can be treated, analyzed, and constructed like a(n,k,d)F{\\\\displaystyle (n,k,d)_{\\\\mathcal {F}}}general error correcting code or[n,k,d]F{\\\\displaystyle [n,k,d]_{\\\\mathcal {F}}}for linear codes, wheren{\\\\displaystyle n}is the length of codewords,k{\\\\displaystyle k}is the length of the message to be codded,d{\\\\displaystyle d}is the distance between codewords, andF{\\\\displaystyle {\\\\mathcal {F}}}is the alphabet.  IfFn{\\\\displaystyle {\\\\mathcal {F}}^{n}}is the universe of possible words then it may be possible to find an error correcting codeC⊂Fn{\\\\displaystyle C\\\\subset {\\\\mathcal {F}}^{n}}such that there exists a unique codewordc∈C{\\\\displaystyle c\\\\in C}for everyw∈Fn{\\\\displaystyle w\\\\in {\\\\mathcal {F}}^{n}}with a Hamming distance ofdisHam(c,w)≤(d−1)/2{\\\\displaystyle dis_{Ham}(c,w)\\\\leq (d-1)/2}.  The first step for constructing a secure sketch is determining the type of errors that will likely occur and then choosing a distance to measure.Hamming distance constructionsWhen there is no risk of data being deleted and only of it being corrupted then the best measurement to use for error correction is the Hamming distance.  There are two common constructions for correcting Hamming errors depending on whether the code is linear or not.  Both constructions start with an error correcting code that has a distance of2t+1{\\\\displaystyle 2t+1}wheret{\\\\displaystyle {t}}is the number of tolerated errors.Code-offset constructionWhen using a(n,k,2t+1)F{\\\\displaystyle (n,k,2t+1)_{\\\\mathcal {F}}}general code, assign a uniformly random codewordc∈C{\\\\displaystyle c\\\\in C}to eachw{\\\\displaystyle w}, then letSS(w)=s=w−c{\\\\displaystyle SS(w)=s=w-c}which is the shift needed to changec{\\\\displaystyle c}intow{\\\\displaystyle w}.  To fix errors inw′{\\\\displaystyle w\\'}subtracts{\\\\displaystyle s}fromw′{\\\\displaystyle w\\'}then correct the errors in the resulting incorrect codeword to getc{\\\\displaystyle c}and finally adds{\\\\displaystyle s}toc{\\\\displaystyle c}to getw{\\\\displaystyle w}.  This meansRec(w′,s)=s+dec(w′−s)=w{\\\\displaystyle Rec(w\\',s)=s+dec(w\\'-s)=w}.  This construction can achieve the best possible tradeoff between error tolerance and entropy loss whenF≥n{\\\\displaystyle {\\\\mathcal {F}}\\\\geq n}and a Reed–Solomon code is used resulting in an entropy loss of2tlog\\u2061(F){\\\\displaystyle 2t\\\\log({\\\\mathcal {F}})}. The only way to improve upon would be to find a code better than Reed–Solomon.Syndrome constructionWhen using a[n,k,2t+1]F{\\\\displaystyle [n,k,2t+1]_{\\\\mathcal {F}}}linear code  let theSS(w)=s{\\\\displaystyle SS(w)=s}be the syndrome ofw{\\\\displaystyle w}.  To correctw′{\\\\displaystyle w\\'}find a vectore{\\\\displaystyle e}such thatsyn(e)=syn(w′)−s{\\\\displaystyle syn(e)=syn(w\\')-s}, thenw=w′−e{\\\\displaystyle w=w\\'-e}.Set difference constructionsWhen working with a very large alphabet or very long strings resulting in a very large universeU{\\\\displaystyle {\\\\mathcal {U}}}, it may be more efficient to treatw{\\\\displaystyle w}andw′{\\\\displaystyle w\\'}as sets and look at set differences to correct errors.  To work with a large setw{\\\\displaystyle w}it is useful to look at its characteristic vectorxw{\\\\displaystyle x_{w}}, which is a binary vector of lengthn{\\\\displaystyle n}that has a value of 1 when an elementa∈U{\\\\displaystyle a\\\\in {\\\\mathcal {U}}}anda∈w{\\\\displaystyle a\\\\in w}, or 0 whena∉w{\\\\displaystyle a\\\\notin w}.  The best way to decrease the size of a secure sketch whenn{\\\\displaystyle n}is large is makek{\\\\displaystyle k}large since the size is determined byn−k{\\\\displaystyle n-k}.  A good code to base this construction on is a[n,n−tα,2t+1]2{\\\\displaystyle [n,n-t\\\\alpha ,2t+1]_{2}}BCH code wheren=2α−1{\\\\displaystyle n=2^{\\\\alpha }-1}andt≪n{\\\\displaystyle t\\\\ll n}sok≤n−log(nt){\\\\displaystyle k\\\\leq n-log{n \\\\choose {t}}}, it is also useful that BCH codes can be decode in sub-linear time.Pin sketch constructionLetSS(w)=s=syn(xw){\\\\displaystyle SS(w)=s=syn(x_{w})}.  To correctw′{\\\\displaystyle w\\'}first findSS(w′)=s′=syn(xw′){\\\\displaystyle SS(w\\')=s\\'=syn(x_{w}\\')}, then find a set v wheresyn(xv)=s′−s{\\\\displaystyle syn(x_{v})=s\\'-s}, finally compute the symmetric difference to getRec(w′,s)=w′△v=w{\\\\displaystyle Rec(w\\',s)=w\\'\\\\triangle v=w}.  While this is not the only construction than can be used to set the difference, it is the easiest one.Edit distance constructionsWhen data can be corrupted or deleted, the best measurement to use is edit distance.  To make a construction based on edit distance, the easiest is to start with a construction for set difference or hamming distance as an intermediate correction step and then build the edit distance construction around that.Other distance measure constructionsThere are many other types of errors and distances that can be used to model other situations.  Most of these other possible constructions are built upon simpler constructions, like edit distance constructions.Improving error tolerance via relaxed notions of correctnessIt can be shown that the error tolerance of a secure sketch can be improved by applying a probabilistic method to error correction and only requesting errors to be correctable with a high probability.  This allows to exceed the Plotkin bound which limits to correctingn/4{\\\\displaystyle n/4}errors, and to approach Shannon\\'s bound allowing for nearlyn/2{\\\\displaystyle n/2}corrections. To achieve this enhanced error correction, a less restrictive error distribution model must be used.Random errorsFor this most restrictive model use a BSCp{\\\\displaystyle _{p}}to create aw′{\\\\displaystyle w\\'}that a probabilityp{\\\\displaystyle p}at each position inw′{\\\\displaystyle w\\'}that the bit received is wrong.  This model can show that entropy loss is limited tonH(p)−o(n){\\\\displaystyle nH(p)-o(n)}, whereH{\\\\displaystyle H}is the binary entropy function, and if min-entropym≥n(H(12−γ))+ε{\\\\displaystyle m\\\\geq n(H({\\\\frac {1}{2}}-\\\\gamma ))+\\\\varepsilon }thenn(12−γ){\\\\displaystyle n({\\\\frac {1}{2}}-\\\\gamma )}errors can be tolerated, for some constantγ>0{\\\\displaystyle \\\\gamma >0}.Input-dependent errorsFor this model errors do not have a known distribution and can be from an adversary, the only constraints arediserr≤t{\\\\displaystyle dis_{\\\\text{err}}\\\\leq t}and that a corrupted word depends only on the inputw{\\\\displaystyle w}and not on the secure sketch.  It can be shown for this error model that there will never be more thant{\\\\displaystyle t}errors since this model can account for all complex noise processes, meaning that Shannon\\'s bound can be reached, to do this a random permutation is prepended to the secure sketch that will reduce entropy loss.Computationally bounded errorsThis differs from the input dependent model by having errors that depend on both the inputw{\\\\displaystyle w}and the secure sketch, and an adversary is limited to polynomial time algorithms for introducing errors.  Since algorithms that can run in better than polynomial time are not currently feasible in the real world, then a positive result using this error model would guarantee that any errors can be fixed.  This is the least restrictive model the only known way to approach Shannon\\'s bound is to use list-decodable codes although this may not always be useful in practice since returning a list instead of a single codeword may not always be acceptable.Privacy guaranteesIn general a secure system attempts to leak as little information as possible to an adversary. In the case of biometrics if information about the biometric reading is leaked the adversary may be able to learn personal information about a user. For example, an adversary notices that there is a certain pattern in the helper strings that implies the ethnicity of the user. We can consider this additional information a functionf(W){\\\\displaystyle f(W)}. If an adversary were to learn a helper string, it must be ensured that, from this data he can not infer any data about the person from which the biometric reading was taken.Correlation between helper string and biometric inputIdeally the helper stringP{\\\\displaystyle P}would reveal no information about the biometric inputw{\\\\displaystyle w}. This is only possible when every subsequent biometric readingw′{\\\\displaystyle w\\'}is identical to the originalw{\\\\displaystyle w}. In this case there is actually no need for the helper string, so it is easy to generate a string that is in no way correlated tow{\\\\displaystyle w}.Since it is desirable to accept biometric inputw′{\\\\displaystyle w\\'}similar tow{\\\\displaystyle w}the helper stringP{\\\\displaystyle P}must be somehow correlated. The more differentw{\\\\displaystyle w}andw′{\\\\displaystyle w\\'}are allowed to be, the more correlation there will be betweenP{\\\\displaystyle P}andw{\\\\displaystyle w}, the more correlated they are the more informationP{\\\\displaystyle P}reveals aboutw{\\\\displaystyle w}. We can consider this information to be a functionf(W){\\\\displaystyle f(W)}. The best possible solution is to make sure the adversary can\\'t learn anything useful from the helper string.Gen(W) as a probabilistic mapA probabilistic mapY(){\\\\displaystyle Y()}hides the results of functions with a small amount of leakageϵ{\\\\displaystyle \\\\epsilon }. The leakage is the difference in probability two adversaries have of guessing some function when one knows the probabilistic map and one does not. Formally:|Pr[A1(Y(W))=f(W)]−Pr[A2()=f(W)]|≤ϵ{\\\\displaystyle |\\\\Pr[A_{1}(Y(W))=f(W)]-\\\\Pr[A_{2}()=f(W)]|\\\\leq \\\\epsilon }If the functionGen\\u2061(W){\\\\displaystyle \\\\operatorname {Gen} (W)}is a probabilistic map, then even if an adversary knows both the helper stringP{\\\\displaystyle P}and the secret stringR{\\\\displaystyle R}they are only negligibly more likely figure something out about the subject as if they knew nothing. The stringR{\\\\displaystyle R}is supposed to kept secret, so even if it is leaked (which should be very unlikely) the adversary can still figure out nothing useful about the subject, as long asϵ{\\\\displaystyle \\\\epsilon }is small. We can considerf(W){\\\\displaystyle f(W)}to be any correlation between the biometric input and some physical characteristic of the person. SettingY=Gen\\u2061(W)=R,P{\\\\displaystyle Y=\\\\operatorname {Gen} (W)=R,P}in the above equation changes it to:|Pr[A1(R,P)=f(W)]−Pr[A2()=f(W)]|≤ϵ{\\\\displaystyle |\\\\Pr[A_{1}(R,P)=f(W)]-\\\\Pr[A_{2}()=f(W)]|\\\\leq \\\\epsilon }This means that if one adversaryA1{\\\\displaystyle A_{1}}has(R,P){\\\\displaystyle (R,P)}and a second adversaryA2{\\\\displaystyle A_{2}}knows nothing, their best guesses atf(W){\\\\displaystyle f(W)}are onlyϵ{\\\\displaystyle \\\\epsilon }apart.Uniform fuzzy extractorsUniform fuzzy extractors are a special case of fuzzy extractors, where the output(R,P){\\\\displaystyle (R,P)}ofGen(W){\\\\displaystyle Gen(W)}are negligibly different from strings picked from the uniform distribution, i.e.(R,P)≈ϵ(Uℓ,U|P|){\\\\displaystyle (R,P)\\\\approx _{\\\\epsilon }(U_{\\\\ell },U_{|P|})}Uniform secure sketchesSince secure sketches imply fuzzy extractors, constructing a uniform secure sketch allows for the easy construction of a uniform fuzzy extractor. In a uniform secure sketch the sketch procedureSS(w){\\\\displaystyle SS(w)}is a randomness extractorExt(w;i){\\\\displaystyle Ext(w;i)}. Wherew{\\\\displaystyle w}is the biometric input andi{\\\\displaystyle i}is the random seed. Since randomness extractors output a string that appears to be from a uniform distribution they hide all the information about their input.ApplicationsExtractor sketches can be used to construct(m,t,ϵ){\\\\displaystyle (m,t,\\\\epsilon )}-fuzzy perfectly one-way hash functions. When used as a hash function the inputw{\\\\displaystyle w}is the object you want to hash. TheP,R{\\\\displaystyle P,R}thatGen(w){\\\\displaystyle Gen(w)}outputs is the hash value. If one wanted to verify that aw′{\\\\displaystyle w\\'}withint{\\\\displaystyle t}from the originalw{\\\\displaystyle w}, they would verify thatRep(w′,P)=R{\\\\displaystyle Rep(w\\',P)=R}.(m,t,ϵ){\\\\displaystyle (m,t,\\\\epsilon )}-fuzzy perfectly one-way hash functions are special hash functions where they accept any input with at mostt{\\\\displaystyle t}errors, compared to traditional hash functions which only accept when the input matches the original exactly. Traditional cryptographic hash functions attempt to guarantee that is it is computationally infeasible to find two different inputs that hash to the same value. Fuzzy perfectly one-way hash functions make an analogous claim. They make it computationally infeasible two find two inputs, that are more thant{\\\\displaystyle t}Hamming distance apart and hash to the same value.Protection against active attacksAn active attack could be one where the adversary can modify the helper stringP{\\\\displaystyle P}. If the adversary is able to changeP{\\\\displaystyle P}to another string that is also acceptable to the reproduce functionRep(W,P){\\\\displaystyle Rep(W,P)}, it causesRep(W,P){\\\\displaystyle Rep(W,P)}to output an incorrect secret stringR~{\\\\displaystyle {\\\\tilde {R}}}. Robust fuzzy extractors solve this problem by allowing the reproduce function to fail, if a modified helper string is provided as input.Robust fuzzy extractorsOne method of constructing robust fuzzy extractors is to use hash functions. This construction requires two hash functionsH1{\\\\displaystyle H_{1}}andH2{\\\\displaystyle H_{2}}.  TheGen(W){\\\\displaystyle Gen(W)}functions produces the helper stringP{\\\\displaystyle P}by appending the output of a secure sketchs=SS(w){\\\\displaystyle s=SS(w)}to the hash of both the readingw{\\\\displaystyle w}and secure sketchs{\\\\displaystyle s}. It generates the secret stringR{\\\\displaystyle R}by applying the second hash function tow{\\\\displaystyle w}ands{\\\\displaystyle s}. Formally:Gen(w):s=SS(w),return:P=(s,H1(w,s)),R=H2(w,s){\\\\displaystyle Gen(w):s=SS(w),return:P=(s,H_{1}(w,s)),R=H_{2}(w,s)}The reproduce functionRep(W,P){\\\\displaystyle Rep(W,P)}also makes use of the hash functionsH1{\\\\displaystyle H_{1}}andH2{\\\\displaystyle H_{2}}. In addition to verifying the biometric input is similar enough to the one recovered using theRec(W,S){\\\\displaystyle Rec(W,S)}function, it also verifies that hash in the second part ofP{\\\\displaystyle P}was actually derived fromw{\\\\displaystyle w}ands{\\\\displaystyle s}. If both of those conditions are met it returnsR{\\\\displaystyle R}which is itself the second hash function applied tow{\\\\displaystyle w}ands{\\\\displaystyle s}. Formally:Rep(w′,P~):{\\\\displaystyle Rep(w\\',{\\\\tilde {P}}):}Gets~{\\\\displaystyle {\\\\tilde {s}}}andh~{\\\\displaystyle {\\\\tilde {h}}}fromP~;w~=Rec(w′,s~).{\\\\displaystyle {\\\\tilde {P}};{\\\\tilde {w}}=Rec(w\\',{\\\\tilde {s}}).}IfΔ(w~,w′)≤t{\\\\displaystyle \\\\Delta ({\\\\tilde {w}},w\\')\\\\leq t}andh~=H1(w~,s~){\\\\displaystyle {\\\\tilde {h}}=H_{1}({\\\\tilde {w}},{\\\\tilde {s}})}thenreturn:H2(w~,s~){\\\\displaystyle return:H_{2}({\\\\tilde {w}},{\\\\tilde {s}})}elsereturn:fail{\\\\displaystyle return:fail}IfP{\\\\displaystyle P}has been tampered with, it will be obvious because,Rep{\\\\displaystyle Rep}will output fail with very high probability. To cause the algorithm accept a differentP{\\\\displaystyle P}an adversary would have to find aw~{\\\\displaystyle {\\\\tilde {w}}}such thatH1(w,s)=H1(w~,s~){\\\\displaystyle H_{1}(w,s)=H_{1}({\\\\tilde {w}},{\\\\tilde {s}})}. Since hash function are believed to be one-way functions, it is computationally infeasible to find such aw~{\\\\displaystyle {\\\\tilde {w}}}.SeeingP{\\\\displaystyle P}would provide the adversary with no useful information. Since, again, hash function are one-way functions, it is computationally infeasible for the adversary to reverse the hash function and figure outw{\\\\displaystyle w}. Part ofP{\\\\displaystyle P}is the secure sketch, but by definition the sketch reveals negligible information about its input. Similarly seeingR{\\\\displaystyle R}(even though it should never see it) would provide the adversary with no useful information as the adversary wouldn\\'t be able to reverse the hash function and see the biometric input.References\"Fuzzy Extractors: A Brief Survey of Results from 2004 to 2006\".\"Biometric Fuzzy Extractor Scheme for Iris Templates\" (PDF). {{cite journal}}: Cite journal requires |journal= (help)\"A Fuzzy Vault Scheme\" (PDF). {{cite journal}}: Cite journal requires |journal= (help)External links\"Minisketch: An optimized C++ library for BCH-based (Pin Sketch) set reconciliation\". github.com. 31 May 2021.']"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "search_engine_ok.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2628e75144484be498372d8828f8e028": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_575301a88cb241fe82fc023695676442",
              "IPY_MODEL_59388f2165f94581a1e8274831770d00",
              "IPY_MODEL_7df9b3b5a8f94e729e81c522dbd8b568"
            ],
            "layout": "IPY_MODEL_2386b75f321a4f749a74fa1af8d9a280"
          }
        },
        "575301a88cb241fe82fc023695676442": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4f56127b816442a9231b5cd64081375",
            "placeholder": "​",
            "style": "IPY_MODEL_875908790ff84dec91e42e70ca2e7afe",
            "value": "100%"
          }
        },
        "59388f2165f94581a1e8274831770d00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca336ba0f7e348a993d451e37cd8e562",
            "max": 1047,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5bdbc33f0e09463c98497b287347aaa8",
            "value": 1047
          }
        },
        "7df9b3b5a8f94e729e81c522dbd8b568": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f016aec1ed3040469bd74b0bcc4a29c4",
            "placeholder": "​",
            "style": "IPY_MODEL_2b11449a0d9340faaf7f76e893ffc208",
            "value": " 1047/1047 [00:00&lt;00:00, 3485.30it/s]"
          }
        },
        "2386b75f321a4f749a74fa1af8d9a280": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4f56127b816442a9231b5cd64081375": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "875908790ff84dec91e42e70ca2e7afe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca336ba0f7e348a993d451e37cd8e562": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bdbc33f0e09463c98497b287347aaa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f016aec1ed3040469bd74b0bcc4a29c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b11449a0d9340faaf7f76e893ffc208": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}