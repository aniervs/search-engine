{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0CqJ8NLuTokR"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "from pprint import pprint\n",
        "from glob import glob\n",
        "import random\n",
        "import spacy\n",
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.spatial.distance import cosine\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip wiki_data"
      ],
      "metadata": {
        "id": "PyYekdtAYXuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "1q9iK9FIbiiy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "91d1f1e3490647d09158101bfd7cca54",
            "45230a2b1ea1404a82b323bde2304476",
            "555f2e5a4c454f7f81aea5a01b50d106",
            "6c3d6da05e924be0bdcf51251514978c",
            "bb528ae73b124df983afd63b308c81e3",
            "a75cdf6f70724adb893cd1d236da334c",
            "bad162f4812b4cdeac29fe30b0b1b8d6",
            "916667e7b92c4a36b3413470f2ff47b2",
            "b9e5eeafc92848419a0df84e24bbfbad",
            "7b86f8725b2c49ae87d4a5c1aa0e3337",
            "0fb6516d15cb44438c429ae9a427dda2"
          ]
        },
        "outputId": "36f3be69-a67a-4937-b169-de0e7df3aa83"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "91d1f1e3490647d09158101bfd7cca54",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/1047 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1047,\n",
              " '= Carmichael number =In number theory, a Carmichael number is a composite numbern{\\\\displaystyle n}which satisfies the modular arithmetic congruence relation:bn−1≡1(modn){\\\\displaystyle b^{n-1}\\\\equiv 1{\\\\pmod {n}}}for all integersb{\\\\displaystyle b}which are relatively prime ton{\\\\displaystyle n}.They are named for Robert Carmichael.The Carmichael numbers are the subset K1 of the Knödel numbers.Equivalently, a Carmichael number is a composite numbern{\\\\displaystyle n}for whichbn≡b(modn){\\\\displaystyle b^{n}\\\\equiv b{\\\\pmod {n}}}for all integersb{\\\\displaystyle b}.OverviewFermat\\'s little theorem states that if p is a prime number, then for any integer b, the number bp − b is an integer multiple of p.  Carmichael numbers are composite numbers which have this property.  Carmichael numbers are also called Fermat pseudoprimes or absolute Fermat pseudoprimes. A Carmichael number will pass a Fermat primality test to every base b relatively prime to the number, even though it is not actually prime.This makes tests based on Fermat\\'s Little Theorem less effective than strong probable prime tests such as the Baillie–PSW primality test and the Miller–Rabin primality test.However, no Carmichael number is either an Euler–Jacobi pseudoprime or a strong pseudoprime to every base relatively prime to itso, in theory, either an Euler or a strong probable prime test could prove that a Carmichael number is, in fact, composite.Arnaultgives a 397-digit Carmichael numberN{\\\\displaystyle N}that is a strong pseudoprime to all prime bases less than 307:N=p⋅(313(p−1)+1)⋅(353(p−1)+1){\\\\displaystyle N=p\\\\cdot (313(p-1)+1)\\\\cdot (353(p-1)+1)}wherep={\\\\displaystyle p=}29674495668685510550154174642905332730771991799853043350995075531276838753171770199594238596428121188033664754218345562493168782883is a 131-digit prime.p{\\\\displaystyle p}is the smallest prime factor ofN{\\\\displaystyle N}, so this Carmichael number is also a (not necessarily strong) pseudoprime to all bases less thanp{\\\\displaystyle p}.As numbers become larger, Carmichael numbers become increasingly rare. For example, there are 20,138,200 Carmichael numbers between 1 and 1021 (approximately one in 50 trillion (5·1013) numbers).Korselt\\'s criterionAn alternative and equivalent definition of Carmichael numbers is given by Korselt\\'s criterion.Theorem (A. Korselt 1899): A positive composite integern{\\\\displaystyle n}is a Carmichael number if and only ifn{\\\\displaystyle n}is square-free, and for all prime divisorsp{\\\\displaystyle p}ofn{\\\\displaystyle n}, it is true thatp−1∣n−1{\\\\displaystyle p-1\\\\mid n-1}.It follows from this theorem that all Carmichael numbers are odd, since any even composite number that is square-free (and hence has only one prime factor of two) will have at least one odd prime factor, and thusp−1∣n−1{\\\\displaystyle p-1\\\\mid n-1}results in an even dividing an odd, a contradiction. (The oddness of Carmichael numbers also follows from the fact that−1{\\\\displaystyle -1}is a Fermat witness for any even composite number.)From the criterion it also follows that Carmichael numbers are cyclic. Additionally, it follows that there are no Carmichael numbers with exactly two prime divisors.DiscoveryKorselt was the first who observed the basic properties of Carmichael numbers, but he did not give any examples. In 1910, Carmichael found the first and smallest such number, 561, which explains the name \"Carmichael number\".That 561 is a Carmichael number can be seen with Korselt\\'s criterion. Indeed,561=3⋅11⋅17{\\\\displaystyle 561=3\\\\cdot 11\\\\cdot 17}is square-free and2∣560{\\\\displaystyle 2\\\\mid 560},10∣560{\\\\displaystyle 10\\\\mid 560}and16∣560{\\\\displaystyle 16\\\\mid 560}.The next six Carmichael numbers are (sequence A002997 in the OEIS):1105=5⋅13⋅17(4∣1104;12∣1104;16∣1104){\\\\displaystyle 1105=5\\\\cdot 13\\\\cdot 17\\\\qquad (4\\\\mid 1104;\\\\quad 12\\\\mid 1104;\\\\quad 16\\\\mid 1104)}1729=7⋅13⋅19(6∣1728;12∣1728;18∣1728){\\\\displaystyle 1729=7\\\\cdot 13\\\\cdot 19\\\\qquad (6\\\\mid 1728;\\\\quad 12\\\\mid 1728;\\\\quad 18\\\\mid 1728)}2465=5⋅17⋅29(4∣2464;16∣2464;28∣2464){\\\\displaystyle 2465=5\\\\cdot 17\\\\cdot 29\\\\qquad (4\\\\mid 2464;\\\\quad 16\\\\mid 2464;\\\\quad 28\\\\mid 2464)}2821=7⋅13⋅31(6∣2820;12∣2820;30∣2820){\\\\displaystyle 2821=7\\\\cdot 13\\\\cdot 31\\\\qquad (6\\\\mid 2820;\\\\quad 12\\\\mid 2820;\\\\quad 30\\\\mid 2820)}6601=7⋅23⋅41(6∣6600;22∣6600;40∣6600){\\\\displaystyle 6601=7\\\\cdot 23\\\\cdot 41\\\\qquad (6\\\\mid 6600;\\\\quad 22\\\\mid 6600;\\\\quad 40\\\\mid 6600)}8911=7⋅19⋅67(6∣8910;18∣8910;66∣8910).{\\\\displaystyle 8911=7\\\\cdot 19\\\\cdot 67\\\\qquad (6\\\\mid 8910;\\\\quad 18\\\\mid 8910;\\\\quad 66\\\\mid 8910).}These first seven Carmichael numbers, from 561 to 8911, were all found by the Czech mathematician Václav Šimerka in 1885 (thus preceding not just Carmichael but also Korselt, although Šimerka did not find anything like Korselt\\'s criterion). His work, however, remained unnoticed.J. Chernick proved a theorem in 1939 which can be used to construct a subset of Carmichael numbers. The number(6k+1)(12k+1)(18k+1){\\\\displaystyle (6k+1)(12k+1)(18k+1)}is a Carmichael number if its three factors are all prime. Whether this formula produces an infinite quantity of Carmichael numbers is an open question (though it is implied by Dickson\\'s conjecture).Paul Erdős heuristically argued there should be infinitely many Carmichael numbers. In 1994 W. R. (Red) Alford, Andrew Granville and Carl Pomerance  used a bound on Olson\\'s constant to show that there really do exist infinitely many Carmichael numbers. Specifically, they showed that for sufficiently largen{\\\\displaystyle n}, there are at leastn2/7{\\\\displaystyle n^{2/7}}Carmichael numbers between 1 andn.{\\\\displaystyle n.}Thomas Wright proved that ifa{\\\\displaystyle a}andm{\\\\displaystyle m}are relatively prime,then there are infinitely many Carmichael numbers in the arithmetic progressiona+k⋅m{\\\\displaystyle a+k\\\\cdot m},wherek=1,2,…{\\\\displaystyle k=1,2,\\\\ldots }.Löh and Niebuhr in 1992 found some very large Carmichael numbers, including one with 1,101,518 factors and over 16 million digits.This has been improved to 10,333,229,505 prime factors and 295,486,761,787 digits, so the largest known Carmichael number is much greater than the largest known prime.PropertiesFactorizationsCarmichael numbers have at least three positive prime factors. The first Carmichael numbers withk=3,4,5,…{\\\\displaystyle k=3,4,5,\\\\ldots }prime factors are (sequence A006931 in the OEIS):The first Carmichael numbers with 4 prime factors are (sequence A074379 in the OEIS):The second Carmichael number (1105) can be expressed as the sum of two squares in more ways than any smaller number. The third Carmichael number (1729) is the Hardy-Ramanujan Number: the smallest number that can be expressed as the sum of two cubes (of positive numbers) in two different ways.DistributionLetC(X){\\\\displaystyle C(X)}denote the number of Carmichael numbers less than or equal toX{\\\\displaystyle X}. The distribution of Carmichael numbers by powers of 10 (sequence A055553 in the OEIS):In 1953, Knödel proved the upper bound:C(X)<Xexp\\u2061(−k1(log\\u2061Xlog\\u2061log\\u2061X)12){\\\\displaystyle C(X)<X\\\\exp \\\\left({-k_{1}\\\\left(\\\\log X\\\\log \\\\log X\\\\right)^{\\\\frac {1}{2}}}\\\\right)}for some constantk1{\\\\displaystyle k_{1}}.In 1956, Erdős improved the bound toC(X)<Xexp\\u2061(−k2log\\u2061Xlog\\u2061log\\u2061log\\u2061Xlog\\u2061log\\u2061X){\\\\displaystyle C(X)<X\\\\exp \\\\left({\\\\frac {-k_{2}\\\\log X\\\\log \\\\log \\\\log X}{\\\\log \\\\log X}}\\\\right)}for some constantk2{\\\\displaystyle k_{2}}. He further gave a heuristic argument suggesting that this upper bound should be close to the true growth rate ofC(X){\\\\displaystyle C(X)}.In the other direction, Alford, Granville and Pomerance proved in 1994 that for sufficiently large X,C(X)>X27.{\\\\displaystyle C(X)>X^{\\\\frac {2}{7}}.}In 2005, this bound was further improved by Harman toC(X)>X0.332{\\\\displaystyle C(X)>X^{0.332}}who subsequently improved the exponent to0.7039⋅0.4736=0.33336704>1/3{\\\\displaystyle 0.7039\\\\cdot 0.4736=0.33336704>1/3}.Regarding the asymptotic distribution of Carmichael numbers, there have been several conjectures. In 1956, Erdős conjectured that there wereX1−o(1){\\\\displaystyle X^{1-o(1)}}Carmichael numbers for X sufficiently large. In 1981, Pomerance sharpened Erdős\\' heuristic arguments to conjecture that there are at leastX⋅L(X)−1+o(1){\\\\displaystyle X\\\\cdot L(X)^{-1+o(1)}}Carmichael numbers up toX{\\\\displaystyle X}, whereL(x)=exp\\u2061(log\\u2061xlog\\u2061log\\u2061log\\u2061xlog\\u2061log\\u2061x){\\\\displaystyle L(x)=\\\\exp {\\\\left({\\\\frac {\\\\log x\\\\log \\\\log \\\\log x}{\\\\log \\\\log x}}\\\\right)}}.However, inside current computational ranges (such as the counts of Carmichael numbers performed by Pinch up to 1021), these conjectures are not yet borne out by the data.GeneralizationsThe notion of Carmichael number generalizes to a Carmichael ideal in any number field K. For any nonzero prime idealp{\\\\displaystyle {\\\\mathfrak {p}}}inOK{\\\\displaystyle {\\\\mathcal {O}}_{K}}, we haveαN(p)≡αmodp{\\\\displaystyle \\\\alpha ^{{\\\\rm {N}}({\\\\mathfrak {p}})}\\\\equiv \\\\alpha {\\\\bmod {\\\\mathfrak {p}}}}for allα{\\\\displaystyle \\\\alpha }inOK{\\\\displaystyle {\\\\mathcal {O}}_{K}}, whereN(p){\\\\displaystyle {\\\\rm {N}}({\\\\mathfrak {p}})}is the norm of the idealp{\\\\displaystyle {\\\\mathfrak {p}}}. (This generalizes Fermat\\'s little theorem, thatmp≡mmodp{\\\\displaystyle m^{p}\\\\equiv m{\\\\bmod {p}}}for all integers m when p is prime.) Call a nonzero ideala{\\\\displaystyle {\\\\mathfrak {a}}}inOK{\\\\displaystyle {\\\\mathcal {O}}_{K}}Carmichael if it is not a prime ideal andαN(a)≡αmoda{\\\\displaystyle \\\\alpha ^{{\\\\rm {N}}({\\\\mathfrak {a}})}\\\\equiv \\\\alpha {\\\\bmod {\\\\mathfrak {a}}}}for allα∈OK{\\\\displaystyle \\\\alpha \\\\in {\\\\mathcal {O}}_{K}}, whereN(a){\\\\displaystyle {\\\\rm {N}}({\\\\mathfrak {a}})}is the norm of the ideala{\\\\displaystyle {\\\\mathfrak {a}}}.  When K isQ{\\\\displaystyle \\\\mathbf {Q} }, the ideala{\\\\displaystyle {\\\\mathfrak {a}}}is principal, and if we let a be its positive generator then the ideala=(a){\\\\displaystyle {\\\\mathfrak {a}}=(a)}is Carmichael exactly when a is a Carmichael number in the usual sense.When K is larger than the rationals it is easy to write down Carmichael ideals inOK{\\\\displaystyle {\\\\mathcal {O}}_{K}}: for any prime number p that splits completely in K, the principal idealpOK{\\\\displaystyle p{\\\\mathcal {O}}_{K}}is a Carmichael ideal. Since infinitely many prime numbers split completely in any number field, there are infinitely many Carmichael ideals inOK{\\\\displaystyle {\\\\mathcal {O}}_{K}}. For example, if p is any prime number that is 1 mod 4, the ideal (p) in the Gaussian integers Z[\\u200ai\\u200a] is a Carmichael ideal.Both prime and Carmichael numbers satisfy the following equality:gcd(∑x=1n−1xn−1,n)=1.{\\\\displaystyle \\\\gcd \\\\left(\\\\sum _{x=1}^{n-1}x^{n-1},n\\\\right)=1.}Lucas–Carmichael numberA positive composite integern{\\\\displaystyle n}is a Lucas–Carmichael number if and only ifn{\\\\displaystyle n}is square-free, and for all prime divisorsp{\\\\displaystyle p}ofn{\\\\displaystyle n}, it is true thatp+1∣n+1{\\\\displaystyle p+1\\\\mid n+1}. The first Lucas–Carmichael numbers are:399, 935, 2015, 2915, 4991, 5719, 7055, 8855, 12719, 18095, 20705, 20999, 22847, 29315, 31535, 46079, 51359, 60059, 63503, 67199, 73535, 76751, 80189, 81719, 88559, 90287, ... (sequence A006972 in the OEIS)Quasi–Carmichael numberQuasi–Carmichael numbers are squarefree composite numbers n with the property that for every prime factor p of n, p\\u2009+\\u2009b divides n\\u2009+\\u2009b positively with b being any integer besides 0. If b = −1, these are Carmichael numbers, and if b = 1, these are Lucas–Carmichael numbers. The first Quasi–Carmichael numbers are:35, 77, 143, 165, 187, 209, 221, 231, 247, 273, 299, 323, 357, 391, 399, 437, 493, 527, 561, 589, 598, 713, 715, 899, 935, 943, 989, 1015, 1073, 1105, 1147, 1189, 1247, 1271, 1295, 1333, 1517, 1537, 1547, 1591, 1595, 1705, 1729, ... (sequence A257750 in the OEIS)Knödel numberAn n-Knödel number for a given positive integer n is a composite number m with the property that each i < m coprime to m satisfiesim−n≡1(modm){\\\\displaystyle i^{m-n}\\\\equiv 1{\\\\pmod {m}}}. The n = 1 case are Carmichael numbers.Higher-order Carmichael numbersCarmichael numbers can be generalized using concepts of abstract algebra.The above definition states that a composite integer n is Carmichaelprecisely when the nth-power-raising function pn from the ring Zn of integers modulo n to itself is the identity function. The identity is the only Zn-algebra endomorphism on Zn so we can restate the definition as asking that pn be an algebra endomorphism of Zn.As above, pn satisfies the same property whenever n is prime.The nth-power-raising function pn is also defined on any Zn-algebra A. A theorem states that n is prime if and only if all such functions pn are algebra endomorphisms.In-between these two conditions lies the definition of Carmichael number of order m for any positive integer m as any composite number n such that pn is an endomorphism on every Zn-algebra that can be generated as Zn-module by m elements. Carmichael numbers of order 1 are just the ordinary Carmichael numbers.An order 2 Carmichael numberAccording to Howe, 17 · 31 · 41 · 43 · 89 · 97 · 167 · 331 is an order 2 Carmichael number. This product is equal to 443,372,888,629,441.PropertiesKorselt\\'s criterion can be generalized to higher-order Carmichael numbers, as shown by Howe.A heuristic argument, given in the same paper, appears to suggest that there are infinitely many Carmichael numbers of order m, for any m. However, not a single Carmichael number of order 3 or above is known.NotesReferencesCarmichael, R. D. (1910). \"Note on a new number theory function\". Bulletin of the American Mathematical Society. 16 (5): 232–238. doi:10.1090/s0002-9904-1910-01892-9.Carmichael, R. D. (1912). \"On composite numbers P which satisfy the Fermat congruenceaP−1≡1modP{\\\\displaystyle a^{P-1}\\\\equiv 1{\\\\bmod {P}}}\". American Mathematical Monthly. 19 (2): 22–27. doi:10.2307/2972687. JSTOR 2972687.Chernick, J. (1939). \"On Fermat\\'s simple theorem\" (PDF). Bull. Amer. Math. Soc. 45 (4): 269–274. doi:10.1090/S0002-9904-1939-06953-X.Korselt, A. R. (1899). \"Problème chinois\". L\\'Intermédiaire des Mathématiciens. 6: 142–143.Löh, G.; Niebuhr, W. (1996). \"A new algorithm for constructing large Carmichael numbers\" (PDF). Math. Comp. 65 (214): 823–836. Bibcode:1996MaCom..65..823L. doi:10.1090/S0025-5718-96-00692-8.Ribenboim, P. (1989). The Book of Prime Number Records. Springer. ISBN 978-0-387-97042-4.Šimerka, V. (1885). \"Zbytky z arithmetické posloupnosti (On the remainders of an arithmetic progression)\". Časopis Pro Pěstování Matematiky a Fysiky. 14 (5): 221–225. doi:10.21136/CPMF.1885.122245.External links\"Carmichael number\", Encyclopedia of Mathematics, EMS Press, 2001 [1994]Encyclopedia of MathematicsTable of Carmichael numbersTables of Carmichael numbers with many prime factorsTables of Carmichael numbers below1018{\\\\displaystyle 10^{18}}\"The Dullness of 1729\". MathPages.com.Weisstein, Eric W. \"Carmichael Number\". MathWorld.Final Answers Modular Arithmetic')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "documents = []\n",
        "\n",
        "for fname in tqdm(glob('wiki_data/texts*.txt')):\n",
        "    with open(fname) as f:\n",
        "        document = \"\"\n",
        "        for line in f:\n",
        "            document = document + line.strip()\n",
        "        \n",
        "        documents.append(document)\n",
        "\n",
        "len(documents), documents[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model"
      ],
      "metadata": {
        "id": "NZivmL96lbiv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "c-wjC5unVQoQ"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en\", disable=[\"parser\", \"ner\", \"tagger\"])\n",
        "\n",
        "def spacy_tokenizer(text):\n",
        "    return [t.lemma_ for t in nlp(text)]\n",
        "\n",
        "vec = TfidfVectorizer(tokenizer=spacy_tokenizer)\n",
        "trained_vectors = vec.fit_transform(documents).todense()\n",
        "texts = [[document, vector] for document, vector in zip(documents, trained_vectors)]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = vec.vocabulary_"
      ],
      "metadata": {
        "id": "nB0upILImaN_"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing the index"
      ],
      "metadata": {
        "id": "zesdyejfXyy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer = dict()"
      ],
      "metadata": {
        "id": "z6W-H8wZXx53"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Searching"
      ],
      "metadata": {
        "id": "KVQKy5G7WJTh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "S-Udd4uQxtvz"
      },
      "outputs": [],
      "source": [
        "def lcp(a: str, b:str):\n",
        "    '''\n",
        "    Computes the longest common prefix of the strings `a` and `b`\n",
        "    '''\n",
        "    lcp = 0\n",
        "    while lcp < len(a) and lcp < len(b) and a[lcp] == b[lcp]:\n",
        "        lcp += 1\n",
        "    return lcp\n",
        "\n",
        "def most_similar(text: str):\n",
        "    '''\n",
        "    Returns the most similar word to `text` in the training vocabulary \n",
        "    by using Edit Distance* (ED from now on) and Longest Common Prefix\n",
        "    (LCP from now).\n",
        "\n",
        "    In general:\n",
        "    - the lower the ED, the more similar the words are.\n",
        "    - the greater the LCP, the more similar the words are.\n",
        "\n",
        "    Assumption:\n",
        "    - Typos are more likely to happen in the middle and the end of the words.\n",
        "    That's why the LCP plays a major role in comparing the similarity of words.\n",
        "\n",
        "    Final Similarity criteria**:\n",
        "    - the similarity of two words `a` and `b` is ED(a, b) / exp(LCP(a, b))\n",
        "    \n",
        "\n",
        "    * Edit Distance is also called Levenshtein Distance\n",
        "    ** It might change later\n",
        "    '''\n",
        "\n",
        "    result = [10**9, None] # [similarity, resulting_word]\n",
        "    \n",
        "    for word in all_words:\n",
        "        result = min(result, [nltk.edit_distance(word, text) / np.exp(lcp(word, text)), word])\n",
        "        \n",
        "    return result[1]\n",
        "\n",
        "def do_search(text: str):\n",
        "    text = text.lower()\n",
        "    if text not in all_words:\n",
        "        text = most_similar(text)\n",
        "    if text not in answer: # indexing the result for this text\n",
        "        new_vector = vec.transform([text]).todense()\n",
        "        texts_tmp = [[cosine(vector, new_vector), document] for document, vector in texts]\n",
        "        texts_tmp.sort()\n",
        "        answer[text] = [document for vector, document in texts_tmp[:10]]\n",
        "\n",
        "    return answer[text]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "do_search('Dijtra') # typo on purpose (Dijtra instead of Dijkstra)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pLrQ3IFYnu0",
        "outputId": "ea2cb2af-1338-4cef-85e0-f49d86e04db8"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['= Dijkstra\\'s algorithm =Dijkstra\\'s algorithm ( DYKE-strəz) is an algorithm for finding the shortest paths between nodes in a graph, which may represent, for example, road networks.  It was conceived by computer scientist Edsger W. Dijkstra in 1956 and published three years later.The algorithm exists in many variants. Dijkstra\\'s original algorithm found the shortest path between two given nodes, but a more common variant fixes a single node as the \"source\" node and finds shortest paths from the source to all other nodes in the graph, producing a shortest-path tree.For a given source node in the graph, the algorithm finds the shortest path between that node and every other.:\\u200a196–206\\u200a It can also be used for finding the shortest paths from a single node to a single destination node by stopping the algorithm once the shortest path to the destination node has been determined. For example, if the nodes of the graph represent cities and edge path costs represent driving distances between pairs of cities connected by a direct road (for simplicity, ignore red lights, stop signs, toll roads and other obstructions), Dijkstra\\'s algorithm can be used to find the shortest route between one city and all other cities. A widely used application of shortest path algorithms is network routing protocols, most notably IS-IS (Intermediate System to Intermediate System) and Open Shortest Path First (OSPF). It is also employed as a subroutine in other algorithms such as Johnson\\'s.The Dijkstra algorithm uses labels that are positive integers or real numbers, which are totally ordered.  It can be generalized to use any labels that are partially ordered, provided the subsequent labels (a subsequent label is produced when traversing an edge) are monotonically non-decreasing.  This generalization is called the generic Dijkstra shortest-path algorithm.Dijkstra\\'s algorithm uses a data structure for storing and querying partial solutions sorted by distance from the start. While the original algorithm uses a min-priority queue and runs in timeΘ((|V|+|E|)log\\u2061|V|){\\\\displaystyle \\\\Theta ((|V|+|E|)\\\\log |V|)}(where|V|{\\\\displaystyle |V|}is the number of nodes and|E|{\\\\displaystyle |E|}is the number of edges), it can also be implemented inΘ(|V|2){\\\\displaystyle \\\\Theta (|V|^{2})}using an array. The idea of this algorithm is also given in Leyzorek et al. 1957. Fredman & Tarjan 1984 propose using a Fibonacci heap min-priority queue to optimize the running time complexity toΘ(|E|+|V|log\\u2061|V|){\\\\displaystyle \\\\Theta (|E|+|V|\\\\log |V|)}. This is asymptotically the fastest known single-source shortest-path algorithm for arbitrary directed graphs with unbounded non-negative weights. However, specialized cases (such as bounded/integer weights, directed acyclic graphs etc.) can indeed be improved further as detailed in Specialized variants. Additionally, if preprocessing is allowed algorithms such as contraction hierarchies can be up to seven orders of magnitude faster.In some fields, artificial intelligence in particular, Dijkstra\\'s algorithm or a variant of it is known as uniform cost search and formulated as an instance of the more general idea of best-first search.HistoryWhat is the shortest way to travel from Rotterdam to Groningen, in general: from given city to given city. It is the algorithm for the shortest path, which I designed in about twenty minutes. One morning I was shopping in Amsterdam with my young fiancée, and tired, we sat down on the café terrace to drink a cup of coffee and I was just thinking about whether I could do this, and I then designed the algorithm for the shortest path. As I said, it was a twenty-minute invention. In fact, it was published in \\'59, three years later. The publication is still readable, it is, in fact, quite nice. One of the reasons that it is so nice was that I designed it without pencil and paper. I learned later that one of the advantages of designing without pencil and paper is that you are almost forced to avoid all avoidable complexities. Eventually, that algorithm became to my great amazement, one of the cornerstones of my fame.Dijkstra thought about the shortest path problem when working at the Mathematical Center in Amsterdam in 1956 as a programmer to demonstrate the capabilities of a new computer called ARMAC. His objective was to choose both a problem and a solution (that would be produced by computer) that non-computing people could understand. He designed the shortest path algorithm and later implemented it for ARMAC for a slightly simplified transportation map of 64 cities in the Netherlands (64, so that 6 bits would be sufficient to encode the city number). A year later, he came across another problem from hardware engineers working on the institute\\'s next computer: minimize the amount of wire needed to connect the pins on the back panel of the machine. As a solution, he re-discovered the algorithm known as Prim\\'s minimal spanning tree algorithm (known earlier to Jarník, and also rediscovered by Prim). Dijkstra published the algorithm in 1959, two years after Prim and 29 years after Jarník.AlgorithmLet the node at which we are starting be called the initial node. Let the distance of node Y be the distance from the initial node to Y. Dijkstra\\'s algorithm will initially start with infinite distances and will try to improve them step by step.Mark all nodes unvisited. Create a set of all the unvisited nodes called the unvisited set.Assign to every node a tentative distance value: set it to zero for our initial node and to infinity for all other nodes. The tentative distance of a node v is the length of the shortest path discovered so far between the node v and the starting node. Since initially no path is known to any other vertex than the source itself (which is a path of length zero), all other tentative distances are initially set to infinity. Set the initial node as current.For the current node, consider all of its unvisited neighbors and calculate their tentative distances through the current node. Compare the newly calculated tentative distance to the current assigned value and assign the smaller one. For example, if the current node A is marked with a  distance of 6, and the edge connecting it with a neighbor B has length 2, then the distance to B through A will be 6 + 2 = 8. If B was previously marked with a distance greater than 8 then change it to 8. Otherwise, the current value will be kept.When we are done considering all of the unvisited neighbors of the current node, mark the current node as visited and remove it from the unvisited set. A visited node will never be checked again.If the destination node has been marked visited (when planning a route between two specific nodes) or if the smallest tentative distance among the nodes in the unvisited set is infinity (when planning a complete traversal; occurs when there is no connection between the initial node and remaining unvisited nodes), then stop. The algorithm has finished.Otherwise, select the unvisited node that is marked with the smallest tentative distance, set it as the new current node, and go back to step 3.When planning a route, it is actually not necessary to wait until the destination node is \"visited\" as above: the algorithm can stop once the destination node has the smallest tentative distance among all \"unvisited\" nodes (and thus could be selected as the next \"current\").DescriptionSuppose you would like to find the shortest path between two intersections on a city map: a starting point and a destination. Dijkstra\\'s algorithm initially marks the distance (from the starting point) to every other intersection on the map with infinity. This is done not to imply that there is an infinite distance, but to note that those intersections have not been visited yet. Some variants of this method leave the intersections\\' distances unlabeled. Now select the current intersection at each iteration.  For the first iteration, the current intersection will be the starting point, and the distance to it (the intersection\\'s label) will be zero. For subsequent iterations (after the first), the current intersection will be a closest unvisited intersection to the starting point (this will be easy to find).From the current intersection, update the distance to every unvisited intersection that is directly connected to it. This is done by determining the sum of the distance between an unvisited intersection and the value of the current intersection and then relabeling the unvisited intersection with this value (the sum) if it is less than the unvisited intersection\\'s current value. In effect, the intersection is relabeled if the path to it through the current intersection is shorter than the previously known paths.  To facilitate shortest path identification, in pencil, mark the road with an arrow pointing to the relabeled intersection if you label/relabel it, and erase all others pointing to it.  After you have updated the distances to each neighboring intersection, mark the current intersection as visited and select an unvisited intersection with minimal distance (from the starting point) – or the lowest label—as the current intersection. Intersections marked as visited are labeled with the shortest path from the starting point to it and will not be revisited or returned to.Continue this process of updating the neighboring intersections with the shortest distances, marking the current intersection as visited, and moving onto a closest unvisited intersection until you have marked the destination as visited. Once you have marked the destination as visited (as is the case with any visited intersection), you have determined the shortest path to it from the starting point and can trace your way back following the arrows in reverse. In the algorithm\\'s implementations, this is usually done (after the algorithm has reached the destination node) by following the nodes\\' parents from the destination node up to the starting node; that\\'s why we also keep track of each node\\'s parent.This algorithm makes no attempt of direct \"exploration\" towards the destination as one might expect. Rather, the sole consideration in determining the next \"current\" intersection is its distance from the starting point. This algorithm therefore expands outward from the starting point, interactively considering every node that is closer in terms of shortest path distance until it reaches the destination. When understood in this way, it is clear how the algorithm necessarily finds the shortest path. However, it may also reveal one of the algorithm\\'s weaknesses: its relative slowness in some topologies.PseudocodeIn the following pseudocode algorithm, dist is an array that contains the current distances from the source to other vertices, i.e. dist[u] is the current distance from the source to the vertex u. The prev array contains pointers to previous-hop nodes on the shortest path from source to the given vertex (equivalently, it is the next-hop on the path from the given vertex to the source). The code u ← vertex in Q with min dist[u], searches for the vertex u in the vertex set Q that has the least dist[u] value. Graph.Edges(u, v) returns the length of the edge joining (i.e. the distance between) the two neighbor-nodes u and v. The variable alt on line 14 is the length of the path from the root node to the neighbor node v if it were to go through u. If this path is shorter than the current shortest path recorded for v, that current path is replaced with this alt path.1  function Dijkstra(Graph, source):23      for each vertex v in Graph.Vertices:4          dist[v] ← INFINITY5          prev[v] ← UNDEFINED6          add v to Q7      dist[source] ← 089      while Q is not empty:10          u ← vertex in Q with min dist[u]1112          for each neighbor v of u still in Q:13              alt ← dist[u] + Graph.Edges(u, v)14              if alt < dist[v]:15                  dist[v] ← alt16                  prev[v] ← u1718      return dist[], prev[]If we are only interested in a shortest path between vertices source and target, we can terminate the search after line 10 if u = target.Now we can read the shortest path from source to target by reverse iteration:1  S ← empty sequence2  u ← target3  if prev[u] is defined or u = source:          // Do something only if the vertex is reachable4      while u is defined:                       // Construct the shortest path with a stack S5          insert u at the beginning of S        // Push the vertex onto the stack6          u ← prev[u]                           // Traverse from target to sourceNow sequence S is the list of vertices constituting one of the shortest paths from source to target, or the empty sequence if no path exists.A more general problem would be to find all the shortest paths between source and target (there might be several different ones of the same length). Then instead of storing only a single node in each entry of prev[] we would store all nodes satisfying the relaxation condition. For example, if both r and source connect to target and both of them lie on different shortest paths through target (because the edge cost is the same in both cases), then we would add both r and source to prev[target]. When the algorithm completes, prev[] data structure will actually describe a graph that is a subset of the original graph with some edges removed. Its key property will be that if the algorithm was run with some starting node, then every path from that node to any other node in the new graph will be the shortest path between those nodes in the original graph, and all paths of that length from the original graph will be present in the new graph. Then to actually find all these shortest paths between two given nodes we would use a path finding algorithm on the new graph, such as depth-first search.Using a priority queueA min-priority queue is an abstract data type that provides 3 basic operations: add_with_priority(), decrease_priority() and extract_min(). As mentioned earlier, using such a data structure can lead to faster computing times than using a basic queue. Notably, Fibonacci heap (Fredman & Tarjan 1984) or Brodal queue offer optimal implementations for those 3 operations. As the algorithm is slightly different, we mention it here, in pseudo-code as well :1  function Dijkstra(Graph, source):2      dist[source] ← 0                           // Initialization34      create vertex priority queue Q56      for each vertex v in Graph.Vertices:7          if v ≠ source8              dist[v] ← INFINITY                 // Unknown distance from source to v9              prev[v] ← UNDEFINED                // Predecessor of v1011         Q.add_with_priority(v, dist[v])121314     while Q is not empty:                      // The main loop15         u ← Q.extract_min()                    // Remove and return best vertex16         for each neighbor v of u:              // only v that are still in Q17             alt ← dist[u] + Graph.Edges(u, v)18             if alt < dist[v]19                 dist[v] ← alt20                 prev[v] ← u21                 Q.decrease_priority(v, alt)2223     return dist, prevInstead of filling the priority queue with all nodes in the initialization phase, it is also possible to initialize it to contain only source; then, inside the if alt < dist[v] block, the decrease_priority() becomes an add_with_priority() operation if the node is not already in the queue.:\\u200a198\\u200aYet another alternative is to add nodes unconditionally to the priority queue and to instead check after extraction that no shorter connection was found yet. This can be done by additionally extracting the associated priority p from the queue and only processing further if p == dist[u] inside the while Q is not empty loop. These alternatives can use entirely array-based priority queues without decrease-key functionality, which have been found to achieve even faster computing times in practice. However, the difference in performance was found to be narrower for denser graphs.Proof of correctnessProof of Dijkstra\\'s algorithm is constructed by induction on the number of visited nodes.Invariant hypothesis: For each node v, dist[v] is the shortest distance from source to v when traveling via visited nodes only, or infinity if no such path exists. (Note: we do not assume dist[v] is the actual shortest distance for unvisited nodes.)The base case is when there is just one visited node, namely the initial node source, in which case the hypothesis is trivial.Otherwise, assume the hypothesis for n-1 visited nodes. In which case, we choose an edge vu where u has the least dist[u] of any unvisited nodes such that dist[u] = dist[v] + Graph.Edges[v,u]. dist[u] is considered to be the shortest distance from source to u because if there were a shorter path, and if w was the first unvisited node on that path then by the original hypothesis dist[w] > dist[u] which creates a contradiction. Similarly if there were a shorter path to u without using unvisited nodes, and if the last but one node on that path were w, then we would have had dist[u] = dist[w] + Graph.Edges[w,u], also a contradiction.After processing u it will still be true that for each unvisited node w, dist[w] will be the shortest distance from source to w using visited nodes only, because if there were a shorter path that doesn\\'t go by u we would have found it previously, and if there were a shorter path using u we would have updated it when processing u.After all nodes are visited, the shortest path from source to any node v consists only of visited nodes, therefore dist[v] is the shortest distance.Running timeBounds of the running time of Dijkstra\\'s algorithm on a graph with edges E and vertices V can be expressed as a function of the number of edges, denoted|E|{\\\\displaystyle |E|}, and the number of vertices, denoted|V|{\\\\displaystyle |V|}, using big-O notation. The complexity bound depends mainly on the data structure used to represent the set Q. In the following, upper bounds can be simplified because|E|{\\\\displaystyle |E|}isO(|V|2){\\\\displaystyle O(|V|^{2})}for any graph, but that simplification disregards the fact that in some problems, other upper bounds on|E|{\\\\displaystyle |E|}may hold.For any data structure for the vertex set Q, the running time is inΘ(|E|⋅Tdk+|V|⋅Tem),{\\\\displaystyle \\\\Theta (|E|\\\\cdot T_{\\\\mathrm {dk} }+|V|\\\\cdot T_{\\\\mathrm {em} }),}whereTdk{\\\\displaystyle T_{\\\\mathrm {dk} }}andTem{\\\\displaystyle T_{\\\\mathrm {em} }}are the complexities of the decrease-key and extract-minimum operations in Q, respectively.The simplest version of Dijkstra\\'s algorithm stores the vertex set Q as an linked list or array, and edges as an adjacency list or matrix. In this case, extract-minimum is simply a linear search through all vertices in Q, so the running time isΘ(|E|+|V|2)=Θ(|V|2){\\\\displaystyle \\\\Theta (|E|+|V|^{2})=\\\\Theta (|V|^{2})}.For sparse graphs, that is, graphs with far fewer than|V|2{\\\\displaystyle |V|^{2}}edges, Dijkstra\\'s algorithm can be implemented more efficiently by storing the graph in the form of adjacency lists and using a self-balancing binary search tree, binary heap, pairing heap, or Fibonacci heap as a priority queue to implement extracting minimum efficiently. To perform decrease-key steps in a binary heap efficiently, it is necessary to use an auxiliary data structure that maps each vertex to its position in the heap, and to keep this structure up to date as the priority queue Q changes. With a self-balancing binary search tree or binary heap, the algorithm requiresΘ((|E|+|V|)log\\u2061|V|){\\\\displaystyle \\\\Theta ((|E|+|V|)\\\\log |V|)}time in the worst case (wherelog{\\\\displaystyle \\\\log }denotes the binary logarithmlog2{\\\\displaystyle \\\\log _{2}}); for connected graphs this time bound can be simplified toΘ(|E|log\\u2061|V|){\\\\displaystyle \\\\Theta (|E|\\\\log |V|)}.  The Fibonacci heap improves this toΘ(|E|+|V|log\\u2061|V|).{\\\\displaystyle \\\\Theta (|E|+|V|\\\\log |V|).}When using binary heaps, the average case time complexity is lower than the worst-case: assuming edge costs are drawn independently from a common probability distribution, the expected number of decrease-key operations is bounded byΘ(|V|log\\u2061(|E|/|V|)){\\\\displaystyle \\\\Theta (|V|\\\\log(|E|/|V|))}, giving a total running time of:\\u200a199–200O(|E|+|V|log\\u2061|E||V|log\\u2061|V|).{\\\\displaystyle O\\\\left(|E|+|V|\\\\log {\\\\frac {|E|}{|V|}}\\\\log |V|\\\\right).}Practical optimizations and infinite graphsIn common presentations of Dijkstra\\'s algorithm, initially all nodes are entered into the priority queue. This is, however, not necessary: the algorithm can start with a priority queue that contains only one item, and insert new items as they are discovered (instead of doing a decrease-key, check whether the key is in the queue; if it is, decrease its key, otherwise insert it).:\\u200a198\\u200a This variant has the same worst-case bounds as the common variant, but maintains a smaller priority queue in practice, speeding up the queue operations.Moreover, not inserting all nodes in a graph makes it possible to extend the algorithm to find the shortest path from a single source to the closest of a set of target nodes on infinite graphs or those too large to represent in memory. The resulting algorithm is called uniform-cost search (UCS) in the artificial intelligence literature and can be expressed in pseudocode asprocedure uniform_cost_search(start) isnode ← startfrontier ← priority queue containing node onlyexplored ← empty setdoif frontier is empty thenreturn failurenode ← frontier.pop()if node is a goal state thenreturn solution(node)explored.add(node)for each of node\\'s neighbors n doif n is not in explored and not in frontier thenfrontier.add(n)else if n is in frontier with higher costreplace existing node with nThe complexity of this algorithm can be expressed in an alternative way for very large graphs: when C* is the length of the shortest path from the start node to any node satisfying the \"goal\" predicate, each edge has cost at least ε, and the number of neighbors per node is bounded by b, then the algorithm\\'s worst-case time and space complexity are both in O(b1+⌊C* ⁄ ε⌋).Further optimizations of Dijkstra\\'s algorithm for the single-target case include bidirectional variants, goal-directed variants such as the A* algorithm (see § Related problems and algorithms), graph pruning to determine which nodes are likely to form the middle segment of shortest paths (reach-based routing), and hierarchical decompositions of the input graph that reduce s–t routing to connecting s and t to their respective \"transit nodes\" followed by shortest-path computation between these transit nodes using a \"highway\".Combinations of such techniques may be needed for optimal practical performance on specific problems.Specialized variantsWhen arc weights are small integers (bounded by a parameterC{\\\\displaystyle C}), specialized queues which take advantage of this fact can be used to speed up Dijkstra\\'s algorithm. The first algorithm of this type was Dial\\'s algorithm (Dial 1969) for graphs with positive integer edge weights, which uses a bucket queue to obtain a running timeO(|E|+|V|C){\\\\displaystyle O(|E|+|V|C)}. The use of a Van Emde Boas tree as the priority queue brings the complexity toO(|E|log\\u2061log\\u2061C){\\\\displaystyle O(|E|\\\\log \\\\log C)}(Ahuja et al. 1990). Another interesting variant based on a combination of a new radix heap and the well-known Fibonacci heap runs in timeO(|E|+|V|log\\u2061C){\\\\displaystyle O(|E|+|V|{\\\\sqrt {\\\\log C}})}(Ahuja et al. 1990). Finally, the best algorithms in this special case are as follows. The algorithm given by (Thorup 2000) runs inO(|E|log\\u2061log\\u2061|V|){\\\\displaystyle O(|E|\\\\log \\\\log |V|)}time and the algorithm given by (Raman 1997) runs inO(|E|+|V|min{(log\\u2061|V|)1/3+ε,(log\\u2061C)1/4+ε}){\\\\displaystyle O(|E|+|V|\\\\min\\\\{(\\\\log |V|)^{1/3+\\\\varepsilon },(\\\\log C)^{1/4+\\\\varepsilon }\\\\})}time.Related problems and algorithmsThe functionality of Dijkstra\\'s original algorithm can be extended with a variety of modifications. For example, sometimes it is desirable to present solutions which are less than mathematically optimal. To obtain a ranked list of less-than-optimal solutions, the optimal solution is first calculated. A single edge appearing in the optimal solution is removed from the graph, and the optimum solution to this new graph is calculated. Each edge of the original solution is suppressed in turn and a new shortest-path calculated. The secondary solutions are then ranked and presented after the first optimal solution.Dijkstra\\'s algorithm is usually the working principle behind link-state routing protocols, OSPF and IS-IS being the most common ones.Unlike Dijkstra\\'s algorithm, the Bellman–Ford algorithm can be used on graphs with negative edge weights, as long as the graph contains no negative cycle reachable from the source vertex s. The presence of such cycles means there is no shortest path, since the total weight becomes lower each time the cycle is traversed. (This statement assumes that a \"path\" is allowed to repeat vertices. In graph theory that is normally not allowed.  In theoretical computer science it often is allowed.) It is possible to adapt Dijkstra\\'s algorithm to handle negative weight edges by combining it with the Bellman-Ford algorithm (to remove negative edges and detect negative cycles), such an algorithm is called Johnson\\'s algorithm.The A* algorithm is a generalization of Dijkstra\\'s algorithm that cuts down on the size of the subgraph that must be explored, if additional information is available that provides a lower bound on the \"distance\" to the target. This approach can be viewed from the perspective of linear programming: there is a natural linear program for computing shortest paths, and solutions to its dual linear program are feasible if and only if they form a consistent heuristic (speaking roughly, since the sign conventions differ from place to place in the literature). This feasible dual / consistent heuristic defines a non-negative reduced cost and A* is essentially running Dijkstra\\'s algorithm with these reduced costs. If the dual satisfies the weaker condition of admissibility, then A* is instead more akin to the Bellman–Ford algorithm.The process that underlies Dijkstra\\'s algorithm is similar to the greedy process used in Prim\\'s algorithm.  Prim\\'s purpose is to find a minimum spanning tree that connects all nodes in the graph; Dijkstra is concerned with only two nodes. Prim\\'s does not evaluate the total weight of the path from the starting node, only the individual edges.Breadth-first search can be viewed as a special-case of Dijkstra\\'s algorithm on unweighted graphs, where the priority queue degenerates into a FIFO queue.The fast marching method can be viewed as a continuous version of Dijkstra\\'s algorithm which computes the geodesic distance on a triangle mesh.Dynamic programming perspectiveFrom a dynamic programming point of view, Dijkstra\\'s algorithm is a successive approximation scheme that solves the dynamic programming functional equation for the shortest path problem by the Reaching method.In fact, Dijkstra\\'s explanation of the logic behind the algorithm, namelyProblem 2. Find the path of minimum total length between two given nodesP{\\\\displaystyle P}andQ{\\\\displaystyle Q}.We use the fact that, ifR{\\\\displaystyle R}is a node on the minimal path fromP{\\\\displaystyle P}toQ{\\\\displaystyle Q}, knowledge of the latter implies the knowledge of the minimal path fromP{\\\\displaystyle P}toR{\\\\displaystyle R}.is a paraphrasing of Bellman\\'s famous Principle of Optimality in the context of the shortest path problem.ApplicationsLeast-cost paths are calculated for instance to establish tracks of electricity lines or oil pipelines. The algorithm has also been used to calculate optimal long-distance footpaths in Ethiopia and contrast them with the situation on the ground.See alsoA* search algorithmBellman–Ford algorithmEuclidean shortest pathFloyd–Warshall algorithmJohnson\\'s algorithmLongest path problemParallel all-pairs shortest path algorithmNotesReferencesCormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L.; Stein, Clifford (2001). \"Section 24.3: Dijkstra\\'s algorithm\". Introduction to Algorithms (Second ed.). MIT Press and McGraw–Hill. pp. 595–601. ISBN 0-262-03293-7.Dial, Robert B. (1969). \"Algorithm 360: Shortest-path forest with topological ordering [H]\". Communications of the ACM. 12 (11): 632–633. doi:10.1145/363269.363610. S2CID 6754003.Fredman, Michael Lawrence; Tarjan, Robert E. (1984). Fibonacci heaps and their uses in improved network optimization algorithms. 25th Annual Symposium on Foundations of Computer Science. IEEE. pp. 338–346. doi:10.1109/SFCS.1984.715934.Fredman, Michael Lawrence; Tarjan, Robert E. (1987). \"Fibonacci heaps and their uses in improved network optimization algorithms\". Journal of the Association for Computing Machinery. 34 (3): 596–615. doi:10.1145/28869.28874. S2CID 7904683.Zhan, F. Benjamin; Noon, Charles E. (February 1998). \"Shortest Path Algorithms: An Evaluation Using Real Road Networks\". Transportation Science. 32 (1): 65–73. doi:10.1287/trsc.32.1.65. S2CID 14986297.Leyzorek, M.; Gray, R. S.; Johnson, A. A.; Ladew, W. C.; Meaker, Jr., S. R.; Petry, R. M.; Seitz, R. N. (1957). Investigation of Model Techniques – First Annual Report – 6 June 1956 – 1 July 1957 – A Study of Model Techniques for Communication Systems. Cleveland, Ohio: Case Institute of Technology.Knuth, D.E. (1977). \"A Generalization of Dijkstra\\'s Algorithm\". Information Processing Letters. 6 (1): 1–5. doi:10.1016/0020-0190(77)90002-3.Ahuja, Ravindra K.; Mehlhorn, Kurt; Orlin, James B.; Tarjan, Robert E. (April 1990). \"Faster Algorithms for the Shortest Path Problem\" (PDF). Journal of the ACM. 37 (2): 213–223. doi:10.1145/77600.77615. hdl:1721.1/47994. S2CID 5499589.Raman, Rajeev (1997). \"Recent results on the single-source shortest paths problem\". SIGACT News. 28 (2): 81–87. doi:10.1145/261342.261352. S2CID 18031586.Thorup, Mikkel (2000). \"On RAM priority Queues\". SIAM Journal on Computing. 30 (1): 86–109. doi:10.1137/S0097539795288246. S2CID 5221089.Thorup, Mikkel (1999). \"Undirected single-source shortest paths with positive integer weights in linear time\". Journal of the ACM. 46 (3): 362–394. doi:10.1145/316542.316548. S2CID 207654795.External linksOral history interview with Edsger W. Dijkstra, Charles Babbage Institute, University of Minnesota, MinneapolisImplementation of Dijkstra\\'s algorithm using TDD, Robert Cecil Martin, The Clean Code Blog',\n",
              " \"= Johnson's algorithm =Johnson's algorithm is a way to find the shortest paths between all pairs of vertices in an edge-weighted directed graph. It allows some of the edge weights to be negative numbers, but no negative-weight cycles may exist. It works by using the Bellman–Ford algorithm to compute a transformation of the input graph that removes all negative weights, allowing Dijkstra's algorithm to be used on the transformed graph. It is named after Donald B. Johnson, who first published the technique in 1977.A similar reweighting technique is also used in Suurballe's algorithm for finding two disjoint paths of minimum total length between the same two vertices in a graph with non-negative edge weights.Algorithm descriptionJohnson's algorithm consists of the following steps:First, a new node q is added to the graph, connected by zero-weight edges to each of the other nodes.Second, the Bellman–Ford algorithm is used, starting from the new vertex q, to find for each vertex v the minimum weight h(v) of a path from q to v. If this step detects a negative cycle, the algorithm is terminated.Next the edges of the original graph are reweighted using the values computed by the Bellman–Ford algorithm: an edge from u to v, having lengthw(u,v){\\\\displaystyle w(u,v)}, is given the new length w(u,v) + h(u) − h(v).Finally, q is removed, and Dijkstra's algorithm is used to find the shortest paths from each node s to every other vertex in the reweighted graph. The distance in the original graph is then computed for each distance D(u , v), by adding h(v) − h(u) to the distance returned by Dijkstra's algorithm.ExampleThe first three stages of Johnson's algorithm are depicted in the illustration below.The graph on the left of the illustration has two negative edges, but no negative cycles. The center graph shows the new vertex q, a shortest path tree as computed by the Bellman–Ford algorithm with q as starting vertex, and the values h(v) computed at each other node as the length of the shortest path from q to that node. Note that these values are all non-positive, because q has a length-zero edge to each vertex and the shortest path can be no longer than that edge. On the right is shown the reweighted graph, formed by replacing each edge weightw(u,v){\\\\displaystyle w(u,v)}by w(u,v) + h(u) − h(v). In this reweighted graph, all edge weights are non-negative, but the shortest path between any two nodes uses the same sequence of edges as the shortest path between the same two nodes in the original graph. The algorithm concludes by applying Dijkstra's algorithm to each of the four starting nodes in the reweighted graph.CorrectnessIn the reweighted graph, all paths between a pair s and t of nodes have the same quantity h(s) − h(t) added to them. The previous statement can be proven as follows: Let p be ans−t{\\\\displaystyle s-t}path. Its weight W in the reweighted graph is given by the following expression:(w(s,p1)+h(s)−h(p1))+(w(p1,p2)+h(p1)−h(p2))+...+(w(pn,t)+h(pn)−h(t)).{\\\\displaystyle {\\\\bigl (}w(s,p_{1})+h(s)-h(p_{1}){\\\\bigr )}+{\\\\bigl (}w(p_{1},p_{2})+h(p_{1})-h(p_{2}){\\\\bigr )}+...+{\\\\bigl (}w(p_{n},t)+h(p_{n})-h(t){\\\\bigr )}.}Every+h(pi){\\\\displaystyle +h(p_{i})}is cancelled by−h(pi){\\\\displaystyle -h(p_{i})}in the previous bracketed expression; therefore, we are left with the following expression for W:(w(s,p1)+w(p1,p2)+...+w(pn,t))+h(s)−h(t){\\\\displaystyle {\\\\bigl (}w(s,p_{1})+w(p_{1},p_{2})+...+w(p_{n},t){\\\\bigr )}+h(s)-h(t)}The bracketed expression is the weight of p in the original weighting.Since the reweighting adds the same amount to the weight of everys−t{\\\\displaystyle s-t}path, a path is a shortest path in the original weighting if and only if it is a shortest path after reweighting. The weight of edges that belong to a shortest path from q to any node is zero, and therefore the lengths of the shortest paths from q to every node become zero in the reweighted graph; however, they still remain shortest paths. Therefore, there can be no negative edges: if edge uv had a negative weight after the reweighting, then the zero-length path from q to u together with this edge would form a negative-length path from q to v, contradicting the fact that all vertices have zero distance from q. The non-existence of negative edges ensures the optimality of the paths found by Dijkstra's algorithm. The distances in the original graph may be calculated from the distances calculated by Dijkstra's algorithm in the reweighted graph by reversing the reweighting transformation.AnalysisThe time complexity of this algorithm, using Fibonacci heaps in the implementation of Dijkstra's algorithm, isO(|V|2log\\u2061|V|+|V||E|){\\\\displaystyle O(|V|^{2}\\\\log |V|+|V||E|)}: the algorithm usesO(|V||E|){\\\\displaystyle O(|V||E|)}time for the Bellman–Ford stage of the algorithm, andO(|V|log\\u2061|V|+|E|){\\\\displaystyle O(|V|\\\\log |V|+|E|)}for each of the|V|{\\\\displaystyle |V|}instantiations of Dijkstra's algorithm. Thus, when the graph is sparse, the total time can be faster than the Floyd–Warshall algorithm, which solves the same problem in timeO(|V|3){\\\\displaystyle O(|V|^{3})}.ReferencesExternal linksBoost: All Pairs Shortest Paths\",\n",
              " \"= Dijkstra–Scholten algorithm =The Dijkstra–Scholten algorithm (named after Edsger W. Dijkstra and Carel S. Scholten) is an algorithm for detecting termination in a distributed system. The algorithm was proposed by Dijkstra and Scholten in 1980.First, consider the case of a simple process graph which is a tree. A distributed computation which is tree-structured is not uncommon. Such a process graph may arise when the computation is strictly a divide-and-conquer type. A node starts the computation and divides the problem in two (or more, usually a multiple of 2) roughly equal parts and distribute those parts to other processors. This process continues recursively until the problems are of sufficiently small size to solve in a single processor.AlgorithmThe Dijkstra–Scholten algorithm is a tree-based algorithm which can be described by the following:The initiator of a computation is the root of the tree.Upon receiving a computational message:If the receiving process is currently not in the computation: the process joins the tree by becoming a child of the sender of the message. (No acknowledgment message is sent at this point.)If the receiving process is already in the computation: the process immediately sends an acknowledgment message to the sender of the message.When a process has no more children and has become idle, the process detaches itself from the tree by sending an acknowledgment message to its tree parent.Termination occurs when the initiator has no children and has become idle.Dijkstra–Scholten algorithm for a treeFor a tree, it is easy to detect termination. When a leaf process determines that it has terminated, it sends a signal to its parent. In general, a process waits for all its children to send signals and then it sends a signal to its parent.The program terminates when the root receives signals from all its children.Dijkstra–Scholten algorithm for directed acyclic graphsThe algorithm for a tree can be extended to acyclic directed graphs. We add an additional integer attribute Deficit to each edge.On an incoming edge, Deficit will denote the difference between the number of messages received and the number of signals sent in reply.When a node wishes to terminate, it waits until it has received signals from outgoing edges reducing their deficits to zero.Then it sends enough signals to ensure that the deficit is zero on each incoming edge.Since the graph is acyclic, some nodes will have no outgoing edges and these nodes will be the first to terminate after sending enough signals to their incoming edges. After that the nodes at higher levels will terminate level by level.Dijkstra–Scholten algorithm for cyclic directed graphsIf cycles are allowed, the previous algorithm does not work. This is because, there may not be any node with zero outgoing edges. So, potentially there is no node which can terminate without consulting other nodes.The Dijkstra–Scholten algorithm solves this problem by implicitly creating a spanning tree of the graph. A spanning-tree is a tree which includes each node of the underlying graph once and the edge-set is a subset of the original set of edges.The tree will be directed (i.e., the channels will be directed) with the source node (which initiates the computation) as the root.The spanning-tree is created in the following way. A variable First_Edge is added to each node. When a node receives a message for the first time, it initializes First_Edge with the edge through which it received the message. First_Edge is never changed afterwards. Note that, the spanning tree is not unique and it depends on the order of messages in the system.Termination is handled by each node in three steps :Send signals on all incoming edges except the first edge. (Each node will send signals which reduces the deficit on each incoming edge to zero.)Wait for signals from all outgoing edges. (The number of signals received on each outgoing edge should reduce each of their deficits to zero.)Send signals on First_Edge. (Once steps 1 and 2 are complete, a node informs its parent in the spanning tree about its intention of terminating.)See alsoHuang's algorithm== References ==\",\n",
              " '= Suurballe\\'s algorithm =In theoretical computer science and network routing, Suurballe\\'s algorithm is an algorithm for finding two disjoint paths in a nonnegatively-weighted directed graph, so that both paths connect the same pair of vertices and have minimum total length. The algorithm was conceived by John W. Suurballe and published in 1974. The main idea of Suurballe\\'s algorithm is to use Dijkstra\\'s algorithm to find one path, to modify the weights of the graph edges, and then to run Dijkstra\\'s algorithm a second time. The output of the algorithm is formed by combining these two paths, discarding edges that are traversed in opposite directions by the paths, and using the remaining edges to form the two paths to return as the output.The modification to the weights is similar to the weight modification in Johnson\\'s algorithm, and preserves the non-negativity of the weights while allowing the second instance of Dijkstra\\'s algorithm to find the correct second path.The problem of finding two disjoint paths of minimum weight can be seen as a special case of a minimum cost flow problem, where in this case there are two units of \"flow\" and nodes have unit \"capacity\". Suurballe\\'s algorithm, also, can be seen as a special case of a minimum cost flow algorithm that repeatedly pushes the maximum possible amount of flow along a shortest augmenting path.The first path found by Suurballe\\'s algorithm is the shortest augmenting path for the initial (zero) flow, and the second path found by Suurballe\\'s algorithm is the shortest augmenting path for the residual graph left after pushing one unit of flow along the first path.DefinitionsLet G be a weighted directed graph with vertex set  V and edge set  E (figure A); let  s be a designated source vertex in  G, and let  t be a designated destination vertex. Let each edge  (u,v) in  E, from vertex  u to vertex  v, have a non-negative cost  w(u,v).Define d(s,u) to be the cost of the shortest path to vertex u from vertex s in the shortest path tree rooted at s (figure C).Note: Node and Vertex are often used interchangeably.AlgorithmSuurballe\\'s algorithm performs the following steps:Find the shortest path tree T rooted at node s by running Dijkstra\\'s algorithm (figure C). This tree contains for every vertex u, a shortest path from s to u. Let P1 be the shortest cost path from s to t (figure B). The edges in T are called tree edges and the remaining edges (the edges missing from figure C) are called non-tree edges.Modify the cost of each edge in the graph by replacing the cost w(u,v) of every edge (u,v) by w′(u,v) = w(u,v) − d(s,v) + d(s,u). According to the resulting modified cost function, all tree edges have a cost of 0, and non-tree edges have a non-negative cost. For example:  If u=B, v=E, then w′(u,v) = w(B,E) − d(A,E) + d(A,B) = 2 − 3 + 1 = 0  If u=E, v=B, then w′(u,v) = w(E,B) − d(A,B) + d(A,E) = 2 − 1 + 3 = 4Create a residual graph Gt formed from G by removing the edges of G on path P1 that are directed into s and then reverse the direction of the zero length edges along path P1 (figure D).Find the shortest path P2 in the residual graph Gt by running Dijkstra\\'s algorithm (figure E).Discard the reversed edges of P2 from both paths. The remaining edges of P1 and P2 form a subgraph with two outgoing edges at s, two incoming edges at t, and one incoming and one outgoing edge at each remaining vertex. Therefore, this subgraph consists of two edge-disjoint paths from s to t and possibly some additional (zero-length) cycles. Return the two disjoint paths from the subgraph.ExampleThe following example shows how Suurballe\\'s algorithm finds the shortest pair of disjoint paths from A to F.Figure A illustrates a weighted graph G.Figure B calculates the shortest path P1 from A to F (A–B–D–F).Figure C illustrates the shortest path tree T rooted at A, and the computed distances from A to every vertex (u).Figure D shows the residual graph Gt with the updated cost of each edge and the edges of path P1 reversed.Figure E calculates path P2 in the residual graph Gt (A–C–D–B–E–F).Figure F illustrates both path P1 and path P2.Figure G finds the shortest pair of disjoint paths by combining the edges of paths P1 and P2 and then discarding the common reversed edges between both paths (B–D). As a result, we get the two shortest pair of disjoint paths (A–B–E–F) and (A–C–D–F).CorrectnessThe weight of any path from s to t in the modified system of weights equals the weight in the original graph, minus d(s,t). Therefore, the shortest two disjoint paths under the modified weights are the same paths as the shortest two paths in the original graph, although they have different weights.Suurballe\\'s algorithm may be seen as a special case of the successive shortest paths method for finding a minimum cost flow with total flow amount two from s to t. The modification to the weights does not affect the sequence of paths found by this method, only their weights. Therefore, the correctness of the algorithm follows from the correctness of the successive shortest paths method.Analysis and running timeThis algorithm requires two iterations of Dijkstra\\'s algorithm. Using Fibonacci heaps, both iterations can be performed in timeO(|E|+|V|log\\u2061|V|){\\\\displaystyle O(|E|+|V|\\\\log |V|)}where|V|{\\\\displaystyle |V|}and|E|{\\\\displaystyle |E|}are the number of vertices and edges respectively. Therefore, the same time bound applies to Suurballe\\'s algorithm.VariationsThe version of Suurballe\\'s algorithm as described above finds paths that have disjoint edges, but that may share vertices. It is possible to use the same algorithm to find vertex-disjoint paths, by replacing each vertex by a pair of adjacent vertices, one with all of the incoming adjacencies  u-in of the original vertex, and one with all of the outgoing adjacencies  u-out. Two edge-disjoint paths in this modified graph necessarily correspond to two vertex-disjoint paths in the original graph, and vice versa, so applying Suurballe\\'s algorithm to the modified graph results in the construction of two vertex-disjoint paths in the original graph. Suurballe\\'s original 1974 algorithm was for the vertex-disjoint version of the problem, and was extended in 1984 by Suurballe and Tarjan to the edge-disjoint version.By using a modified version of Dijkstra\\'s algorithm that simultaneously computes the distances to each vertex t in the graphs Gt, it is also possible to find the total lengths of the shortest pairs of paths from a given source vertex s to every other vertex in the graph, in an amount of time that is proportional to a single instance of Dijkstra\\'s algorithm.Note: The pair of adjacent vertices resulting from the split are connected by a zero cost uni-directional edge from the incoming to outgoing vertex. The source vertex becomes s-out and the destination vertex becomes t-in.See alsoEdge disjoint shortest pair algorithm== References ==',\n",
              " '= Yen\\'s algorithm =Yen\\'s algorithm computes single-source K-shortest loopless paths for a graph with non-negative edge cost.  The algorithm was published by Jin Y. Yen in 1971 and employs any shortest path algorithm to find the best path, then proceeds to find K − 1 deviations of the best path.AlgorithmTerminology and notationDescriptionThe algorithm can be broken down into two parts, determining the first k-shortest path,A1{\\\\displaystyle A^{1}}, and then determining all other k-shortest paths. It is assumed that the containerA{\\\\displaystyle A}will hold the k-shortest path, whereas the containerB{\\\\displaystyle B}, will hold the potential k-shortest paths. To determineA1{\\\\displaystyle A^{1}}, the shortest path from the source to the sink, any efficient shortest path algorithm can be used.To find theAk{\\\\displaystyle A^{k}}, wherek{\\\\displaystyle k}ranges from2{\\\\displaystyle 2}toK{\\\\displaystyle K}, the algorithm assumes that all paths fromA1{\\\\displaystyle A^{1}}toAk−1{\\\\displaystyle A^{k-1}}have previously been found. Thek{\\\\displaystyle k}iteration can be divided into two processes, finding all the deviationsAki{\\\\displaystyle {A^{k}}_{i}}and choosing a minimum length path to becomeAk{\\\\displaystyle A^{k}}. Note that in this iteration,i{\\\\displaystyle i}ranges from1{\\\\displaystyle 1}toQkk{\\\\displaystyle {Q^{k}}_{k}}.The first process can be further subdivided into three operations, choosing theRki{\\\\displaystyle {R^{k}}_{i}}, findingSki{\\\\displaystyle {S^{k}}_{i}}, and then addingAki{\\\\displaystyle {A^{k}}_{i}}to the containerB{\\\\displaystyle B}. The root path,Rki{\\\\displaystyle {R^{k}}_{i}}, is chosen by finding the subpath inAk−1{\\\\displaystyle A^{k-1}}that follows the firsti{\\\\displaystyle i}nodes ofAj{\\\\displaystyle A^{j}}, wherej{\\\\displaystyle j}ranges from1{\\\\displaystyle 1}tok−1{\\\\displaystyle k-1}. Then, if a path is found, the cost of edgedi(i+1){\\\\displaystyle d_{i(i+1)}}ofAj{\\\\displaystyle A^{j}}is set to infinity. Next, the spur path,Ski{\\\\displaystyle {S^{k}}_{i}}, is found by computing the shortest path from the spur node, nodei{\\\\displaystyle i}, to the sink. The removal of previous used edges from(i){\\\\displaystyle (i)}to(i+1){\\\\displaystyle (i+1)}ensures that the spur path is different.Aki=Rki+Ski{\\\\displaystyle {A^{k}}_{i}={R^{k}}_{i}+{S^{k}}_{i}}, the addition of the root path and the spur path, is added toB{\\\\displaystyle B}. Next, the edges that were removed, i.e. had their cost set to infinity, are restored to their initial values.The second process determines a suitable path forAk{\\\\displaystyle A^{k}}by finding the path in containerB{\\\\displaystyle B}with the lowest cost. This path is removed from containerB{\\\\displaystyle B}and inserted into containerA{\\\\displaystyle A}and the algorithm continues to the next iteration.PseudocodeThe algorithm assumes that the Dijkstra algorithm is used to find the shortest path between two nodes, but any shortest path algorithm can be used in its place.function YenKSP(Graph, source, sink, K):// Determine the shortest path from the source to the sink.A[0] = Dijkstra(Graph, source, sink);// Initialize the set to store the potential kth shortest path.B = [];for k from 1 to K:// The spur node ranges from the first node to the next to last node in the previous k-shortest path.for i from 0 to size(A[k − 1]) − 2:// Spur node is retrieved from the previous k-shortest path, k − 1.spurNode = A[k-1].node(i);// The sequence of nodes from the source to the spur node of the previous k-shortest path.rootPath = A[k-1].nodes(0, i);for each path p in A:if rootPath == p.nodes(0, i):// Remove the links that are part of the previous shortest paths which share the same root path.remove p.edge(i,i + 1) from Graph;for each node rootPathNode in rootPath except spurNode:remove rootPathNode from Graph;// Calculate the spur path from the spur node to the sink.// Consider also checking if any spurPath foundspurPath = Dijkstra(Graph, spurNode, sink);// Entire path is made up of the root path and spur path.totalPath = rootPath + spurPath;// Add the potential k-shortest path to the heap.if (totalPath not in B):B.append(totalPath);// Add back the edges and nodes that were removed from the graph.restore edges to Graph;restore nodes in rootPath to Graph;if B is empty:// This handles the case of there being no spur paths, or no spur paths left.// This could happen if the spur paths have already been exhausted (added to A),// or there are no spur paths at all - such as when both the source and sink vertices// lie along a \"dead end\".break;// Sort the potential k-shortest paths by cost.B.sort();// Add the lowest cost path becomes the k-shortest path.A[k] = B[0];// In fact we should rather use shift since we are removing the first elementB.pop();return A;ExampleThe example uses Yen\\'s K-Shortest Path Algorithm to compute three paths from(C){\\\\displaystyle (C)}to(H){\\\\displaystyle (H)}. Dijkstra\\'s algorithm is used to calculate the best path from(C){\\\\displaystyle (C)}to(H){\\\\displaystyle (H)}, which is(C)−(E)−(F)−(H){\\\\displaystyle (C)-(E)-(F)-(H)}with cost 5. This path is appended to containerA{\\\\displaystyle A}and becomes the first k-shortest path,A1{\\\\displaystyle A^{1}}.Node(C){\\\\displaystyle (C)}ofA1{\\\\displaystyle A^{1}}becomes the spur node with a root path of itself,R21=(C){\\\\displaystyle {R^{2}}_{1}=(C)}. The edge,(C)−(E){\\\\displaystyle (C)-(E)}, is removed because it coincides with the root path and a path in containerA{\\\\displaystyle A}. Dijkstra\\'s algorithm is used to compute the spur pathS21{\\\\displaystyle {S^{2}}_{1}}, which is(C)−(D)−(F)−(H){\\\\displaystyle (C)-(D)-(F)-(H)}, with a cost of 8.A21=R21+S21=(C)−(D)−(F)−(H){\\\\displaystyle {A^{2}}_{1}={R^{2}}_{1}+{S^{2}}_{1}=(C)-(D)-(F)-(H)}is added to containerB{\\\\displaystyle B}as a potential k-shortest path.Node(E){\\\\displaystyle (E)}ofA1{\\\\displaystyle A^{1}}becomes the spur node withR22=(C)−(E){\\\\displaystyle {R^{2}}_{2}=(C)-(E)}. The edge,(E)−(F){\\\\displaystyle (E)-(F)}, is removed because it coincides with the root path and a path in containerA{\\\\displaystyle A}. Dijkstra\\'s algorithm is used to compute the spur pathS22{\\\\displaystyle {S^{2}}_{2}}, which is(E)−(G)−(H){\\\\displaystyle (E)-(G)-(H)}, with a cost of 7.A22=R22+S22=(C)−(E)−(G)−(H){\\\\displaystyle {A^{2}}_{2}={R^{2}}_{2}+{S^{2}}_{2}=(C)-(E)-(G)-(H)}is added to containerB{\\\\displaystyle B}as a potential k-shortest path.Node(F){\\\\displaystyle (F)}ofA1{\\\\displaystyle A^{1}}becomes the spur node with a root path,R23=(C)−(E)−(F){\\\\displaystyle {R^{2}}_{3}=(C)-(E)-(F)}. The edge,(F)−(H){\\\\displaystyle (F)-(H)}, is removed because it coincides with the root path and a path in containerA{\\\\displaystyle A}. Dijkstra\\'s algorithm is used to compute the spur pathS23{\\\\displaystyle {S^{2}}_{3}}, which is(F)−(G)−(H){\\\\displaystyle (F)-(G)-(H)}, with a cost of 8.A23=R23+S23=(C)−(E)−(F)−(G)−(H){\\\\displaystyle {A^{2}}_{3}={R^{2}}_{3}+{S^{2}}_{3}=(C)-(E)-(F)-(G)-(H)}is added to containerB{\\\\displaystyle B}as a potential k-shortest path.Of the three paths in container B,A22{\\\\displaystyle {A^{2}}_{2}}is chosen to becomeA2{\\\\displaystyle A^{2}}because it has the lowest cost of 7. This process is continued to the 3rd k-shortest path. However, within this 3rd iteration, note that some spur paths do not exist. And the path that is chosen to becomeA3{\\\\displaystyle A^{3}}is(C)−(D)−(F)−(H){\\\\displaystyle (C)-(D)-(F)-(H)}.FeaturesSpace complexityTo store the edges of the graph, the shortest path listA{\\\\displaystyle A}, and the potential shortest path listB{\\\\displaystyle B},N2+KN{\\\\displaystyle N^{2}+KN}memory addresses are required. At worse case, the every node in the graph has an edge to every other node in the graph, thusN2{\\\\displaystyle N^{2}}addresses are needed. OnlyKN{\\\\displaystyle KN}addresses are need for both listA{\\\\displaystyle A}andB{\\\\displaystyle B}because at most onlyK{\\\\displaystyle K}paths will be stored, where it is possible for each path to haveN{\\\\displaystyle N}nodes.Time complexityThe time complexity of Yen\\'s algorithm is dependent on the shortest path algorithm used in the computation of the spur paths, so the Dijkstra algorithm is assumed. Dijkstra\\'s algorithm has a worse case time complexity ofO(N2){\\\\displaystyle O(N^{2})}, but using a Fibonacci heap it becomesO(M+Nlog\\u2061N){\\\\displaystyle O(M+N\\\\log N)}, whereM{\\\\displaystyle M}is the amount of edges in the graph. Since Yen\\'s algorithm makesKl{\\\\displaystyle Kl}calls to the Dijkstra in computing the spur paths, wherel{\\\\displaystyle l}is the length of spur paths. In a condensed graph, the expected value ofl{\\\\displaystyle l}isO(log\\u2061N){\\\\displaystyle O(\\\\log N)}, while the worst case isN{\\\\displaystyle N}., the time complexity becomesO(KN(M+Nlog\\u2061N)){\\\\displaystyle O(KN(M+N\\\\log N))}.ImprovementsYen\\'s algorithm can be improved by using a heap to storeB{\\\\displaystyle B}, the set of potential k-shortest paths. Using a heap instead of a list will improve the performance of the algorithm, but not the complexity.  One method to slightly decrease complexity is to skip the nodes where there are non-existent spur paths. This case is produced when all the spur paths from a spur node have been used in the previousAk{\\\\displaystyle A^{k}}. Also, if containerB{\\\\displaystyle B}hasK−k{\\\\displaystyle K-k}paths of minimum length, in reference to those in containerA{\\\\displaystyle A}, then they can be extract and inserted into containerA{\\\\displaystyle A}since no shorter paths will be found.Lawler\\'s modificationEugene Lawler proposed a modification to Yen\\'s algorithm in which duplicates path are not calculated as opposed to the original algorithm where they are calculated and then discarded when they are found to be duplicates. These duplicates paths result from calculating spur paths of nodes in the root ofAk{\\\\displaystyle A^{k}}. For instance,Ak{\\\\displaystyle A^{k}}deviates fromAk−1{\\\\displaystyle A^{k-1}}at some node(i){\\\\displaystyle (i)}. Any spur path,Skj{\\\\displaystyle {S^{k}}_{j}}wherej=0,…,i{\\\\displaystyle j=0,\\\\ldots ,i}, that is calculated will be a duplicate because they have already been calculated during thek−1{\\\\displaystyle k-1}iteration. Therefore, only spur paths for nodes that were on the spur path ofAk−1{\\\\displaystyle A^{k-1}}must be calculated, i.e. onlySkh{\\\\displaystyle {S^{k}}_{h}}whereh{\\\\displaystyle h}ranges from(i+1)k−1{\\\\displaystyle (i+1)^{k-1}}to(Qk)k−1{\\\\displaystyle (Q_{k})^{k-1}}. To perform this operation forAk{\\\\displaystyle A^{k}}, a record is needed to identify the node whereAk−1{\\\\displaystyle A^{k-1}}branched fromAk−2{\\\\displaystyle A^{k-2}}.See alsoYen\\'s improvement to the Bellman–Ford algorithmReferencesExternal linksOpen Source Python Implementation on GitHubOpen Source C++ ImplementationOpen Source C++ Implementation using Boost Graph Library',\n",
              " '= Path-based strong component algorithm =In graph theory, the strongly connected components of a directed graph may be found using an algorithm that uses depth-first search in combination with two stacks, one to keep track of the vertices in the current component and the second to keep track of the current search path. Versions of this algorithm have been proposed by Purdom (1970), Munro (1971), Dijkstra (1976), Cheriyan & Mehlhorn (1996), and Gabow (2000); of these, Dijkstra\\'s version was the first to achieve linear time.DescriptionThe algorithm performs a depth-first search of the given graph G, maintaining as it does two stacks S and P (in addition to the normal call stack for a recursive function).Stack S contains all the vertices that have not yet been assigned to a strongly connected component, in the order in which the depth-first search reaches the vertices.Stack P contains vertices that have not yet been determined to belong to different strongly connected components from each other. It also uses a counter C of the number of vertices reached so far, which it uses to compute the preorder numbers of the vertices.When the depth-first search reaches a vertex v, the algorithm performs the following steps:Set the preorder number of v to C, and increment C.Push v onto S and also onto P.For each edge from v to a neighboring vertex w:If the preorder number of w has not yet been assigned (the edge is a tree edge), recursively search w;Otherwise, if w has not yet been assigned to a strongly connected component (the edge is a forward/back/cross edge):Repeatedly pop vertices from P until the top element of P has a preorder number less than or equal to the preorder number of w.If v is the top element of P:Pop vertices from S until v has been popped, and assign the popped vertices to a new component.Pop v from P.The overall algorithm consists of a loop through the vertices of the graph, calling this recursive search on each vertex that does not yet have a preorder number assigned to it.Related algorithmsLike this algorithm, Tarjan\\'s strongly connected components algorithm also uses depth first search together with a stack to keep track of vertices that have not yet been assigned to a component, and moves these vertices into a new component when it finishes expanding the final vertex of its component. However, in place of the stack P, Tarjan\\'s algorithm uses  a vertex-indexed array of preorder numbers, assigned in the order that vertices are first visited in the depth-first search. The preorder array is used to keep track of when to form a new component.NotesReferencesCheriyan, J.; Mehlhorn, K. (1996), \"Algorithms for dense graphs and networks on the random access computer\", Algorithmica, 15 (6): 521–549, doi:10.1007/BF01940880, S2CID 8930091.Dijkstra, Edsger (1976), A Discipline of Programming, NJ: Prentice Hall, Ch. 25.Gabow, Harold N. (2000), \"Path-based depth-first search for strong and biconnected components\", Information Processing Letters, 74 (3–4): 107–114, doi:10.1016/S0020-0190(00)00051-X, MR 1761551.Munro, Ian (1971), \"Efficient determination of the transitive closure of a directed graph\", Information Processing Letters, 1 (2): 56–58, doi:10.1016/0020-0190(71)90006-8.Purdom, P., Jr. (1970), \"A transitive closure algorithm\", BIT, 10: 76–94, doi:10.1007/bf01940892, S2CID 20818200.Sedgewick, R. (2004), \"19.8 Strong Components in Digraphs\", Algorithms in Java, Part 5 – Graph Algorithms (3rd ed.), Cambridge MA: Addison-Wesley, pp. 205–216.',\n",
              " '= A* search algorithm =A* (pronounced \"A-star\") is a graph traversal and path search algorithm, which is often used in many fields of computer science due to its completeness, optimality, and optimal efficiency. One major practical drawback is itsO(bd){\\\\displaystyle O(b^{d})}space complexity, as it stores all generated nodes in memory. Thus, in practical travel-routing systems, it is generally outperformed by algorithms which can pre-process the graph to attain better performance, as well as memory-bounded approaches; however, A* is still the best solution in many cases.Peter Hart, Nils Nilsson and Bertram Raphael of Stanford Research Institute (now SRI International) first published the algorithm in 1968. It can be seen as an extension of Dijkstra\\'s algorithm. A* achieves better performance by using heuristics to guide its search.HistoryA* was created as part of the Shakey project, which had the aim of building a mobile robot that could plan its own actions. Nils Nilsson originally proposed using the Graph Traverser algorithm for Shakey\\'s path planning. Graph Traverser is guided by a heuristic function h(n), the estimated distance from node n to the goal node: it entirely ignores g(n), the distance from the start node to n. Bertram Raphael suggested using the sum, g(n) + h(n). Peter Hart invented the concepts we now call admissibility and consistency of heuristic functions. A* was originally designed for finding least-cost paths when the cost of a path is the sum of its  costs, but it has been shown that A* can be used to find optimal paths for any problem satisfying the conditions of a cost algebra.The original 1968 A* paper contained a theorem stating that no A*-like algorithm could expand fewer nodes than A* if the heuristic function is consistent and A*\\'s tie-breaking rule is suitably chosen. A ″correction″ was published a few years later claiming that consistency was not required, but this was shown to be false in Dechter and Pearl\\'s definitive study of A*\\'s optimality (now called optimal efficiency), which gave an example of A* with a heuristic that was admissible but not consistent expanding arbitrarily more nodes than an alternative A*-like algorithm.DescriptionA* is an informed search algorithm, or a best-first search, meaning that it is formulated in terms of weighted graphs: starting from a specific starting node of a graph, it aims to find a path to the given goal node having the smallest cost (least distance travelled, shortest time, etc.).  It does this by maintaining a tree of paths originating at the start node and extending those paths one edge at a time until its termination criterion is satisfied.At each iteration of its main loop, A* needs to determine which of its paths to extend. It does so based on the cost of the path and an estimate of the cost required to extend the path all the way to the goal. Specifically, A* selects the path that minimizesf(n)=g(n)+h(n){\\\\displaystyle f(n)=g(n)+h(n)}where n is the next node on the path, g(n) is the cost of the path from the start node to n, and h(n) is a heuristic function that estimates the cost of the cheapest path from n to the goal. A* terminates when the path it chooses to extend is a path from start to goal or if there are no paths eligible to be extended. The heuristic function is problem-specific. If the heuristic function is admissible, meaning that it never overestimates the actual cost to get to the goal, A* is guaranteed to return a least-cost path from start to goal.Typical implementations of A* use a priority queue to perform the repeated selection of minimum (estimated) cost nodes to expand. This priority queue is known as the open set or fringe. At each step of the algorithm, the node with the lowest f(x) value is removed from the queue, the f and g values of its neighbors are updated accordingly, and these neighbors are added to the queue. The algorithm continues until a removed node (thus the node with the lowest f value out of all fringe nodes) is a goal node. The f value of that goal is then also the cost of the shortest path, since h at the goal is zero in an admissible heuristic.The algorithm described so far gives us only the length of the shortest path. To find the actual sequence of steps, the algorithm can be easily revised so that each node on the path keeps track of its predecessor. After this algorithm is run, the ending node will point to its predecessor, and so on, until some node\\'s predecessor is the start node.As an example, when searching for the shortest route on a map, h(x) might represent the straight-line distance to the goal, since that is physically the smallest possible distance between any two points. For a grid map from a video game, using the Manhattan distance or the octile distance becomes better depending on the set of movements available (4-way or 8-way).If the heuristic h satisfies the additional condition h(x) ≤ d(x, y) + h(y) for every edge (x, y) of the graph (where d denotes the length of that edge), then h is called monotone, or consistent. With a consistent heuristic, A* is guaranteed to find an optimal path without processing any node more than once and A* is equivalent to running Dijkstra\\'s algorithm with the reduced cost d\\'(x, y) = d(x, y) + h(y) − h(x).PseudocodeThe following pseudocode describes the algorithm:Remark: In this pseudocode, if a node is reached by one path, removed from openSet, and subsequently reached by a cheaper path, it will be added to openSet again. This is essential to guarantee that the path returned is optimal if the heuristic function is admissible but not  consistent.   If the heuristic is consistent, when a node is removed from openSet the path to it is guaranteed to be optimal so the test ‘tentative_gScore < gScore[neighbor]’ will always fail if the node is reached again.ExampleAn example of an A* algorithm in action where nodes are cities connected with roads and h(x) is the     straight-line distance to target point:Key: green: start; blue: goal; orange: visitedThe A* algorithm also has real-world applications. In this example, edges are railroads and h(x) is the great-circle distance (the shortest possible distance on a sphere) to the target. The algorithm is searching for a path between Washington, D.C. and Los Angeles.Implementation detailsThere are a number of simple optimizations or implementation details that can significantly affect the performance of an A* implementation.  The first detail to note is that the way the priority queue handles ties can have a significant effect on performance in some situations.  If ties are broken so the queue behaves in a LIFO manner, A* will behave like depth-first search among equal cost paths (avoiding exploring more than one equally optimal solution).When a path is required at the end of the search, it is common to keep with each node a reference to that node\\'s parent.  At the end of the search these references can be used to recover the optimal path.  If these references are being kept then it can be important that the same node doesn\\'t appear in the priority queue more than once (each entry corresponding to a different path to the node, and each with a different cost).  A standard approach here is to check if a node about to be added already appears in the priority queue.  If it does, then the priority and parent pointers are changed to correspond to the lower cost path. A standard binary heap based priority queue does not directly support the operation of searching for one of its elements, but it can be augmented with a hash table that maps elements to their position in the heap, allowing this decrease-priority operation to be performed in logarithmic time. Alternatively, a Fibonacci heap can perform the same decrease-priority operations in constant amortized time.Special casesDijkstra\\'s algorithm, as another example of a uniform-cost search algorithm, can be viewed as a special case of A* whereh(x)=0{\\\\displaystyle h(x)=0}for all x. General depth-first search can be implemented using A* by considering that there is a global counter C initialized with a very large value. Every time we process a node we assign C to all of its newly discovered neighbors. After each single assignment, we decrease the counter C by one. Thus the earlier a node is discovered, the higher itsh(x){\\\\displaystyle h(x)}value. Both Dijkstra\\'s algorithm and depth-first search can be implemented more efficiently without including anh(x){\\\\displaystyle h(x)}value at each node.PropertiesTermination and CompletenessOn finite graphs with non-negative edge weights A* is guaranteed to terminate and is complete, i.e. it will always find a solution (a path from start to goal) if one exists. On infinite graphs with a finite branching factor and edge costs that are bounded away from zero (d(x,y)>ε>0{\\\\textstyle d(x,y)>\\\\varepsilon >0}for some fixedε{\\\\displaystyle \\\\varepsilon }), A* is guaranteed to terminate only if there exists a solution.AdmissibilityA search algorithm is said to be admissible if it is guaranteed to return an optimal solution. If the heuristic function used by A* is admissible, then A* is admissible. An intuitive ″proof″ of this  is as follows:When A* terminates its search, it has found a path from start to goal whose actual cost is lower than the estimated cost of any path from start to goal through any open node (the node\\'sf{\\\\displaystyle f}value). When the heuristic is admissible, those estimates are optimistic (not quite—see the next paragraph), so A* can safely ignore those nodes because they cannot possibly lead to a cheaper solution than the one it already has. In other words, A* will never overlook the possibility of a lower-cost path from start to goal and so it will continue to search until no such possibilities exist.The actual proof is a bit more involved because thef{\\\\displaystyle f}values of open nodes are not guaranteed to be optimistic even if the heuristic is admissible. This is because theg{\\\\displaystyle g}values of open nodes are not guaranteed to be optimal, so the sumg+h{\\\\displaystyle g+h}is not guaranteed to be optimistic.Optimality and ConsistencyAlgorithm A is optimally efficient with respect to a set of alternative algorithms Alts on a set of problems P if for every problem P in P and every algorithm A′ in Alts, the set of nodes expanded by A in solving P is a subset (possibly equal) of the set of nodes expanded by A′ in solving P. The definitive study of the optimal efficiency of A* is due to Rina Dechter and Judea Pearl.They considered a variety of  definitions of Alts and P  in combination with A*\\'s heuristic being merely admissible or being both consistent and admissible.  The most interesting positive result they proved is that A*, with a consistent heuristic, is optimally efficient with respect to all admissible A*-like search algorithms on all ″non-pathological″ search problems.  Roughly speaking, their notion of non-pathological problem is what we now mean by ″up to tie-breaking″.  This result does not hold if A*\\'s heuristic is admissible but not consistent. In that case, Dechter and Pearl showed there exist admissible A*-like algorithms that can expand arbitrarily fewer nodes than A* on some non-pathological problems.Optimal efficiency is about the set of nodes expanded, not the number of node expansions (the number of iterations of A*\\'s main loop).  When the heuristic being used is admissible but not consistent, it is possible for a node to be expanded by A* many times, an exponential number of times in the worst case.In such circumstances Dijkstra\\'s algorithm could outperform A* by a large margin. However, more recent research found that this pathological case only occurs in certain contrived situations where the edge weight of the search graph is exponential in the size of the graph, and that certain inconsistent (but admissible) heuristics can lead to a reduced number of node expansions in A* searches.Bounded relaxationWhile the admissibility criterion guarantees an optimal solution path, it also means that A* must examine all equally meritorious paths to find the optimal path. To compute approximate shortest paths, it is possible to speed up the search at the expense of optimality by relaxing the admissibility criterion. Oftentimes we want to bound this relaxation, so that we can guarantee that the solution path is no worse than (1 + ε) times the optimal solution path. This new guarantee is referred to as ε-admissible.There are a number of ε-admissible algorithms:Weighted A*/Static Weighting\\'s. If ha(n) is an admissible heuristic function, in the weighted version of the A* search one uses hw(n) = ε ha(n), ε > 1 as the heuristic function, and perform the A* search as usual (which eventually happens faster than using ha since fewer nodes are expanded). The path hence found by the search algorithm can have a cost of at most ε times that of the least cost path in the graph.Dynamic Weighting uses the cost functionf(n)=g(n)+(1+εw(n))h(n){\\\\displaystyle f(n)=g(n)+(1+\\\\varepsilon w(n))h(n)}, wherew(n)={1−d(n)Nd(n)≤N0otherwise{\\\\displaystyle w(n)={\\\\begin{cases}1-{\\\\frac {d(n)}{N}}&d(n)\\\\leq N\\\\\\\\0&{\\\\text{otherwise}}\\\\end{cases}}}, and whered(n){\\\\displaystyle d(n)}is the depth of the search and N is the anticipated length of the solution path.Sampled Dynamic Weighting uses sampling of nodes to better estimate and debias the heuristic error.Aε∗{\\\\displaystyle A_{\\\\varepsilon }^{*}}. uses two heuristic functions. The first is the FOCAL list, which is used to select candidate nodes, and the second hF is used to select the most promising node from the FOCAL list.Aε selects nodes with the functionAf(n)+BhF(n){\\\\displaystyle Af(n)+Bh_{F}(n)}, where A and B are constants. If no nodes can be selected, the algorithm will backtrack with the functionCf(n)+DhF(n){\\\\displaystyle Cf(n)+Dh_{F}(n)}, where C and D are constants.AlphA* attempts to promote depth-first exploitation by preferring recently expanded nodes. AlphA* uses the cost functionfα(n)=(1+wα(n))f(n){\\\\displaystyle f_{\\\\alpha }(n)=(1+w_{\\\\alpha }(n))f(n)}, wherewα(n)={λg(π(n))≤g(n~)Λotherwise{\\\\displaystyle w_{\\\\alpha }(n)={\\\\begin{cases}\\\\lambda &g(\\\\pi (n))\\\\leq g({\\\\tilde {n}})\\\\\\\\\\\\Lambda &{\\\\text{otherwise}}\\\\end{cases}}}, where λ and Λ are constants withλ≤Λ{\\\\displaystyle \\\\lambda \\\\leq \\\\Lambda }, π(n) is the parent of n, and ñ is the most recently expanded node.ComplexityThe time complexity of A* depends on the heuristic. In the worst case of an unbounded search space, the number of nodes expanded is exponential in the depth of the solution (the shortest path) d: O(bd), where b is the branching factor (the average number of successors per state). This assumes that a goal state exists at all, and is reachable from the start state; if it is not, and the state space is infinite, the algorithm will not terminate.The heuristic function has a major effect on the practical performance of A* search, since a good heuristic allows A* to prune away many of the bd nodes that an uninformed search would expand. Its quality can be expressed in terms of the effective branching factor b*, which can be determined empirically for a problem instance by measuring the number of nodes generated by expansion, N, and the depth of the solution, then solvingN+1=1+b∗+(b∗)2+⋯+(b∗)d.{\\\\displaystyle N+1=1+b^{*}+(b^{*})^{2}+\\\\dots +(b^{*})^{d}.}Good heuristics are those with low effective branching factor (the optimal being b* = 1).The time complexity is polynomial when the search space is a tree, there is a single goal state, and the heuristic function h meets the following condition:|h(x)−h∗(x)|=O(log\\u2061h∗(x)){\\\\displaystyle |h(x)-h^{*}(x)|=O(\\\\log h^{*}(x))}where h* is the optimal heuristic, the exact cost to get from x to the goal. In other words, the error of h will not grow faster than the logarithm of the \"perfect heuristic\" h* that returns the true distance from x to the goal.The space complexity of A* is roughly the same as that of all other graph search algorithms, as it keeps all generated nodes in memory. In practice, this turns out to be the biggest drawback of A* search, leading to the development of memory-bounded heuristic searches, such as Iterative deepening A*, memory bounded A*, and SMA*.ApplicationsA* is often used for the common pathfinding problem in applications such as video games, but was originally designed as a general graph traversal algorithm.It finds applications in diverse problems, including the problem of parsing using stochastic grammars in NLP.Other cases include an Informational search with online learning.Relations to other algorithmsWhat sets A* apart from a greedy best-first search algorithm is that it takes the cost/distance already traveled, g(n), into account.Some common variants of Dijkstra\\'s algorithm can be viewed as a special case of A* where the heuristich(n)=0{\\\\displaystyle h(n)=0}for all nodes; in turn, both Dijkstra and A* are special cases of dynamic programming.A* itself is a special case of a generalization of branch and bound.VariantsAnytime A*Block A*D*Field D*FringeFringe Saving A* (FSA*)Generalized Adaptive A* (GAA*)Incremental heuristic searchReduced A*Iterative deepening A* (IDA*)Jump point searchLifelong Planning A* (LPA*)New Bidirectional A* (NBA*)Simplified Memory bounded A* (SMA*)Theta*A* can also be adapted to a bidirectional search algorithm. Special care needs to be taken for the stopping criterion.See alsoBreadth-first searchDepth-first searchAny-angle path planning, search for paths that are not limited to move along graph edges but rather can take on any angleNotesReferencesFurther readingNilsson, N. J. (1980). Principles of Artificial Intelligence. Palo Alto, California: Tioga Publishing Company. ISBN 978-0-935382-01-3.External linksClear visual A* explanation, with advice and thoughts on path-findingVariation on A* called Hierarchical Path-Finding A* (HPA*)Brian Grinstead. \"A* Search Algorithm in JavaScript (Updated)\". Archived from the original on 15 February 2020. Retrieved 8 February 2021.',\n",
              " '= Bellman–Ford algorithm =The Bellman–Ford algorithm is an algorithm that computes shortest paths from a single source vertex to all of the other vertices in a weighted digraph.It is slower than Dijkstra\\'s algorithm for the same problem, but more versatile, as it is capable of handling graphs in which some of the edge weights are negative numbers.The algorithm was first proposed by Alfonso Shimbel (1955), but is instead named after Richard Bellman and Lester Ford Jr., who published it in 1958 and 1956, respectively. Edward F. Moore also published a variation of the algorithm in 1959, and for this reason it is also sometimes called the Bellman–Ford–Moore algorithm.Negative edge weights are found in various applications of graphs, hence the usefulness of this algorithm.If a graph contains a \"negative cycle\" (i.e. a cycle whose edges sum to a negative value) that is reachable from the source, then there is no cheapest path: any path that has a point on the negative cycle can be made cheaper by one more walk around the negative cycle. In such a case, the Bellman–Ford algorithm can detect and report the negative cycle.AlgorithmLike Dijkstra\\'s algorithm, Bellman–Ford proceeds by relaxation, in which approximations to the correct distance are replaced by better ones until they eventually reach the solution. In both algorithms, the approximate distance to each vertex is always an overestimate of the true distance, and is replaced by the minimum of its old value and the length of a newly found path.However, Dijkstra\\'s algorithm uses a priority queue to greedily select the closest vertex that has not yet been processed, and performs this relaxation process on all of its outgoing edges; by contrast, the Bellman–Ford algorithm simply relaxes all the edges, and does this|V|−1{\\\\displaystyle |V|-1}times, where|V|{\\\\displaystyle |V|}is the number of vertices in the graph. In each of these repetitions, the number of vertices with correctly calculated distances grows, from which it follows that eventually all vertices will have their correct distances. This method allows the Bellman–Ford algorithm to be applied to a wider class of inputs than Dijkstra. The intermediate answers depend on the order of edges relaxed, but the final answer remains the same.Bellman–Ford runs inO(|V|⋅|E|){\\\\displaystyle O(|V|\\\\cdot |E|)}time, where|V|{\\\\displaystyle |V|}and|E|{\\\\displaystyle |E|}are the number of vertices and edges respectively.function BellmanFord(list vertices, list edges, vertex source) is// This implementation takes in a graph, represented as// lists of vertices (represented as integers [0..n-1]) and edges,// and fills two arrays (distance and predecessor) holding// the shortest path from the source to each vertexdistance := list of size npredecessor := list of size n// Step 1: initialize graphfor each vertex v in vertices dodistance[v] := inf             // Initialize the distance to all vertices to infinitypredecessor[v] := null         // And having a null predecessordistance[source] := 0              // The distance from the source to itself is, of course, zero// Step 2: relax edges repeatedlyrepeat |V|−1 times:for each edge (u, v) with weight w in edges doif distance[u] + w < distance[v] thendistance[v] := distance[u] + wpredecessor[v] := u// Step 3: check for negative-weight cyclesfor each edge (u, v) with weight w in edges doif distance[u] + w < distance[v] thenerror \"Graph contains a negative-weight cycle\"return distance, predecessorSimply put, the algorithm initializes the distance to the source to 0 and all other nodes to infinity. Then for all edges, if the distance to the destination can be shortened by taking the edge, the distance is updated to the new lower value. At each iteration i that the edges are scanned, the algorithm finds all shortest paths of at most length i edges (and possibly some paths longer than i edges). Since the longest possible path without a cycle can be|V|−1{\\\\displaystyle |V|-1}edges, the edges must be scanned|V|−1{\\\\displaystyle |V|-1}times to ensure the shortest path has been found for all nodes. A final scan of all the edges is performed and if any distance is updated, then a path of length|V|{\\\\displaystyle |V|}edges has been found which can only occur if at least one negative cycle exists in the graph.Proof of correctnessThe correctness of the algorithm can be shown by induction:Lemma. After i repetitions of for loop,if Distance(u) is not infinity, it is equal to the length of some path from s to u; andif there is a path from s to u with at most i edges, then Distance(u) is at most the length of the shortest path from s to u with at most i edges.Proof. For the base case of induction, consider i=0 and the moment before for loop is executed for the first time. Then, for the source vertex, source.distance = 0, which is correct. For other vertices u, u.distance = infinity, which is also correct because there is no path from source to u with 0 edges.For the inductive case, we first prove the first part. Consider a moment when a vertex\\'s distance is updated byv.distance := u.distance + uv.weight. By inductive assumption, u.distance is the length of some path from source to u. Then u.distance + uv.weight is the length of the path from source to v that follows the path from  source to u and then goes to v.For the second part, consider a shortest path P (there may be more than one) from source to v with at most i edges. Let u be the last vertex before v on this path. Then, the part of the path from source to u is a shortest path from source to u with at most i-1 edges, since if it were not, then there must be some strictly shorter path from source to u with at most i-1 edges, and we could then append the edge uv to this path to obtain a path with at most i edges that is strictly shorter than P—a contradiction. By inductive assumption, u.distance after i−1 iterations is at most the length of this path from source to u. Therefore, uv.weight + u.distance is at most the length of P. In the ith iteration, v.distance gets compared with uv.weight + u.distance, and is set equal to it if uv.weight + u.distance is smaller. Therefore, after i iterations, v.distance is at most the length of P, i.e., the length of the shortest path from source to v that uses at most i edges.If there are no negative-weight cycles, then every shortest path visits each vertex at most once, so at step 3 no further improvements can be made. Conversely, suppose no improvement can be made. Then for any cycle with vertices v[0], ..., v[k−1],v[i].distance <= v[i-1 (mod k)].distance + v[i-1 (mod k)]v[i].weightSumming around the cycle, the v[i].distance and v[i−1 (mod k)].distance terms cancel, leaving0 <= sum from 1 to k of v[i-1 (mod k)]v[i].weightI.e., every cycle has nonnegative weight.Finding negative cyclesWhen the algorithm is used to find shortest paths, the existence of negative cycles is a problem, preventing the algorithm from finding a correct answer. However, since it terminates upon finding a negative cycle, the Bellman–Ford algorithm can be used for applications in which this is the target to be sought – for example in cycle-cancelling techniques in network flow analysis.Applications in routingA distributed variant of the Bellman–Ford algorithm is used in distance-vector routing protocols, for example the Routing Information Protocol (RIP). The algorithm is distributed because it involves a number of nodes (routers) within an Autonomous system (AS), a collection of IP networks typically owned by an ISP.It consists of the following steps:Each node calculates the distances between itself and all other nodes within the AS and stores this information as a table.Each node sends its table to all neighboring nodes.When a node receives distance tables from its neighbors, it calculates the shortest routes to all other nodes and updates its own table to reflect any changes.The main disadvantages of the Bellman–Ford algorithm in this setting are as follows:It does not scale well.Changes in network topology are not reflected quickly since updates are spread node-by-node.Count to infinity if link or node failures render a node unreachable from some set of other nodes, those nodes may spend forever gradually increasing their estimates of the distance to it, and in the meantime there may be routing loops.ImprovementsThe Bellman–Ford algorithm may be improved in practice (although not in the worst case) by the observation that, if an iteration of the main loop of the algorithm terminates without making any changes, the algorithm can be immediately terminated, as subsequent iterations will not make any more changes. With this early termination condition, the main loop may in some cases use many fewer than |V| − 1 iterations, even though the worst case of the algorithm remains unchanged. The following improvements all maintain theO(|V|⋅|E|){\\\\displaystyle O(|V|\\\\cdot |E|)}worst-case time complexity.A variation of the Bellman-Ford algorithm known as Shortest Path Faster Algorithm, first described by Moore (1959), reduces the number of relaxation steps that need to be performed within each iteration of the algorithm. If a vertex v has a distance value that has not changed since the last time the edges out of v were relaxed, then there is no need to relax the edges out of v a second time. In this way, as the number of vertices with correct distance values grows, the number whose outgoing edges that need to be relaxed in each iteration shrinks, leading to a constant-factor savings in time for dense graphs.Yen (1970) described another improvement to the Bellman–Ford algorithm. His improvement first assigns some arbitrary linear order on all vertices and then partitions the set of all edges into two subsets. The first subset, Ef, contains all edges (vi, vj) such that i < j; the second, Eb, contains edges (vi, vj) such that i > j. Each vertex is visited in the order v1, v2, ..., v|V|, relaxing each outgoing edge from that vertex in Ef. Each vertex is then visited in the order v|V|, v|V|−1, ..., v1, relaxing each outgoing edge from that vertex in Eb. Each iteration of the main loop of the algorithm, after the first one, adds at least two edges to the set of edges whose relaxed distances match the correct shortest path distances: one from Ef and one from Eb. This modification reduces the worst-case number of iterations of the main loop of the algorithm from |V| − 1 to|V|/2{\\\\displaystyle |V|/2}.Another improvement, by Bannister & Eppstein (2012), replaces the arbitrary linear order of the vertices used in Yen\\'s second improvement by a random permutation. This change makes the worst case for Yen\\'s improvement (in which the edges of a shortest path strictly alternate between the two subsets Ef and Eb) very unlikely to happen. With a randomly permuted vertex ordering, the expected number of iterations needed in the main loop is at most|V|/3{\\\\displaystyle |V|/3}.NotesReferencesOriginal sourcesShimbel, A. (1955). Structure in communication nets. Proceedings of the Symposium on Information Networks. New York, New York: Polytechnic Press of the Polytechnic Institute of Brooklyn. pp. 199–203.Bellman, Richard (1958). \"On a routing problem\". Quarterly of Applied Mathematics. 16: 87–90. doi:10.1090/qam/102435. MR 0102435.Ford, Lester R. Jr. (August 14, 1956). Network Flow Theory. Paper P-923. Santa Monica, California: RAND Corporation.Moore, Edward F. (1959). The shortest path through a maze. Proc. Internat. Sympos. Switching Theory 1957, Part II. Cambridge, Massachusetts: Harvard Univ. Press. pp. 285–292. MR 0114710.Yen, Jin Y. (1970). \"An algorithm for finding shortest routes from all source nodes to a given destination in general networks\". Quarterly of Applied Mathematics. 27 (4): 526–530. doi:10.1090/qam/253822. MR 0253822.Bannister, M. J.; Eppstein, D. (2012). Randomized speedup of the Bellman–Ford algorithm. Analytic Algorithmics and Combinatorics (ANALCO12), Kyoto, Japan. pp. 41–47. arXiv:1111.5414. doi:10.1137/1.9781611973020.6.Secondary sourcesBang-Jensen, Jørgen; Gutin, Gregory (2000). \"Section 2.3.4: The Bellman-Ford-Moore algorithm\". Digraphs: Theory, Algorithms and Applications (First ed.). ISBN 978-1-84800-997-4.Schrijver, Alexander (2005). \"On the history of combinatorial optimization (till 1960)\" (PDF). Handbook of Discrete Optimization. Elsevier: 1–68.Cormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L. Introduction to Algorithms. MIT Press and McGraw-Hill., Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7. Section 24.1: The Bellman–Ford algorithm, pp. 588–592. Problem 24-1, pp. 614–615. Third Edition. MIT Press, 2009. ISBN 978-0-262-53305-8. Section 24.1: The Bellman–Ford algorithm, pp. 651–655.Heineman, George T.; Pollice, Gary; Selkow, Stanley (2008). \"Chapter 6: Graph Algorithms\". Algorithms in a Nutshell. O\\'Reilly Media. pp. 160–164. ISBN 978-0-596-51624-6.Kleinberg, Jon; Tardos, Éva (2006). Algorithm Design. New York: Pearson Education, Inc.Sedgewick, Robert (2002). \"Section 21.7: Negative Edge Weights\". Algorithms in Java (3rd ed.). ISBN 0-201-36121-3. Archived from the original on 2008-05-31. Retrieved 2007-05-28.',\n",
              " '= K shortest path routing =The k shortest path routing problem is a generalization of the shortest path routing problem in a given network. It asks not only about a shortest path but also about next k−1 shortest paths (which may be longer than the shortest path). A variation of the problem is the loopless k shortest paths.Finding k shortest paths is possible by extending Dijkstra algorithm or Bellman-Ford algorithm and extend them to find more than one path.HistorySince 1957 many papers were published on the k shortest path routing problem. Most of the fundamental works were done between 1960s and 2001. Since then, most of the research has been on the problem\\'s applications and its variants. In 2010, Michael Günther et al. published a book on Symbolic calculation of k-shortest paths and related measures with the stochastic process algebra tool CASPA.AlgorithmThe Dijkstra algorithm can be generalized to find the k shortest paths.VariationsThere are two main variations of the k shortest path routing problem.  In one variation, paths are allowed to visit the same node more than once, thus creating loops.  In another variation, paths are required to be simple and loopless.  The loopy version is solvable using Eppstein\\'s algorithm and the loopless variation is solvable by Yen\\'s algorithm.Loopy variantIn this variant, the problem is simplified by not requiring paths to be loopless.  A solution was given by B. L. Fox in 1975 in which the k-shortest paths are determined in O(m + kn log n) asymptotic time complexity (using big O notation.  In 1998, David Eppstein reported an approach that maintains an asymptotic complexity of O(m + n log n + k) by computing an implicit representation of the paths, each of which can be output in O(n) extra time. In 2015, Akuba et al. devised an indexing method as a significantly faster alternative for Eppstein\\'s algorithm, in which a data structure called an index is constructed from a graph and then top-k distances between arbitrary pairs of vertices can be rapidly obtained.Loopless variantIn the loopless variant, the paths are forbidden to contain loops which adds an additional level of complexity. It can be solved using Yen\\'s algorithm to find the lengths of all shortest paths from a fixed node to all other nodes in an n-node non negative-distance network, a technique requiring only 2n2 additions and n2 comparison, fewer than other available shortest path algorithms need.  The running time complexity is pseudo-polynomial, being O(kn(m + n log n)) (where m and n represent the number of edges and vertices, respectively). In 2007, John Hershberger and Subhash Suri proposed a replacement paths algorithm, a more efficient implementation of Lawler\\'s  and Yen\\'s algorithm with O(n) improvement in time.Some examples and descriptionExample #1The following example makes use of Yen’s model to find k shortest paths between communicating end nodes. That is, it finds a shortest path, second shortest path, etc. up to the Kth shortest path. More details can be found here.The code provided in this example attempts to solve the k shortest path routing problem for a 15-nodes network containing a combination of unidirectional and bidirectional links:Example #2Another example is the use of k shortest paths algorithm to track multiple objects. The technique implements a multiple object tracker based on the k shortest paths routing algorithm. A set of probabilistic occupancy maps is used as input. An object detector provides the input.The complete details can be found at \"Computer Vision Laboratory – CVLAB\".Example #3Another use of k shortest paths algorithms is to design a transit network that enhances passengers\\' experience in public transportation systems. Such an example of a transit network can be constructed by putting traveling time under consideration. In addition to traveling time, other conditions may be taken depending upon economical and geographical limitations. Despite variations in parameters, the k shortest path algorithms finds the most optimal solutions that satisfies almost all user needs. Such applications of k shortest path algorithms are becoming common, recently Xu, He, Song, and Chaudry (2012) studied the k shortest path problems in transit network systems.ApplicationsThe k shortest path routing is a good alternative for:Geographic path planningNetwork routing, especially in optical mesh network where there are additional constraints that cannot be solved by using ordinary shortest path algorithms.Hypothesis generation in computational linguisticsSequence alignment and metabolic pathway finding in bioinformaticsMultiple object tracking as described aboveRoad Networks: road junctions are the nodes (vertices) and each  edge (link) of the graph is associated with a road segment between two junctions.Related problemsThe breadth-first search algorithm is used when the search is only limited to two operations.The Floyd–Warshall algorithm solves all pairs shortest paths.Johnson\\'s algorithm solves all pairs\\' shortest paths, and may be faster than Floyd–Warshall on sparse graphs.Perturbation theory finds (at worst) the locally shortest path.Cherkassky et al. provide more algorithms and associated evaluations.See alsoConstrained shortest path routingNotesExternal linksImplementation of Yen\\'s algorithmImplementation of Yen\\'s and fastest k shortest simple paths algorithmshttp://www.technical-recipes.com/2012/the-k-shortest-paths-algorithm-in-c/#more-2432Multiple objects tracking technique using K-shortest path algorithm: http://cvlab.epfl.ch/software/ksp/Computer Vision Laboratory: http://cvlab.epfl.ch/software/ksp/',\n",
              " \"= Widest path problem =In graph algorithms, the widest path problem is the problem of finding a path between two designated vertices in a weighted graph, maximizing the weight of the minimum-weight edge in the path. The widest path problem is also known as the maximum capacity path problem. It is possible to adapt most shortest path algorithms to compute widest paths, by modifying them to use the bottleneck distance instead of path length. However, in many cases even faster algorithms are possible.For instance, in a graph that represents connections between routers in the Internet, where the weight of an edge represents the bandwidth of a connection between two routers, the widest path problem is the problem of finding an end-to-end path between two Internet nodes that has the maximum possible bandwidth.  The smallest edge weight on this path is known as the capacity or bandwidth of the path. As well as its applications in network routing, the widest path problem is also an important component of the Schulze method for deciding the winner of a multiway election, and has been applied to digital compositing, metabolic pathway analysis, and the computation of maximum flows.A closely related problem, the minimax path problem or bottleneck shortest path problem asks for the path that minimizes the maximum weight of any of its edges. It has applications that include transportation planning. Any algorithm for the widest path problem can be transformed into an algorithm for the minimax path problem, or vice versa, by reversing the sense of all the weight comparisons performed by the algorithm, or equivalently by replacing every edge weight by its negation.Undirected graphsIn an undirected graph, a widest path may be found as the path between the two vertices in the maximum spanning tree of the graph, and a minimax path may be found as the path between the two vertices in the minimum spanning tree.In any graph, directed or undirected, there is a straightforward algorithm for finding a widest path once the weight of its minimum-weight edge is known: simply delete all smaller edges and search for any path among the remaining edges using breadth first search or depth first search. Based on this test, there also exists a linear time algorithm for finding a widest s-t path in an undirected graph, that does not use the maximum spanning tree. The main idea of the algorithm is to apply the linear-time path-finding algorithm to the median edge weight in the graph, and then either to delete all smaller edges or contract all larger edges according to whether a path does or does not exist, and recurse in the resulting smaller graph.Fernández, Garfinkel & Arbiol (1998) use undirected bottleneck shortest paths in order to form composite aerial photographs that combine multiple images of overlapping areas. In the subproblem to which the widest path problem applies, two images have already been transformed into a common coordinate system; the remaining task is to select a seam, a curve that passes through the region of overlap and divides one of the two images from the other. Pixels on one side of the seam will be copied from one of the images, and pixels on the other side of the seam will be copied from the other image. Unlike other compositing methods that average pixels from both images, this produces a valid photographic image of every part of the region being photographed. They weight the edges of a grid graph by a numeric estimate of how visually apparent a seam across that edge would be, and find a bottleneck shortest path for these weights. Using this path as the seam, rather than a more conventional shortest path, causes their system to find a seam that is difficult to discern at all of its points, rather than allowing it to trade off greater visibility in one part of the image for lesser visibility elsewhere.A solution to the minimax path problem between the two opposite corners of a grid graph can be used to find the weak Fréchet distance between two polygonal chains. Here, each grid graph vertex represents a pair of line segments, one from each chain, and the weight of an edge represents the Fréchet distance needed to pass from one pair of segments to another.If all edge weights of an undirected graph are positive, then the minimax distances between pairs of points (the maximum edge weights of minimax paths) form an ultrametric; conversely every finite ultrametric space comes from minimax distances in this way. A data structure constructed from the minimum spanning tree allows the minimax distance between any pair of vertices to be queried in constant time per query, using lowest common ancestor queries in a Cartesian tree. The root of the Cartesian tree represents the heaviest minimum spanning tree edge, and the children of the root are Cartesian trees recursively constructed from the subtrees of the minimum spanning tree formed by removing the heaviest edge. The leaves of the Cartesian tree represent the vertices of the input graph, and the minimax distance between two vertices equals the weight of the Cartesian tree node that is their lowest common ancestor. Once the minimum spanning tree edges have been sorted, this Cartesian tree can be constructed in linear time.Directed graphsIn directed graphs, the maximum spanning tree solution cannot be used. Instead, several different algorithms are known; the choice of which algorithm to use depends on whether a start or destination vertex for the path is fixed, or whether paths for many start or destination vertices must be found simultaneously.All pairsThe all-pairs widest path problem has applications in the Schulze method for choosing a winner in multiway elections in which voters rank the candidates in preference order. The Schulze method constructs a complete directed graph in which the vertices represent the candidates and every two vertices are connected by an edge. Each edge is directed from the winner to the loser of a pairwise contest between the two candidates it connects, and is labeled with the margin of victory of that contest. Then the method computes widest paths between all pairs of vertices, and the winner is the candidate whose vertex has wider paths to each opponent than vice versa. The results of an election using this method are consistent with the Condorcet method – a candidate who wins all pairwise contests automatically wins the whole election – but it generally allows a winner to be selected, even in situations where the Concorcet method itself fails. The Schulze method has been used by several organizations including the Wikimedia Foundation.To compute the widest path widths for all pairs of nodes in a dense directed graph, such as the ones that arise in the voting application, the asymptotically fastest known approach takes time O(n(3+ω)/2) where ω is the exponent for fast matrix multiplication. Using the best known algorithms for matrix multiplication, this time bound becomes  O(n2.688). Instead, the reference implementation for the Schulze method uses a modified version of the simpler Floyd–Warshall algorithm, which takes O(n3) time. For sparse graphs, it may be more efficient to repeatedly apply a single-source widest path algorithm.Single sourceIf the edges are sorted by their weights, then a modified version of Dijkstra's algorithm can compute the bottlenecks between a designated start vertex and every other vertex in the graph, in linear time. The key idea behind the speedup over a conventional version of Dijkstra's algorithm is that the sequence of bottleneck distances to each vertex, in the order that the vertices are considered by this algorithm, is a monotonic subsequence of the sorted sequence of edge weights; therefore, the priority queue of Dijkstra's algorithm can be implemented as a bucket queue: an array indexed by the numbers from 1 to m (the number of edges in the graph), where array cell i contains the vertices whose bottleneck distance is the weight of the edge with position i in the sorted order. This method allows the widest path problem to be solved as quickly as sorting; for instance, if the edge weights are represented as integers, then the time bounds for integer sorting a list of m integers would apply also to this problem.Single source and single destinationBerman & Handler (1987) suggest that service vehicles and emergency vehicles should use minimax paths when returning from a service call to their base. In this application, the time to return is less important than the response time if another service call occurs while the vehicle is in the process of returning. By using a minimax path, where the weight of an edge is the maximum travel time from a point on the edge to the farthest possible service call, one can plan a route that minimizes the maximum possible delay between receipt of a service call and arrival of a responding vehicle. Ullah, Lee & Hassoun (2009) use maximin paths to model the dominant reaction chains in metabolic networks; in their model, the weight of an edge is the free energy of the metabolic reaction represented by the edge.Another application of widest paths arises in the Ford–Fulkerson algorithm for the maximum flow problem. Repeatedly augmenting a flow along a maximum capacity path in the residual network of the flow leads to a small bound, O(m log U), on the number of augmentations needed to find a maximum flow; here, the edge capacities are assumed to be integers that are at most U. However, this analysis does not depend on finding a path that has the exact maximum of capacity; any path whose capacity is within a constant factor of the maximum suffices. Combining this approximation idea with the shortest path augmentation method of the Edmonds–Karp algorithm leads to a maximum flow algorithm with running time O(mn log U).It is possible to find maximum-capacity paths and minimax paths with a single source and single destination very efficiently even in models of computation that allow only comparisons of the input graph's edge weights and not arithmetic on them. The algorithm maintains a set S of edges that are known to contain the bottleneck edge of the optimal path; initially, S is just the set of all m edges of the graph. At each iteration of the algorithm, it splits S into an ordered sequence of subsets S1, S2, ... of approximately equal size; the number of subsets in this partition is chosen in such a way that all of the split points between subsets can be found by repeated median-finding in time O(m). The algorithm then reweights each edge of the graph by the index of the subset containing the edge, and uses the modified Dijkstra algorithm on the reweighted graph; based on the results of this computation, it can determine in linear time which of the subsets contains the bottleneck edge weight. It then replaces S by the subset Si that it has determined to contain the bottleneck weight, and starts the next iteration with this new set S. The number of subsets into which S can be split increases exponentially with each step, so the number of iterations is proportional to the iterated logarithm function, O(log*n), and the total time is O(m log*n). In a model of computation where each edge weight is a machine integer, the use of repeated bisection in this algorithm can be replaced by a list-splitting technique of Han & Thorup (2002), allowing S to be split into O(√m) smaller sets Si in a single step and leading to a linear overall time bound.Euclidean point setsA variant of the minimax path problem has also been considered for sets of points in the Euclidean plane. As in the undirected graph problem, this Euclidean minimax path problem can be solved efficiently by finding a Euclidean minimum spanning tree: every path in the tree is a minimax path. However, the problem becomes more complicated when a path is desired that not only minimizes the hop length but also, among paths with the same hop length, minimizes or approximately minimizes the total length of the path. The solution can be approximated using geometric spanners.In number theory, the unsolved Gaussian moat problem asks whether or not minimax paths in the Gaussian prime numbers have bounded or unbounded minimax length. That is, does there exist a constant B such that, for every pair of points p and q in the infinite Euclidean point set defined by the Gaussian primes, the minimax path in the Gaussian primes between p and q has minimax edge length at most B?== References ==\"]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "do_search('Belman') # typo on purpose (Belman instead of Bellman)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GjY-G0OZT-i",
        "outputId": "c36f9e9b-bcb4-416d-c946-dbc5a500778b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['= RSA (cryptosystem) =RSA (Rivest–Shamir–Adleman) is a public-key cryptosystem that is widely used for secure data transmission.  It is also one of the oldest.  The acronym \"RSA\" comes from the surnames of Ron Rivest, Adi Shamir and Leonard Adleman, who publicly described the algorithm in 1977.  An equivalent system was developed secretly in 1973 at GCHQ (the British signals intelligence agency) by the English mathematician Clifford Cocks.  That system was declassified in 1997.In a public-key cryptosystem, the encryption key is public and distinct from the decryption key, which is kept secret (private).An RSA user creates and publishes a public key based on two large prime numbers, along with an auxiliary value.  The prime numbers are kept secret.  Messages can be encrypted by anyone, via the public key, but can only be decoded by someone who knows the prime numbers.The security of RSA relies on the practical difficulty of factoring the product of two large prime numbers, the \"factoring problem\". Breaking RSA encryption is known as the RSA problem. Whether it is as difficult as the factoring problem is an open question. There are no published methods to defeat the system if a large enough key is used.RSA is a relatively slow algorithm. Because of this, it is not commonly used to directly encrypt user data. More often, RSA is used to transmit shared keys for symmetric-key cryptography, which are then used for bulk encryption–decryption.HistoryThe idea of an asymmetric public-private key cryptosystem is attributed to Whitfield Diffie and Martin Hellman, who published this concept in 1976. They also introduced digital signatures and attempted to apply number theory. Their formulation used a shared-secret-key created from exponentiation of some number, modulo a prime number. However, they left open the problem of realizing a one-way function, possibly because the difficulty of factoring was not well-studied at the time.Ron Rivest, Adi Shamir, and Leonard Adleman at the Massachusetts Institute of Technology made several attempts over the course of a year to create a one-way function that was hard to invert. Rivest and Shamir, as computer scientists, proposed many potential functions, while Adleman, as a mathematician, was responsible for finding their weaknesses. They tried many approaches, including \"knapsack-based\" and \"permutation polynomials\". For a time, they thought what they wanted to achieve was impossible due to contradictory requirements. In April 1977, they spent Passover at the house of a student and drank a good deal of Manischewitz wine before returning to their homes at around midnight. Rivest, unable to sleep, lay on the couch with a math textbook and started thinking about their one-way function. He spent the rest of the night formalizing his idea, and he had much of the paper ready by daybreak. The algorithm is now known as RSA –  the initials of their surnames in same order as their paper.Clifford Cocks, an English mathematician working for the British intelligence agency Government Communications Headquarters (GCHQ), described an equivalent system in an internal document in 1973. However, given the relatively expensive computers needed to implement it at the time, it was considered to be mostly a curiosity and, as far as is publicly known, was never deployed. His discovery, however, was not revealed until 1997 due to its top-secret classification.Kid-RSA (KRSA) is a simplified public-key cipher published in 1997, designed for educational purposes. Some people feel that learning Kid-RSA gives insight into RSA and other public-key ciphers, analogous to simplified DES.PatentA patent describing the RSA algorithm was granted to MIT on 20 September 1983: U.S. Patent 4,405,829 \"Cryptographic communications system and method\". From DWPI\\'s abstract of the patent:The system includes a communications channel coupled to at least one terminal having an encoding device and to at least one terminal having a decoding device. A message-to-be-transferred is enciphered to ciphertext at the encoding terminal by encoding the message as a number M in a predetermined set. That number is then raised to a first predetermined power (associated with the intended receiver) and finally computed. The remainder or residue, C, is... computed when the exponentiated number is divided by the product of two predetermined prime numbers (associated with the intended receiver).A detailed description of the algorithm was published in August 1977, in Scientific American\\'s Mathematical Games column.  This preceded the patent\\'s filing date of December 1977.  Consequently, the patent had no legal standing outside the United States. Had Cocks\\'s work been publicly known, a patent in the United States would not have been legal either.When the patent was issued, terms of patent were 17 years. The patent was about to expire on 21 September 2000, but RSA Security released the algorithm to the public domain on 6 September 2000.OperationThe RSA algorithm involves four steps: key generation, key distribution, encryption, and decryption.A basic principle behind RSA is the observation that it is practical to find three very large positive integers e, d, and n, such that with modular exponentiation for all integers m (with 0 ≤ m < n):(me)d≡m(modn){\\\\displaystyle (m^{e})^{d}\\\\equiv m{\\\\pmod {n}}}and that knowing e and n, or even m, it can be extremely difficult to find d. The triple bar (≡) here denotes modular congruence.In addition, for some operations it is convenient that the order of the two exponentiations can be changed and that this relation also implies(md)e≡m(modn).{\\\\displaystyle (m^{d})^{e}\\\\equiv m{\\\\pmod {n}}.}RSA involves a public key and a private key. The public key can be known by everyone and is used for encrypting messages. The intention is that messages encrypted with the public key can only be decrypted in a reasonable amount of time by using the private key. The public key is represented by the integers n and e, and the private key by the integer d (although n is also used during the decryption process, so it might be considered to be a part of the private key too). m represents the message (previously prepared with a certain technique explained below).Key generationThe keys for the RSA algorithm are generated in the following way:Choose two distinct prime numbers p and q.For security purposes, the integers p and q should be chosen at random and should be similar in magnitude but differ in length by a few digits to make factoring harder. Prime integers can be efficiently found using a primality test.p and q are kept secret.Compute n = pq.n is used as the modulus for both the public and private keys. Its length, usually expressed in bits, is the key length.n is released as part of the public key.Compute λ(n), where λ is Carmichael\\'s totient function. Since n = pq, λ(n) = lcm(λ(p), λ(q)), and since p and q are prime, λ(p) = φ(p) = p − 1, and likewise λ(q) = q − 1. Hence λ(n) = lcm(p − 1, q − 1).λ(n) is kept secret.The lcm may be calculated through the Euclidean algorithm, since lcm(a, b) = |ab|/gcd(a, b).Choose an integer e such that 1 < e < λ(n) and gcd(e, λ(n)) = 1; that is, e and λ(n) are coprime.e having a short bit-length and small Hamming weight results in more efficient encryption –  the most commonly chosen value for e is 216 + 1 = 65537. The smallest (and fastest) possible value for e is 3, but such a small value for e has been shown to be less secure in some settings.e is released as part of the public key.Determine d as d ≡ e−1 (mod λ(n)); that is, d is the modular multiplicative inverse of e modulo λ(n).This means: solve for d the equation d⋅e ≡ 1 (mod λ(n)); d can be computed efficiently by using the extended Euclidean algorithm, since, thanks to e and λ(n) being coprime, said equation is a form of Bézout\\'s identity, where d is one of the coefficients.d is kept secret as the private key exponent.The public key consists of the modulus n and the public (or encryption) exponent e. The private key consists of the private (or decryption) exponent d, which must be kept secret. p, q, and λ(n) must also be kept secret because they can be used to calculate d. In fact, they can all be discarded after d has been computed.In the original RSA paper, the Euler totient function φ(n) = (p − 1)(q − 1) is used instead of λ(n) for calculating the private exponent d. Since φ(n) is always divisible by λ(n), the algorithm works as well. The possibility of using Euler totient function results also from Lagrange\\'s theorem applied to the multiplicative group of integers modulo pq. Thus any d satisfying d⋅e ≡ 1 (mod φ(n)) also satisfies d⋅e ≡ 1 (mod λ(n)). However, computing d modulo φ(n) will sometimes yield a result that is larger than necessary (i.e. d > λ(n)). Most of the implementations of RSA will accept exponents generated using either method (if they use the private exponent d at all, rather than using the optimized decryption method based on the Chinese remainder theorem described below), but some standards such as FIPS 186-4 may require that d < λ(n). Any \"oversized\" private exponents not meeting this criterion may always be reduced modulo λ(n) to obtain a smaller equivalent exponent.Since any common factors of (p − 1) and (q − 1) are present in the factorisation of n − 1 = pq − 1 = (p − 1)(q − 1) + (p − 1) + (q − 1), it is recommended that (p − 1) and (q − 1) have only very small common factors, if any, besides the necessary 2.Note: The authors of the original RSA paper carry out the key generation by choosing d and then computing e as the modular multiplicative inverse of d modulo φ(n), whereas most current implementations of RSA, such as those following PKCS#1, do the reverse (choose e and compute d). Since the chosen key can be small, whereas the computed key normally is not, the RSA paper\\'s algorithm optimizes decryption compared to encryption, while the modern algorithm optimizes encryption instead.Key distributionSuppose that Bob wants to send information to Alice. If they decide to use RSA, Bob must know Alice\\'s public key to encrypt the message, and Alice must use her private key to decrypt the message.To enable Bob to send his encrypted messages, Alice transmits her public key (n, e) to Bob via a reliable, but not necessarily secret, route. Alice\\'s private key (d) is never distributed.EncryptionAfter Bob obtains Alice\\'s public key, he can send a message M to Alice.To do it, he first turns M (strictly speaking, the un-padded plaintext) into an integer m (strictly speaking, the padded plaintext), such that 0 ≤ m < n by using an agreed-upon reversible protocol known as a padding scheme. He then computes the ciphertext c, using Alice\\'s public key e, corresponding toc≡me(modn).{\\\\displaystyle c\\\\equiv m^{e}{\\\\pmod {n}}.}This can be done reasonably quickly, even for very large numbers, using modular exponentiation. Bob then transmits c to Alice. Note that at least nine values of m will yield a ciphertext c equal tom,but this is very unlikely to occur in practice.DecryptionAlice can recover m from c by using her private key exponent d by computingcd≡(me)d≡m(modn).{\\\\displaystyle c^{d}\\\\equiv (m^{e})^{d}\\\\equiv m{\\\\pmod {n}}.}Given m, she can recover the original message M by reversing the padding scheme.ExampleHere is an example of RSA encryption and decryption. The parameters used here are artificially small, but one can also use OpenSSL to generate and examine a real keypair.Choose two distinct prime numbers, such asp=61{\\\\displaystyle p=61}andq=53.{\\\\displaystyle q=53.}Compute n = pq givingn=61×53=3233.{\\\\displaystyle n=61\\\\times 53=3233.}Compute the Carmichael\\'s totient function of the product as λ(n) = lcm(p − 1, q − 1) givingλ(3233)=lcm\\u2061(60,52)=780.{\\\\displaystyle \\\\lambda (3233)=\\\\operatorname {lcm} (60,52)=780.}Choose any number 1 < e < 780 that is coprime to 780. Choosing a prime number for e leaves us only to check that e is not a divisor of 780.Lete=17{\\\\displaystyle e=17}.Compute d, the modular multiplicative inverse of e (mod λ(n)), yielding  as1=(17×413)mod780.{\\\\displaystyle 1=(17\\\\times 413){\\\\bmod {7}}80.}The public key is (n = 3233, e = 17). For a padded plaintext message m, the encryption function isc(m)=memodn=m17mod3233.{\\\\displaystyle {\\\\begin{aligned}c(m)&=m^{e}{\\\\bmod {n}}\\\\\\\\&=m^{17}{\\\\bmod {3}}233.\\\\end{aligned}}}The private key is (n = 3233, d = 413). For an encrypted ciphertext c, the decryption function ism(c)=cdmodn=c413mod3233.{\\\\displaystyle {\\\\begin{aligned}m(c)&=c^{d}{\\\\bmod {n}}\\\\\\\\&=c^{413}{\\\\bmod {3}}233.\\\\end{aligned}}}For instance, in order to encrypt m = 65, we calculatec=6517mod3233=2790.{\\\\displaystyle c=65^{17}{\\\\bmod {3}}233=2790.}To decrypt c = 2790, we calculatem=2790413mod3233=65.{\\\\displaystyle m=2790^{413}{\\\\bmod {3}}233=65.}Both of these calculations can be computed efficiently using the square-and-multiply algorithm for modular exponentiation. In real-life situations the primes selected would be much larger; in our example it would be trivial to factor n = 3233 (obtained from the freely available public key) back to the primes p and q. e, also from the public key, is then inverted to get d, thus acquiring the private key.Practical implementations use the Chinese remainder theorem to speed up the calculation using modulus of factors (mod pq using mod p and mod q).The values dp, dq and qinv, which are part of the private key are computed as follows:dp=dmod(p−1)=413mod(61−1)=53,dq=dmod(q−1)=413mod(53−1)=49,qinv=q−1modp=53−1mod61=38⇒(qinv×q)modp=38×53mod61=1.{\\\\displaystyle {\\\\begin{aligned}d_{p}&=d{\\\\bmod {(}}p-1)=413{\\\\bmod {(}}61-1)=53,\\\\\\\\d_{q}&=d{\\\\bmod {(}}q-1)=413{\\\\bmod {(}}53-1)=49,\\\\\\\\q_{\\\\text{inv}}&=q^{-1}{\\\\bmod {p}}=53^{-1}{\\\\bmod {6}}1=38\\\\\\\\&\\\\Rightarrow (q_{\\\\text{inv}}\\\\times q){\\\\bmod {p}}=38\\\\times 53{\\\\bmod {6}}1=1.\\\\end{aligned}}}Here is how dp, dq and qinv are used for efficient decryption (encryption is efficient by choice of a suitable d and e pair):m1=cdpmodp=279053mod61=4,m2=cdqmodq=279049mod53=12,h=(qinv×(m1−m2))modp=(38×−8)mod61=1,m=m2+h×q=12+1×53=65.{\\\\displaystyle {\\\\begin{aligned}m_{1}&=c^{d_{p}}{\\\\bmod {p}}=2790^{53}{\\\\bmod {6}}1=4,\\\\\\\\m_{2}&=c^{d_{q}}{\\\\bmod {q}}=2790^{49}{\\\\bmod {5}}3=12,\\\\\\\\h&=(q_{\\\\text{inv}}\\\\times (m_{1}-m_{2})){\\\\bmod {p}}=(38\\\\times -8){\\\\bmod {6}}1=1,\\\\\\\\m&=m_{2}+h\\\\times q=12+1\\\\times 53=65.\\\\end{aligned}}}Signing messagesSuppose Alice uses Bob\\'s public key to send him an encrypted message. In the message, she can claim to be Alice, but Bob has no way of verifying that the message was from Alice, since anyone can use Bob\\'s public key to send him encrypted messages. In order to verify the origin of a message, RSA can also be used to sign a message.Suppose Alice wishes to send a signed message to Bob. She can use her own private key to do so. She produces a hash value of the message, raises it to the power of d (modulo n) (as she does when decrypting a message), and attaches it as a \"signature\" to the message. When Bob receives the signed message, he uses the same hash algorithm in conjunction with Alice\\'s public key. He raises the signature to the power of e (modulo n) (as he does when encrypting a message), and compares the resulting hash value with the message\\'s hash value. If the two agree, he knows that the author of the message was in possession of Alice\\'s private key and that the message has not been tampered with since being sent.This works because of exponentiation rules:h=hash\\u2061(m),{\\\\displaystyle h=\\\\operatorname {hash} (m),}(he)d=hed=hde=(hd)e≡h(modn).{\\\\displaystyle (h^{e})^{d}=h^{ed}=h^{de}=(h^{d})^{e}\\\\equiv h{\\\\pmod {n}}.}Thus the keys may be swapped without loss of generality, that is, a private key of a key pair may be used either to:Decrypt a message only intended for the recipient, which may be encrypted by anyone having the public key (asymmetric encrypted transport).Encrypt a message which may be decrypted by anyone, but which can only be encrypted by one person; this provides a digital signature.Proofs of correctnessProof using Fermat\\'s little theoremThe proof of the correctness of RSA is based on Fermat\\'s little theorem, stating that  ap − 1 ≡ 1 (mod p) for any integer a and prime p, not dividing a.We want to show that(me)d≡m(modpq){\\\\displaystyle (m^{e})^{d}\\\\equiv m{\\\\pmod {pq}}}for every integer m when p and q are distinct prime numbers and e and d are positive integers satisfying ed ≡ 1 (mod λ(pq)).Since λ(pq) = lcm(p − 1, q − 1) is, by construction, divisible by both p − 1 and q − 1, we can writeed−1=h(p−1)=k(q−1){\\\\displaystyle ed-1=h(p-1)=k(q-1)}for some nonnegative integers h and k.To check whether two numbers, such as med and m, are congruent mod pq, it suffices (and in fact is equivalent) to check that they are congruent mod p and mod q separately. To show med ≡ m (mod p), we consider two cases:If m ≡ 0 (mod p), m is a multiple of p. Thus med is a multiple of p. So med ≡ 0 ≡ m (mod p).If m≢{\\\\displaystyle \\\\not \\\\equiv }0 (mod p),med=med−1m=mh(p−1)m=(mp−1)hm≡1hm≡m(modp),{\\\\displaystyle m^{ed}=m^{ed-1}m=m^{h(p-1)}m=(m^{p-1})^{h}m\\\\equiv 1^{h}m\\\\equiv m{\\\\pmod {p}},}where we used Fermat\\'s little theorem to replace mp−1 mod p with 1.The verification that med ≡ m (mod q) proceeds in a completely analogous way:If m ≡ 0 (mod q), med is a multiple of q. So med ≡ 0 ≡ m (mod q).If m≢{\\\\displaystyle \\\\not \\\\equiv }0 (mod q),med=med−1m=mk(q−1)m=(mq−1)km≡1km≡m(modq).{\\\\displaystyle m^{ed}=m^{ed-1}m=m^{k(q-1)}m=(m^{q-1})^{k}m\\\\equiv 1^{k}m\\\\equiv m{\\\\pmod {q}}.}This completes the proof that, for any integer m, and integers e, d such that ed ≡ 1 (mod λ(pq)),(me)d≡m(modpq).{\\\\displaystyle (m^{e})^{d}\\\\equiv m{\\\\pmod {pq}}.}Notes:Proof using Euler\\'s theoremAlthough the original paper of Rivest, Shamir, and Adleman used Fermat\\'s little theorem to explain why RSA works, it is common to find proofs that rely instead on Euler\\'s theorem.We want to show that med ≡ m (mod n), where n = pq is a product of two different prime numbers, and e and d are positive integers satisfying ed ≡ 1 (mod φ(n)). Since e and d are positive, we can write ed = 1 + hφ(n) for some non-negative integer h. Assuming that m is relatively prime to n, we havemed=m1+hφ(n)=m(mφ(n))h≡m(1)h≡m(modn),{\\\\displaystyle m^{ed}=m^{1+h\\\\varphi (n)}=m(m^{\\\\varphi (n)})^{h}\\\\equiv m(1)^{h}\\\\equiv m{\\\\pmod {n}},}where the second-last congruence follows from Euler\\'s theorem.More generally, for any e and d satisfying ed ≡ 1 (mod λ(n)), the same conclusion follows from Carmichael\\'s generalization of Euler\\'s theorem, which states that mλ(n) ≡ 1 (mod n) for all m relatively prime to n.When m is not relatively prime to n, the argument just given is invalid. This is highly improbable (only a proportion of 1/p + 1/q − 1/(pq) numbers have this property), but even in this case, the desired congruence is still true. Either m ≡ 0 (mod p) or m ≡ 0 (mod q), and these cases can be treated using the previous proof.PaddingAttacks against plain RSAThere are a number of attacks against plain RSA as described below.When encrypting with low encryption exponents (e.g., e = 3) and small values of the m (i.e., m < n1/e), the result of me is strictly less than the modulus n. In this case, ciphertexts can be decrypted easily by taking the eth root of the ciphertext over the integers.If the same clear-text message is sent to e or more recipients in an encrypted way, and the receivers share the same exponent e, but different p, q, and therefore n, then it is easy to decrypt the original clear-text message via the Chinese remainder theorem. Johan Håstad noticed that this attack is possible even if the clear texts are not equal, but the attacker knows a linear relation between them. This attack was later improved by Don Coppersmith (see Coppersmith\\'s attack).Because RSA encryption is a deterministic encryption algorithm (i.e., has no random component) an attacker can successfully launch a chosen plaintext attack against the cryptosystem, by encrypting likely plaintexts under the public key and test whether they are equal to the ciphertext. A cryptosystem is called semantically secure if an attacker cannot distinguish two encryptions from each other, even if the attacker knows (or has chosen) the corresponding plaintexts. RSA without padding is not semantically secure.RSA has the property that the product of two ciphertexts is equal to the encryption of the product of the respective plaintexts. That is, m1em2e ≡ (m1m2)e (mod n). Because of this multiplicative property, a chosen-ciphertext attack is possible. E.g., an attacker who wants to know the decryption of a ciphertext c ≡ me (mod n) may ask the holder of the private key d to decrypt an unsuspicious-looking ciphertext c′ ≡ cre (mod n) for some value r chosen by the attacker. Because of the multiplicative property, c′ is the encryption of mr (mod n). Hence, if the attacker is successful with the attack, they will learn mr (mod n), from which they can derive the message m by multiplying mr with the modular inverse of r modulo n.Given the private exponent d, one can efficiently factor the modulus n = pq. And given factorization of the modulus n = pq, one can obtain any private key (d′, n) generated against a public key (e′,  n).Padding schemesTo avoid these problems, practical RSA implementations typically embed some form of structured, randomized padding into the value m before encrypting it. This padding ensures that m does not fall into the range of insecure plaintexts, and that a given message, once padded, will encrypt to one of a large number of different possible ciphertexts.Standards such as PKCS#1 have been carefully designed to securely pad messages prior to RSA encryption. Because these schemes pad the plaintext m with some number of additional bits, the size of the un-padded message M must be somewhat smaller. RSA padding schemes must be carefully designed so as to prevent sophisticated attacks that may be facilitated by a predictable message structure.  Early versions of the PKCS#1 standard (up to version 1.5) used a construction that appears to make RSA semantically secure. However, at Crypto 1998, Bleichenbacher showed that this version is vulnerable to a practical adaptive chosen-ciphertext attack. Furthermore, at Eurocrypt 2000, Coron et al. showed that for some types of messages, this padding does not provide a high enough level of security. Later versions of the standard include Optimal Asymmetric Encryption Padding (OAEP), which prevents these attacks. As such, OAEP should be used in any new application, and PKCS#1 v1.5 padding should be replaced wherever possible. The PKCS#1 standard also incorporates processing schemes designed to provide additional security for RSA signatures, e.g. the Probabilistic Signature Scheme for RSA (RSA-PSS).Secure padding schemes such as RSA-PSS are as essential for the security of message signing as they are for message encryption. Two USA patents on PSS were granted (U.S. Patent 6,266,771 and U.S. Patent 7,036,014); however, these patents expired on 24 July 2009 and 25 April 2010 respectively. Use of PSS no longer seems to be encumbered by patents. Note that using different RSA key pairs for encryption and signing is potentially more secure.Security and practical considerationsUsing the Chinese remainder algorithmFor efficiency, many popular crypto libraries (such as OpenSSL, Java and .NET) use for decryption and signing the following optimization based on the Chinese remainder theorem. The following values are precomputed and stored as part of the private key:p{\\\\displaystyle p}andq{\\\\displaystyle q}–  the primes from the key generation,dP=d(modp−1),{\\\\displaystyle d_{P}=d{\\\\pmod {p-1}},}dQ=d(modq−1),{\\\\displaystyle d_{Q}=d{\\\\pmod {q-1}},}qinv=q−1(modp).{\\\\displaystyle q_{\\\\text{inv}}=q^{-1}{\\\\pmod {p}}.}These values allow the recipient to compute the exponentiation m = cd (mod pq) more efficiently as follows:m1=cdP(modp),{\\\\displaystyle m_{1}=c^{d_{P}}{\\\\pmod {p}},}m2=cdQ(modq),{\\\\displaystyle m_{2}=c^{d_{Q}}{\\\\pmod {q}},}h=qinv(m1−m2)(modp){\\\\displaystyle h=q_{\\\\text{inv}}(m_{1}-m_{2}){\\\\pmod {p}}},m=m2+hq(modpq).{\\\\displaystyle m=m_{2}+hq{\\\\pmod {pq}}.}This is more efficient than computing exponentiation by squaring, even though two modular exponentiations have to be computed. The reason is that these two modular exponentiations both use a smaller exponent and a smaller modulus.Integer factorization and RSA problemThe security of the RSA cryptosystem is based on two mathematical problems: the problem of factoring large numbers and the RSA problem. Full decryption of an RSA ciphertext is thought to be infeasible on the assumption that both of these problems are hard, i.e., no efficient algorithm exists for solving them. Providing security against partial decryption may require the addition of a secure padding scheme.The RSA problem is defined as the task of taking eth roots modulo a composite n: recovering a value m such that c ≡ me (mod n), where (n, e) is an RSA public key, and c is an RSA ciphertext. Currently the most promising approach to solving the RSA problem is to factor the modulus n. With the ability to recover prime factors, an attacker can compute the secret exponent d from a public key (n, e), then decrypt c using the standard procedure. To accomplish this, an attacker factors n into p and q, and computes lcm(p − 1, q − 1) that allows the determination of d from e. No polynomial-time method for factoring large integers on a classical computer has yet been found, but it has not been proven that none exists; see integer factorization for a discussion of this problem.Multiple polynomial quadratic sieve (MPQS) can be used to factor the public modulus n.The first RSA-512 factorization in 1999 used hundreds of computers and required the equivalent of 8,400 MIPS years, over an elapsed time of approximately seven months. By 2009, Benjamin Moody could factor an 512-bit RSA key in 73 days using only public software (GGNFS) and his desktop computer (a dual-core Athlon64 with a 1,900 MHz CPU). Just less than 5 gigabytes of disk storage was required and about 2.5 gigabytes of RAM for the sieving process.Rivest, Shamir, and Adleman noted that Miller has shown that – assuming the truth of the extended Riemann hypothesis – finding d from n and e is as hard as factoring n into p and q (up to a polynomial time difference). However, Rivest, Shamir, and Adleman noted, in section IX/D of their paper, that they had not found a proof that inverting RSA is as hard as factoring.As of 2020, the largest publicly known factored RSA number had 829 bits (250 decimal digits, RSA-250). Its factorization, by a state-of-the-art distributed implementation, took approximately 2700 CPU years. In practice, RSA keys are typically 1024 to 4096 bits long. In 2003, RSA Security estimated that 1024-bit keys were likely to become crackable by 2010. As of 2020, it is not known whether such keys can be cracked, but minimum recommendations have moved to at least 2048 bits. It is generally presumed that RSA is secure if n is sufficiently large, outside of quantum computing.If n is 300 bits or shorter, it can be factored in a few hours in a personal computer, using software already freely available. Keys of 512 bits have been shown to be practically breakable in 1999, when RSA-155 was factored by using several hundred computers, and these are now factored in a few weeks using common hardware. Exploits using 512-bit code-signing certificates that may have been factored were reported in 2011. A theoretical hardware device named TWIRL, described by Shamir and Tromer in 2003, called into question the security of 1024-bit keys.In 1994, Peter Shor showed that a quantum computer – if one could ever be practically created for the purpose – would be able to factor in polynomial time, breaking RSA; see Shor\\'s algorithm.Faulty key generationFinding the large primes p and q is usually done by testing random numbers of the correct size with probabilistic primality tests that quickly eliminate virtually all of the nonprimes.The numbers p and q should not be \"too close\", lest the Fermat factorization for n be successful. If p − q is less than 2n1/4 (n = p⋅q, which even for \"small\" 1024-bit values of n is 3×1077), solving for p and q is trivial. Furthermore, if either p − 1 or q − 1 has only small prime factors, n can be factored quickly by Pollard\\'s p − 1 algorithm, and hence such values of p or q should be discarded.It is important that the private exponent d be large enough. Michael J. Wiener showed that if p is between q and 2q (which is quite typical) and d < n1/4/3, then d can be computed efficiently from n and e.There is no known attack against small public exponents such as e = 3, provided that the proper padding is used. Coppersmith\\'s attack has many applications in attacking RSA specifically if the public exponent e is small and if the encrypted message is short and not padded. 65537 is a commonly used value for e; this value can be regarded as a compromise between avoiding potential small-exponent attacks and still allowing efficient encryptions (or signature verification). The NIST Special Publication on Computer Security (SP 800-78 Rev. 1 of August 2007) does not allow public exponents e smaller than 65537, but does not state a reason for this restriction.In October 2017, a team of researchers from Masaryk University announced the ROCA vulnerability, which affects RSA keys generated by an algorithm embodied in a library from Infineon known as RSALib. A large number of smart cards and trusted platform modules (TPM) were shown to be affected. Vulnerable RSA keys are easily identified using a test program the team released.Importance of strong random number generationA cryptographically strong random number generator, which has been properly seeded with adequate entropy, must be used to generate the primes p and q. An analysis comparing millions of public keys gathered from the Internet was carried out in early 2012 by Arjen K. Lenstra, James P. Hughes, Maxime Augier, Joppe W. Bos, Thorsten Kleinjung and Christophe Wachter. They were able to factor 0.2% of the keys using only Euclid\\'s algorithm.They exploited a weakness unique to cryptosystems based on integer factorization. If n = pq is one public key, and n′ = p′q′ is another, then if by chance p = p′ (but q is not equal to q′), then a simple computation of gcd(n, n′) = p factors both n and n′, totally compromising both keys. Lenstra et al. note that this problem can be minimized by using a strong random seed of bit length twice the intended security level, or by employing a deterministic function to choose q given p, instead of choosing p and q independently.Nadia Heninger was part of a group that did a similar experiment. They used an idea of Daniel J. Bernstein to compute the GCD of each RSA key n against the product of all the other keys n′ they had found (a 729-million-digit number), instead of computing each gcd(n, n′) separately, thereby achieving a very significant speedup, since after one large division, the GCD problem is of normal size.Heninger says in her blog that the bad keys occurred almost entirely in embedded applications, including \"firewalls, routers, VPN devices, remote server administration devices, printers, projectors, and VOIP phones\" from more than 30 manufacturers. Heninger explains that the one-shared-prime problem uncovered by the two groups results from situations where the pseudorandom number generator is poorly seeded initially, and then is reseeded between the generation of the first and second primes. Using seeds of sufficiently high entropy obtained from key stroke timings or electronic diode noise or atmospheric noise from a radio receiver tuned between stations should solve the problem.Strong random number generation is important throughout every phase of public-key cryptography. For instance, if a weak generator is used for the symmetric keys that are being distributed by RSA, then an eavesdropper could bypass RSA and guess the symmetric keys directly.Timing attacksKocher described a new attack on RSA in 1995: if the attacker Eve knows Alice\\'s hardware in sufficient detail and is able to measure the decryption times for several known ciphertexts, Eve can deduce the decryption key d quickly. This attack can also be applied against the RSA signature scheme. In 2003, Boneh and Brumley demonstrated a more practical attack capable of recovering RSA factorizations over a network connection (e.g., from a Secure Sockets Layer (SSL)-enabled webserver). This attack takes advantage of information leaked by the Chinese remainder theorem optimization used by many RSA implementations.One way to thwart these attacks is to ensure that the decryption operation takes a constant amount of time for every ciphertext. However, this approach can significantly reduce performance. Instead, most RSA implementations use an alternate technique known as cryptographic blinding. RSA blinding makes use of the multiplicative property of RSA. Instead of computing cd (mod n), Alice first chooses a secret random value r and computes (rec)d (mod n). The result of this computation, after applying Euler\\'s theorem, is rcd (mod n), and so the effect of r can be removed by multiplying by its inverse. A new value of r is chosen for each ciphertext. With blinding applied, the decryption time is no longer correlated to the value of the input ciphertext, and so the timing attack fails.Adaptive chosen-ciphertext attacksIn 1998, Daniel Bleichenbacher described the first practical adaptive chosen-ciphertext attack against RSA-encrypted messages using the PKCS #1 v1 padding scheme (a padding scheme randomizes and adds structure to an RSA-encrypted message, so it is possible to determine whether a decrypted message is valid). Due to flaws with the PKCS #1 scheme, Bleichenbacher was able to mount a practical attack against RSA implementations of the Secure Sockets Layer protocol and to recover session keys. As a result of this work, cryptographers now recommend the use of provably secure padding schemes such as Optimal Asymmetric Encryption Padding, and RSA Laboratories has released new versions of PKCS #1 that are not vulnerable to these attacks.A variant of this attack, dubbed \"BERserk\", came back in 2014. It impacted the Mozilla NSS Crypto Library, which was used notably by Firefox and Chrome.Side-channel analysis attacksA side-channel attack using branch-prediction analysis (BPA) has been described. Many processors use a branch predictor to determine whether a conditional branch in the instruction flow of a program is likely to be taken or not. Often these processors also implement simultaneous multithreading (SMT). Branch-prediction analysis attacks use a spy process to discover (statistically) the private key when processed with these processors.Simple Branch Prediction Analysis (SBPA) claims to improve BPA in a non-statistical way. In their paper, \"On the Power of Simple Branch Prediction Analysis\", the authors of SBPA (Onur Aciicmez and Cetin Kaya Koc) claim to have discovered 508 out of 512 bits of an RSA key in 10 iterations.A power-fault attack on RSA implementations was described in 2010. The author recovered the key by varying the CPU power voltage outside limits; this caused multiple power faults on the server.Tricky implementationThere are many details to keep in mind in order to implement RSA securely (strong PRNG, acceptable public exponent...) . This makes the implementation challenging, to the point the book Practical Cryptography With Go suggests avoiding RSA if possible.ImplementationsSome cryptography libraries that provide support for RSA include:BotanBouncy CastlecryptlibCrypto++LibgcryptNettleOpenSSLwolfCryptGnuTLSmbed TLSLibreSSLSee alsoAcoustic cryptanalysisComputational complexity theoryCryptographic key lengthDiffie–Hellman key exchangeKey exchangeKey managementElliptic-curve cryptographyPublic-key cryptographyTrapdoor functionReferencesFurther readingMenezes, Alfred; van Oorschot, Paul C.; Vanstone, Scott A. (October 1996). Handbook of Applied Cryptography. CRC Press. ISBN 978-0-8493-8523-0.Cormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L.; Stein, Clifford (2001). Introduction to Algorithms (2nd ed.). MIT Press and McGraw-Hill. pp. 881–887. ISBN 978-0-262-03293-3.External linksThe Original RSA Patent as filed with the U.S. Patent Office by Rivest; Ronald L. (Belmont, MA), Shamir; Adi (Cambridge, MA), Adleman; Leonard M. (Arlington, MA), December 14, 1977, U.S. Patent 4,405,829.PKCS #1: RSA Cryptography Standard (RSA Laboratories website)The PKCS #1 standard \"provides recommendations for the implementation of public-key cryptography based on the RSA algorithm, covering the following aspects: cryptographic primitives; encryption schemes; signature schemes with appendix; ASN.1 syntax for representing keys and for identifying the schemes\".Explanation of RSA using colored lamps on YouTubeThorough walk through of RSAPrime Number Hide-And-Seek: How the RSA Cipher WorksOnur Aciicmez, Cetin Kaya Koc, Jean-Pierre Seifert: On the Power of Simple Branch Prediction AnalysisExample of an RSA implementation with PKCS#1 padding (GPL source code)Kocher\\'s article about timing attacksAn animated explanation of RSA with its mathematical background by CrypToolGrime, James. \"RSA Encryption\". Numberphile. Brady Haran.How RSA Key used for Encryption in real world',\n",
              " '= 1-planar graph =In topological graph theory, a 1-planar graph is a graph that can be drawn in the Euclidean plane in such a way that each edge has at most one crossing point, where it crosses a single additional edge. If a 1-planar graph, one of the most natural generalizations of planar graphs, is drawn that way, the drawing is called a 1-plane graph or 1-planar embedding of the graph.Coloring1-planar graphs were first studied by Ringel (1965), who showed that they can be colored with at most seven colors. Later, the precise number of colors needed to color these graphs, in the worst case, was shown to be six. The example of the complete graph K6, which is 1-planar, shows that 1-planar graphs may sometimes require six colors. However, the proof that six colors are always enough is more complicated.Ringel\\'s motivation was in trying to solve a variation of total coloring for planar graphs, in which one simultaneously colors the vertices and faces of a planar graph in such a way that no two adjacent vertices have the same color, no two adjacent faces have the same color, and no vertex and face that are adjacent to each other have the same color. This can obviously be done using eight colors by applying the four color theorem to the given graph and its dual graph separately, using two disjoint sets of four colors. However, fewer colors may be obtained by forming an auxiliary graph that has a vertex for each vertex or face of the given planar graph, and in which two auxiliary graph vertices are adjacent whenever they correspond to adjacent features of the given planar graph. A vertex coloring of the auxiliary graph corresponds to a vertex-face coloring of the original planar graph. This auxiliary graph is 1-planar, from which it follows that Ringel\\'s vertex-face coloring problem may also be solved with six colors. The graph K6 cannot be formed as an auxiliary graph in this way, but nevertheless the vertex-face coloring problem also sometimes requires six colors; for instance, if the planar graph to be colored is a triangular prism, then its eleven vertices and faces require six colors, because no three of them may be given a single color.Edge densityEvery 1-planar graph with n vertices has at most 4n − 8 edges. More strongly, each 1-planar drawing has at most n − 2 crossings; removing one edge from each crossing pair of edges leaves a planar graph, which can have at most 3n − 6 edges, from which the 4n − 8 bound on the number of edges in the original 1-planar graph immediately follows.  However, unlike planar graphs (for which all maximal planar graphs on a given vertex set have the same number of edges as each other), there exist maximal 1-planar graphs (graphs to which no additional edges can be added while preserving 1-planarity) that have significantly fewer than 4n − 8 edges. The bound of 4n − 8 on the maximum possible number of edges in a 1-planar graph can be used to show that the complete graph K7 on seven vertices is not 1-planar, because this graph has 21 edges and in this case 4n − 8 = 20 < 21.A 1-planar graph is said to be an optimal 1-planar graph if it has exactly 4n − 8 edges, the maximum possible. In a 1-planar embedding of an optimal 1-planar graph, the uncrossed edges necessarily form a quadrangulation (a polyhedral graph in which every face is a quadrilateral). Every quadrangulation gives rise to an optimal 1-planar graph in this way, by adding the two diagonals to each of its quadrilateral faces. It follows that every optimal 1-planar graph is Eulerian (all of its vertices have even degree), that the minimum degree in such a graph is six, and that every optimal 1-planar graph has at least eight vertices of degree exactly six. Additionally, every optimal 1-planar graph is 4-vertex-connected, and every 4-vertex cut in such a graph is a separating cycle in the underlying quadrangulation.The graphs that have straight 1-planar drawings (that is, drawings in which each edge is represented by a line segment, and in which each line segment is crossed by at most one other edge) have a slightly tighter bound of 4n − 9 on the maximum number of edges, achieved by infinitely many graphs.Complete multipartite graphsA complete classification of the 1-planar complete graphs, complete bipartite graphs, and more generally complete multipartite graphs is known. Every complete bipartite graph of the form K2,n is 1-planar, as is every complete tripartite graph of the form K1,1,n. Other than these infinite sets of examples, the only complete multipartite 1-planar graphs are K6, K1,1,1,6, K1,1,2,3, K2,2,2,2, K1,1,1,2,2, and their subgraphs. The minimal non-1-planar complete multipartite graphs are K3,7, K4,5, K1,3,4, K2,3,3, and K1,1,1,1,3.For instance, the complete bipartite graph K3,6 is 1-planar because it is a subgraph of K1,1,1,6, but K3,7 is not 1-planar.Computational complexityIt is NP-complete to test whether a given graph is 1-planar, and it remains NP-complete even for the graphs formed from planar graphs by adding a single edge and for graphs of bounded bandwidth. The problem is fixed-parameter tractable when parameterized by cyclomatic number or by tree-depth, so it may be solved in polynomial time when those parameters are bounded.In contrast to Fáry\\'s theorem for planar graphs, not every 1-planar graph may be drawn 1-planarly with straight line segments for its edges. However, testing whether a 1-planar drawing may be straightened in this way can be done in polynomial time. Additionally, every 3-vertex-connected 1-planar graph has a 1-planar drawing in which at most one edge, on the outer face of the drawing, has a bend in it. This drawing can be constructed in linear time from a 1-planar embedding of the graph. The 1-planar graphs have bounded book thickness, but some 1-planar graphs including K2,2,2,2 have book thickness at least four.1-planar graphs have bounded local treewidth, meaning that there is a (linear) function f such that the 1-planar graphs of diameter d have treewidth at most f(d); the same property holds more generally for the graphs that can be embedded onto a surface of bounded genus with a bounded number of crossings per edge. They also have separators, small sets of vertices the removal of which decomposes the graph into connected components whose size is a constant fraction of the size of the whole graph. Based on these properties, numerous algorithms for planar graphs, such as Baker\\'s technique for designing approximation algorithms, can be extended to 1-planar graphs. For instance, this method leads to a polynomial-time approximation scheme for the maximum independent set of a 1-planar graph.Generalizations and related conceptsThe class of graphs analogous to outerplanar graphs for 1-planarity are called the outer-1-planar graphs. These are graphs that can be drawn in a disk, with the vertices on the boundary of the disk, and with at most one crossing per edge. These graphs can always be drawn (in an outer-1-planar way) with straight edges and right angle crossings. By using dynamic programming on the SPQR tree of a given graph, it is possible to test whether it is outer-1-planar in linear time. The triconnected components of the graph (nodes of the SPQR tree) can consist only of cycle graphs, bond graphs, and four-vertex complete graphs, from which it also follows that outer-1-planar graphs are planar and have treewidth at most three.The 1-planar graphs include the 4-map graphs, graphs formed from the adjacencies of regions in the plane with at most four regions meeting in any point. Conversely, every optimal 1-planar graph is a 4-map graph. However, 1-planar graphs that are not optimal 1-planar may not be map graphs.1-planar graphs have been generalized to k-planar graphs, graphs for which each edge is crossed at most k times (0-planar graphs are exactly the planar graphs). Ringel defined the local crossing number of G to be the least non-negative integer k such that G has a k-planar drawing. Because the local crossing number is the maximum degree of the intersection graph of the edges of an optimal drawing, and the thickness (minimum number of planar graphs into which the edges can be partitioned) can be seen as the chromatic number of an intersection graph of an appropriate drawing, it follows from Brooks\\' theorem that the thickness is at most one plus the local crossing number. The k-planar graphs with n vertices have at most O(k1/2n) edges, and treewidth O((kn)1/2). A shallow minor of a k-planar graph, with depth d, is itself a (2d + 1)k-planar graph, so the shallow minors of 1-planar graphs and of k-planar graphs are also sparse graphs, implying that the 1-planar and k-planar graphs have bounded expansion.Nonplanar graphs may also be parameterized by their crossing number, the minimum number of pairs of edges that cross in any drawing of the graph. A graph with crossing number k is necessarily k-planar, but not necessarily vice versa. For instance, the Heawood graph has crossing number 3, but it is not necessary for its three crossings to all occur on the same edge of the graph, so it is 1-planar, and can in fact be drawn in a way that simultaneously optimizes the total number of crossings and the crossings per edge.Another related concept for nonplanar graphs is graph skewness, the minimal number of edges that must be removed to make a graph planar.ReferencesFurther readingKobourov, Stephen; Liotta, Giuseppe; Montecchiani, Fabrizio (2017), \"An annotated bibliography on 1-planarity\", Computer Science Review, 25: 49–67, arXiv:1703.02261, Bibcode:2017arXiv170302261K, doi:10.1016/j.cosrev.2017.06.002, S2CID 7732463',\n",
              " '= 26-fullerene graph =In the mathematical field of graph theory, the 26-fullerene graph is a polyhedral graph with V = 26 vertices and E = 39 edges. Its planar embedding has three hexagonal faces (including the one shown as the external face of the illustration) and twelve pentagonal faces. As a planar graph with only pentagonal and hexagonal faces, meeting in three faces per vertex, this graph is a fullerene. The existence of this fullerene has been known since at least 1968.PropertiesThe 26-fullerene graph hasD3h{\\\\displaystyle D_{3h}}prismatic symmetry, the same group of symmetries as the triangular prism. This symmetry group has 12 elements; it has six symmetries that arbitrarily permute the three hexagonal faces of the graph and preserve the orientation of its planar embedding, and another six orientation-reversing symmetries.The number of fullerenes with a given even number of vertices grows quickly in the number of vertices; 26 is the largest number of vertices for which the fullerene structure is unique. The only two smaller fullerenes are the graph of the regular dodecahedron (a fullerene with 20 vertices) and the graph of the truncated hexagonal trapezohedron (a 24-vertex fullerene), which are the two types of cells in the Weaire–Phelan structure.The 26-fullerene graph has many perfect matchings. One must remove at least five edges from the graph in order to obtain a subgraph that has exactly one perfect matching. This is a unique property of this graph among fullerenes in the sense that, for every other number of vertices of a fullerene, there exists at least one fullerene from which one can remove four edges to obtain a subgraph with a unique perfect matching.The vertices of the 26-fullerene graph can be labeled with sequences of 12 bits, in such a way that distance in the graph equals half of the Hamming distance between these bitvectors.This can also be interpreted as an isometric embedding from the graph into a 12-dimensional taxicab geometry. The 26-fullerene graph is one of only five fullerenes with such an embedding.In popular cultureIn 2009, The New York Times published a puzzle involving Hamiltonian paths in this graph, taking advantage of the correspondence between its 26 vertices and the 26 letters of the English alphabet.== References ==',\n",
              " '= 3-Way =In cryptography, 3-Way is a block cipher designed in 1994 by Joan Daemen. It is closely related to BaseKing; the two are variants of the same general cipher technique.3-Way has a block size of 96 bits, notably not a power of two such as the more common 64 or 128 bits. The key length is also 96 bits. The figure 96 arises from the use of three 32 bit words in the algorithm, from which also is derived the cipher\\'s name. When 3-Way was invented, 96-bit keys and blocks were quite strong, but more recent ciphers have a 128-bit block, and few now have keys shorter than 128 bits. 3-Way is an 11-round substitution–permutation network.3-Way is designed to be very efficient in a wide range of platforms from 8-bit processors to specialized hardware, and has some elegant mathematical features which enable nearly all the decryption to be done in exactly the same circuits as did the encryption.3-Way, just as its counterpart BaseKing, is vulnerable to related key cryptanalysis. John Kelsey, Bruce Schneier, and David Wagner showed how it can be broken with one related key query and about222{\\\\displaystyle 2^{22}}chosen plaintexts.ReferencesJ. Daemen; R. Govaerts; Joos Vandewalle (1993). \"A New Approach to Block Cipher Design\". Fast Software Encryption (FSE) 1993. Springer-Verlag. pp. 18–32.J. Kelsey; B. Schneier; D. Wagner (November 1997). \"Related-Key Cryptanalysis of 3-WAY, Biham-DES, CAST, DES-X, NewDES, RC2, and TEA\" (PDF/PostScript). ICICS \\'97 Proceedings. Springer-Verlag. pp. 233–246. Retrieved 2007-02-14.External linksSCAN\\'s entry for 3-WayChapter 7 of Daemen\\'s thesis (gzipped Postscript)',\n",
              " '= 3D Topicscape =3D Topicscape, a software application, is a Personal Information Manager that provides a template loosely based on mind-mapping  or concept mapping.  It presents the mind map as a 3D scene where each node is a cone (or pyramid, or variation on such a shape).  It can also display in a 2D format.  Nodes are arranged in a way that indicates how they are related in much the same way as a mind map.  In addition to its use for information management it is claimed to be suitable as a task manager, and for use in project management.A Topicscape is created by importing folders (by Drag-and-drop or menus), importing from other mind mapping software including FreeMind, PersonalBrain and MindManager or by hand with mouse clicks or keyboard shortcuts.  Import sources may be converted to a new Topicscape or added as a portion of an existing one.The number of levels that can be stored is not limited, but up to seven levels of the hierarchy may be viewed at once.  Any node may be chosen as the centre of the 3D scene and choosing one at the edge will cause more to come into view.Topicscape\\'s most obvious difference from 2D mind mapping software is that it provides a zooming interface and simulates flying as noted by Wall Street Journal columnist Jeremy Wagstaff in his column \"Fly through your computer.\"  The BBC World Service and PC World  have also reviewed 3D Topicscape.Versions3D Topicscape public Beta in Jan 20063D Topicscape 1.0, May 20063D Topicscape Lite 1.05; 1.07, Dec 2007; 1.2, Aug 20083D Topicscape Pro 1.2, Feb 2007; 1.3, May 2007; 1.56, Dec 2007; 1.59, May 2008; 1.6, Jul 2008;  1.63, Sep 2008; 2.0, Apr 2009; 2.5 Dec 2009;  2.6 Feb 2010; 2.7 April 20103D Topicscape Student Edition Beta, Sep 2007; 1.0, Feb 2008; 2.0, Dec 2009File FormatUses an embedded Firebird relational database to store user-provided and operational metadata.  Files attached to nodes (topics) may be linked to in their original location or be held in a folder (directory) associated with a given Topicscape.  Links to files in a Topicscape\\'s folder are relative.  Topicscape folders may therefore be moved without breaking such links.Import file formats supported include FreeMind, OML, MindManager versions 5-8, PersonalBrain, and text (outline-numbered);Export file formats can be those for FreeMind, OPML, HTML and text structured for re-import, or text for reading.See alsoBrainstormingList of concept- and mind-mapping softwarePersonal information managersMind map== References ==',\n",
              " '= A* search algorithm =A* (pronounced \"A-star\") is a graph traversal and path search algorithm, which is often used in many fields of computer science due to its completeness, optimality, and optimal efficiency. One major practical drawback is itsO(bd){\\\\displaystyle O(b^{d})}space complexity, as it stores all generated nodes in memory. Thus, in practical travel-routing systems, it is generally outperformed by algorithms which can pre-process the graph to attain better performance, as well as memory-bounded approaches; however, A* is still the best solution in many cases.Peter Hart, Nils Nilsson and Bertram Raphael of Stanford Research Institute (now SRI International) first published the algorithm in 1968. It can be seen as an extension of Dijkstra\\'s algorithm. A* achieves better performance by using heuristics to guide its search.HistoryA* was created as part of the Shakey project, which had the aim of building a mobile robot that could plan its own actions. Nils Nilsson originally proposed using the Graph Traverser algorithm for Shakey\\'s path planning. Graph Traverser is guided by a heuristic function h(n), the estimated distance from node n to the goal node: it entirely ignores g(n), the distance from the start node to n. Bertram Raphael suggested using the sum, g(n) + h(n). Peter Hart invented the concepts we now call admissibility and consistency of heuristic functions. A* was originally designed for finding least-cost paths when the cost of a path is the sum of its  costs, but it has been shown that A* can be used to find optimal paths for any problem satisfying the conditions of a cost algebra.The original 1968 A* paper contained a theorem stating that no A*-like algorithm could expand fewer nodes than A* if the heuristic function is consistent and A*\\'s tie-breaking rule is suitably chosen. A ″correction″ was published a few years later claiming that consistency was not required, but this was shown to be false in Dechter and Pearl\\'s definitive study of A*\\'s optimality (now called optimal efficiency), which gave an example of A* with a heuristic that was admissible but not consistent expanding arbitrarily more nodes than an alternative A*-like algorithm.DescriptionA* is an informed search algorithm, or a best-first search, meaning that it is formulated in terms of weighted graphs: starting from a specific starting node of a graph, it aims to find a path to the given goal node having the smallest cost (least distance travelled, shortest time, etc.).  It does this by maintaining a tree of paths originating at the start node and extending those paths one edge at a time until its termination criterion is satisfied.At each iteration of its main loop, A* needs to determine which of its paths to extend. It does so based on the cost of the path and an estimate of the cost required to extend the path all the way to the goal. Specifically, A* selects the path that minimizesf(n)=g(n)+h(n){\\\\displaystyle f(n)=g(n)+h(n)}where n is the next node on the path, g(n) is the cost of the path from the start node to n, and h(n) is a heuristic function that estimates the cost of the cheapest path from n to the goal. A* terminates when the path it chooses to extend is a path from start to goal or if there are no paths eligible to be extended. The heuristic function is problem-specific. If the heuristic function is admissible, meaning that it never overestimates the actual cost to get to the goal, A* is guaranteed to return a least-cost path from start to goal.Typical implementations of A* use a priority queue to perform the repeated selection of minimum (estimated) cost nodes to expand. This priority queue is known as the open set or fringe. At each step of the algorithm, the node with the lowest f(x) value is removed from the queue, the f and g values of its neighbors are updated accordingly, and these neighbors are added to the queue. The algorithm continues until a removed node (thus the node with the lowest f value out of all fringe nodes) is a goal node. The f value of that goal is then also the cost of the shortest path, since h at the goal is zero in an admissible heuristic.The algorithm described so far gives us only the length of the shortest path. To find the actual sequence of steps, the algorithm can be easily revised so that each node on the path keeps track of its predecessor. After this algorithm is run, the ending node will point to its predecessor, and so on, until some node\\'s predecessor is the start node.As an example, when searching for the shortest route on a map, h(x) might represent the straight-line distance to the goal, since that is physically the smallest possible distance between any two points. For a grid map from a video game, using the Manhattan distance or the octile distance becomes better depending on the set of movements available (4-way or 8-way).If the heuristic h satisfies the additional condition h(x) ≤ d(x, y) + h(y) for every edge (x, y) of the graph (where d denotes the length of that edge), then h is called monotone, or consistent. With a consistent heuristic, A* is guaranteed to find an optimal path without processing any node more than once and A* is equivalent to running Dijkstra\\'s algorithm with the reduced cost d\\'(x, y) = d(x, y) + h(y) − h(x).PseudocodeThe following pseudocode describes the algorithm:Remark: In this pseudocode, if a node is reached by one path, removed from openSet, and subsequently reached by a cheaper path, it will be added to openSet again. This is essential to guarantee that the path returned is optimal if the heuristic function is admissible but not  consistent.   If the heuristic is consistent, when a node is removed from openSet the path to it is guaranteed to be optimal so the test ‘tentative_gScore < gScore[neighbor]’ will always fail if the node is reached again.ExampleAn example of an A* algorithm in action where nodes are cities connected with roads and h(x) is the     straight-line distance to target point:Key: green: start; blue: goal; orange: visitedThe A* algorithm also has real-world applications. In this example, edges are railroads and h(x) is the great-circle distance (the shortest possible distance on a sphere) to the target. The algorithm is searching for a path between Washington, D.C. and Los Angeles.Implementation detailsThere are a number of simple optimizations or implementation details that can significantly affect the performance of an A* implementation.  The first detail to note is that the way the priority queue handles ties can have a significant effect on performance in some situations.  If ties are broken so the queue behaves in a LIFO manner, A* will behave like depth-first search among equal cost paths (avoiding exploring more than one equally optimal solution).When a path is required at the end of the search, it is common to keep with each node a reference to that node\\'s parent.  At the end of the search these references can be used to recover the optimal path.  If these references are being kept then it can be important that the same node doesn\\'t appear in the priority queue more than once (each entry corresponding to a different path to the node, and each with a different cost).  A standard approach here is to check if a node about to be added already appears in the priority queue.  If it does, then the priority and parent pointers are changed to correspond to the lower cost path. A standard binary heap based priority queue does not directly support the operation of searching for one of its elements, but it can be augmented with a hash table that maps elements to their position in the heap, allowing this decrease-priority operation to be performed in logarithmic time. Alternatively, a Fibonacci heap can perform the same decrease-priority operations in constant amortized time.Special casesDijkstra\\'s algorithm, as another example of a uniform-cost search algorithm, can be viewed as a special case of A* whereh(x)=0{\\\\displaystyle h(x)=0}for all x. General depth-first search can be implemented using A* by considering that there is a global counter C initialized with a very large value. Every time we process a node we assign C to all of its newly discovered neighbors. After each single assignment, we decrease the counter C by one. Thus the earlier a node is discovered, the higher itsh(x){\\\\displaystyle h(x)}value. Both Dijkstra\\'s algorithm and depth-first search can be implemented more efficiently without including anh(x){\\\\displaystyle h(x)}value at each node.PropertiesTermination and CompletenessOn finite graphs with non-negative edge weights A* is guaranteed to terminate and is complete, i.e. it will always find a solution (a path from start to goal) if one exists. On infinite graphs with a finite branching factor and edge costs that are bounded away from zero (d(x,y)>ε>0{\\\\textstyle d(x,y)>\\\\varepsilon >0}for some fixedε{\\\\displaystyle \\\\varepsilon }), A* is guaranteed to terminate only if there exists a solution.AdmissibilityA search algorithm is said to be admissible if it is guaranteed to return an optimal solution. If the heuristic function used by A* is admissible, then A* is admissible. An intuitive ″proof″ of this  is as follows:When A* terminates its search, it has found a path from start to goal whose actual cost is lower than the estimated cost of any path from start to goal through any open node (the node\\'sf{\\\\displaystyle f}value). When the heuristic is admissible, those estimates are optimistic (not quite—see the next paragraph), so A* can safely ignore those nodes because they cannot possibly lead to a cheaper solution than the one it already has. In other words, A* will never overlook the possibility of a lower-cost path from start to goal and so it will continue to search until no such possibilities exist.The actual proof is a bit more involved because thef{\\\\displaystyle f}values of open nodes are not guaranteed to be optimistic even if the heuristic is admissible. This is because theg{\\\\displaystyle g}values of open nodes are not guaranteed to be optimal, so the sumg+h{\\\\displaystyle g+h}is not guaranteed to be optimistic.Optimality and ConsistencyAlgorithm A is optimally efficient with respect to a set of alternative algorithms Alts on a set of problems P if for every problem P in P and every algorithm A′ in Alts, the set of nodes expanded by A in solving P is a subset (possibly equal) of the set of nodes expanded by A′ in solving P. The definitive study of the optimal efficiency of A* is due to Rina Dechter and Judea Pearl.They considered a variety of  definitions of Alts and P  in combination with A*\\'s heuristic being merely admissible or being both consistent and admissible.  The most interesting positive result they proved is that A*, with a consistent heuristic, is optimally efficient with respect to all admissible A*-like search algorithms on all ″non-pathological″ search problems.  Roughly speaking, their notion of non-pathological problem is what we now mean by ″up to tie-breaking″.  This result does not hold if A*\\'s heuristic is admissible but not consistent. In that case, Dechter and Pearl showed there exist admissible A*-like algorithms that can expand arbitrarily fewer nodes than A* on some non-pathological problems.Optimal efficiency is about the set of nodes expanded, not the number of node expansions (the number of iterations of A*\\'s main loop).  When the heuristic being used is admissible but not consistent, it is possible for a node to be expanded by A* many times, an exponential number of times in the worst case.In such circumstances Dijkstra\\'s algorithm could outperform A* by a large margin. However, more recent research found that this pathological case only occurs in certain contrived situations where the edge weight of the search graph is exponential in the size of the graph, and that certain inconsistent (but admissible) heuristics can lead to a reduced number of node expansions in A* searches.Bounded relaxationWhile the admissibility criterion guarantees an optimal solution path, it also means that A* must examine all equally meritorious paths to find the optimal path. To compute approximate shortest paths, it is possible to speed up the search at the expense of optimality by relaxing the admissibility criterion. Oftentimes we want to bound this relaxation, so that we can guarantee that the solution path is no worse than (1 + ε) times the optimal solution path. This new guarantee is referred to as ε-admissible.There are a number of ε-admissible algorithms:Weighted A*/Static Weighting\\'s. If ha(n) is an admissible heuristic function, in the weighted version of the A* search one uses hw(n) = ε ha(n), ε > 1 as the heuristic function, and perform the A* search as usual (which eventually happens faster than using ha since fewer nodes are expanded). The path hence found by the search algorithm can have a cost of at most ε times that of the least cost path in the graph.Dynamic Weighting uses the cost functionf(n)=g(n)+(1+εw(n))h(n){\\\\displaystyle f(n)=g(n)+(1+\\\\varepsilon w(n))h(n)}, wherew(n)={1−d(n)Nd(n)≤N0otherwise{\\\\displaystyle w(n)={\\\\begin{cases}1-{\\\\frac {d(n)}{N}}&d(n)\\\\leq N\\\\\\\\0&{\\\\text{otherwise}}\\\\end{cases}}}, and whered(n){\\\\displaystyle d(n)}is the depth of the search and N is the anticipated length of the solution path.Sampled Dynamic Weighting uses sampling of nodes to better estimate and debias the heuristic error.Aε∗{\\\\displaystyle A_{\\\\varepsilon }^{*}}. uses two heuristic functions. The first is the FOCAL list, which is used to select candidate nodes, and the second hF is used to select the most promising node from the FOCAL list.Aε selects nodes with the functionAf(n)+BhF(n){\\\\displaystyle Af(n)+Bh_{F}(n)}, where A and B are constants. If no nodes can be selected, the algorithm will backtrack with the functionCf(n)+DhF(n){\\\\displaystyle Cf(n)+Dh_{F}(n)}, where C and D are constants.AlphA* attempts to promote depth-first exploitation by preferring recently expanded nodes. AlphA* uses the cost functionfα(n)=(1+wα(n))f(n){\\\\displaystyle f_{\\\\alpha }(n)=(1+w_{\\\\alpha }(n))f(n)}, wherewα(n)={λg(π(n))≤g(n~)Λotherwise{\\\\displaystyle w_{\\\\alpha }(n)={\\\\begin{cases}\\\\lambda &g(\\\\pi (n))\\\\leq g({\\\\tilde {n}})\\\\\\\\\\\\Lambda &{\\\\text{otherwise}}\\\\end{cases}}}, where λ and Λ are constants withλ≤Λ{\\\\displaystyle \\\\lambda \\\\leq \\\\Lambda }, π(n) is the parent of n, and ñ is the most recently expanded node.ComplexityThe time complexity of A* depends on the heuristic. In the worst case of an unbounded search space, the number of nodes expanded is exponential in the depth of the solution (the shortest path) d: O(bd), where b is the branching factor (the average number of successors per state). This assumes that a goal state exists at all, and is reachable from the start state; if it is not, and the state space is infinite, the algorithm will not terminate.The heuristic function has a major effect on the practical performance of A* search, since a good heuristic allows A* to prune away many of the bd nodes that an uninformed search would expand. Its quality can be expressed in terms of the effective branching factor b*, which can be determined empirically for a problem instance by measuring the number of nodes generated by expansion, N, and the depth of the solution, then solvingN+1=1+b∗+(b∗)2+⋯+(b∗)d.{\\\\displaystyle N+1=1+b^{*}+(b^{*})^{2}+\\\\dots +(b^{*})^{d}.}Good heuristics are those with low effective branching factor (the optimal being b* = 1).The time complexity is polynomial when the search space is a tree, there is a single goal state, and the heuristic function h meets the following condition:|h(x)−h∗(x)|=O(log\\u2061h∗(x)){\\\\displaystyle |h(x)-h^{*}(x)|=O(\\\\log h^{*}(x))}where h* is the optimal heuristic, the exact cost to get from x to the goal. In other words, the error of h will not grow faster than the logarithm of the \"perfect heuristic\" h* that returns the true distance from x to the goal.The space complexity of A* is roughly the same as that of all other graph search algorithms, as it keeps all generated nodes in memory. In practice, this turns out to be the biggest drawback of A* search, leading to the development of memory-bounded heuristic searches, such as Iterative deepening A*, memory bounded A*, and SMA*.ApplicationsA* is often used for the common pathfinding problem in applications such as video games, but was originally designed as a general graph traversal algorithm.It finds applications in diverse problems, including the problem of parsing using stochastic grammars in NLP.Other cases include an Informational search with online learning.Relations to other algorithmsWhat sets A* apart from a greedy best-first search algorithm is that it takes the cost/distance already traveled, g(n), into account.Some common variants of Dijkstra\\'s algorithm can be viewed as a special case of A* where the heuristich(n)=0{\\\\displaystyle h(n)=0}for all nodes; in turn, both Dijkstra and A* are special cases of dynamic programming.A* itself is a special case of a generalization of branch and bound.VariantsAnytime A*Block A*D*Field D*FringeFringe Saving A* (FSA*)Generalized Adaptive A* (GAA*)Incremental heuristic searchReduced A*Iterative deepening A* (IDA*)Jump point searchLifelong Planning A* (LPA*)New Bidirectional A* (NBA*)Simplified Memory bounded A* (SMA*)Theta*A* can also be adapted to a bidirectional search algorithm. Special care needs to be taken for the stopping criterion.See alsoBreadth-first searchDepth-first searchAny-angle path planning, search for paths that are not limited to move along graph edges but rather can take on any angleNotesReferencesFurther readingNilsson, N. J. (1980). Principles of Artificial Intelligence. Palo Alto, California: Tioga Publishing Company. ISBN 978-0-935382-01-3.External linksClear visual A* explanation, with advice and thoughts on path-findingVariation on A* called Hierarchical Path-Finding A* (HPA*)Brian Grinstead. \"A* Search Algorithm in JavaScript (Updated)\". Archived from the original on 15 February 2020. Retrieved 8 February 2021.',\n",
              " '= A5/1 =A5/1 is a stream cipher used to provide over-the-air communication privacy in the GSM cellular telephone standard. It is one of several implementations of the A5 security protocol. It was initially kept secret, but became public knowledge through leaks and reverse engineering. A number of serious weaknesses in the cipher have been identified.History and usageA5/1 is used in Europe and the United States. A5/2 was a deliberate weakening of the algorithm for certain export regions. A5/1 was developed in 1987, when GSM was not yet considered for use outside Europe, and A5/2 was developed in 1989. Though both were initially kept secret, the general design was leaked in 1994 and the algorithms were entirely reverse engineered in 1999 by Marc Briceno from a GSM telephone. In 2000, around 130 million GSM customers relied on A5/1 to protect the confidentiality of their voice communications.Security researcher Ross Anderson reported in 1994 that \"there was a terrific row between the NATO signal intelligence agencies in the mid-1980s over whether GSM encryption should be strong or not. The Germans said it should be, as they shared a long border with the Warsaw Pact; but the other countries didn\\'t feel this way, and the algorithm as now fielded is a French design.\"DescriptionA GSM transmission is organised as sequences of bursts. In a typical channel and in one direction, one burst is sent every 4.615 milliseconds and contains 114 bits available for information. A5/1 is used to produce for each burst a 114 bit sequence of keystream which is XORed with the 114 bits prior to modulation. A5/1 is initialised using a 64-bit key together with a publicly known 22-bit frame number. Older fielded GSM implementations using Comp128v1 for key generation, had 10 of the key bits fixed at zero, resulting in an effective key length of 54 bits. This weakness was rectified with the introduction of Comp128v3 which yields proper 64 bits keys. When operating in GPRS / EDGE mode, higher bandwidth radio modulation allows for larger 348 bits frames, and A5/3 is then used in a stream cipher mode to maintain confidentiality.A5/1 is based around a combination of three linear-feedback shift registers (LFSRs) with irregular clocking.  The three shift registers are specified as follows:The bits are indexed with the least significant bit (LSB) as 0.The registers are clocked in a stop/go fashion using a majority rule. Each register has an associated clocking bit. At each cycle, the clocking bit of all three registers is examined and the majority bit is determined. A register is clocked if the clocking bit agrees with the majority bit. Hence at each step at least two or three registers are clocked, and each register steps with probability 3/4.Initially, the registers are set to zero. Then for 64 cycles, the 64-bit secret key K is mixed in according to the following scheme: in cycle0≤i<64{\\\\displaystyle 0\\\\leq {i}<64}, the ith key bit is added to the least significant bit of each register using XOR —R[0]=R[0]⊕K[i].{\\\\displaystyle R[0]=R[0]\\\\oplus K[i].}Each register is then clocked.Similarly, the 22-bits of the frame number are added in 22 cycles. Then the entire system is clocked using the normal majority clocking mechanism for 100 cycles, with the output discarded. After this is completed, the cipher is ready to produce two 114 bit sequences of output keystream, first 114 for downlink, last 114 for uplink.SecurityA number of attacks on A5/1 have been published, and the American National Security Agency is able to routinely decrypt A5/1 messages according to released internal documents.Some attacks require an expensive preprocessing stage after which the cipher can be broken in minutes or seconds. Originally, the weaknesses were passive attacks using the known plaintext assumption. In 2003, more serious weaknesses were identified which can be exploited in the ciphertext-only scenario, or by an active attacker. In 2006 Elad Barkan, Eli Biham and Nathan Keller demonstrated attacks against A5/1, A5/3, or even GPRS that allow attackers to tap GSM mobile phone conversations and decrypt them either in real-time, or at any later time.According to professor Jan Arild Audestad, at the standardization process which started in 1982, A5/1 was originally proposed to have a key length of 128 bits. At that time, 128 bits was projected to be secure for at least 15 years. It is now believed that 128 bits would in fact also still be secure until the advent of quantum computing. Audestad, Peter van der Arend, and Thomas Haug says that the British insisted on weaker encryption, with Haug saying he was told by the British delegate that this was to allow the British secret service to eavesdrop more easily. The British proposed a key length of 48 bits, while the West Germans wanted stronger encryption to protect against East German spying, so the compromise became a key length of 54 bits.Known-plaintext attacksThe first attack on the A5/1 was proposed by Ross Anderson in 1994. Anderson\\'s basic idea was to guess the complete content of the registers R1 and R2 and about half of the register R3. In this way the clocking of all three registers is determined and the second half of R3 can be computed.In 1997, Golic presented an attack based on solving sets of linear equations which has a time complexity of 240.16 (the units are in terms of number of solutions of a system of linear equations which are required).In 2000, Alex Biryukov, Adi Shamir and David Wagner showed that A5/1 can be cryptanalysed in real time using a time-memory tradeoff attack, based on earlier work by Jovan Golic. One tradeoff allows an attacker to reconstruct the key in one second from two minutes of known plaintext or in several minutes from two seconds of known plain text, but he must first complete an expensive preprocessing stage which requires 248 steps to compute around 300 GB of data. Several tradeoffs between preprocessing, data requirements, attack time and memory complexity are possible.The same year, Eli Biham and Orr Dunkelman also published an attack on A5/1 with a total work complexity of 239.91 A5/1 clockings given 220.8 bits of known plaintext. The attack requires 32 GB of data storage after a precomputation stage of 238.Ekdahl and Johansson published an attack on the initialisation procedure which breaks A5/1 in a few minutes using two to five minutes of conversation plaintext. This attack does not require a preprocessing stage. In 2004, Maximov et al. improved this result to an attack requiring \"less than one minute of computations, and a few seconds of known conversation\". The attack was further improved by Elad Barkan and Eli Biham in 2005.Attacks on A5/1 as used in GSMIn 2003, Barkan et al. published several attacks on GSM encryption. The first is an active attack. GSM phones can be convinced to use the much weaker A5/2 cipher briefly. A5/2 can be broken easily, and the phone uses the same key as for the stronger A5/1 algorithm. A second attack on A5/1 is outlined, a ciphertext-only time-memory tradeoff attack which requires a large amount of precomputation.In 2006, Elad Barkan, Eli Biham, Nathan Keller published the full version of their 2003 paper, with attacks against A5/X сiphers. The authors claim: We present a very practical ciphertext-only cryptanalysis of GSM encrypted communication, and various active attacks on the GSM protocols. These attacks can even break into GSM networks that use \"unbreakable\" ciphers. We first describe a ciphertext-only attack on A5/2 that requires a few dozen milliseconds of encrypted off-the-air cellular conversation and finds the correct key in less than a second on a personal computer. We extend this attack to a (more complex) ciphertext-only attack on A5/1. We then describe new (active) attacks on the protocols of networks that use A5/1, A5/3, or even GPRS. These attacks exploit flaws in the GSM protocols, and they work whenever the mobile phone supports a weak cipher such as A5/2. We emphasize that these attacks are on the protocols, and are thus applicable whenever the cellular phone supports a weak cipher, for example, they are also applicable for attacking A5/3 networks using the cryptanalysis of A5/1. Unlike previous attacks on GSM that require unrealistic information, like long known plaintext periods, our attacks are very practical and do not require any knowledge of the content of the conversation. Furthermore, we describe how to fortify the attacks to withstand reception errors. As a result, our attacks allow attackers to tap conversations and decrypt them either in real-time, or at any later time.In 2007 Universities of Bochum and Kiel started a research project to create a massively parallel FPGA-based cryptographic accelerator COPACOBANA. COPACOBANA was the first commercially available solution using fast time-memory trade-off techniques that could be used to attack the popular A5/1 and A5/2 algorithms, used in GSM voice encryption, as well as the Data Encryption Standard (DES). It also enables brute force attacks against GSM eliminating the need of large precomputed lookup tables.In 2008, the group The Hackers Choice launched a project to develop a practical attack on A5/1. The attack requires the construction of a large look-up table of approximately 3 terabytes. Together with the scanning capabilities developed as part of the sister project, the group expected to be able to record any GSM call or SMS encrypted with A5/1, and within about 3–5 minutes derive the encryption key and hence listen to the call and read the SMS in clear.  But the tables weren\\'t released.A similar effort, the A5/1 Cracking Project, was announced at the 2009 Black Hat security conference by cryptographers Karsten Nohl and Sascha Krißler. It created the look-up tables using Nvidia GPGPUs via a peer-to-peer distributed computing architecture.  Starting in the middle of September 2009, the project ran the equivalent of 12 Nvidia GeForce GTX 260. According to the authors, the approach can be used on any cipher with key size up to 64-bits.In December 2009, the A5/1 Cracking Project attack tables for A5/1 were announced by Chris Paget and Karsten Nohl. The tables use a combination of compression techniques, including rainbow tables and distinguished point chains. These tables constituted only parts of the 1.7 TB completed table and had been computed during three months using 40 distributed CUDA nodes and then published over BitTorrent. More recently the project has announced a switch to faster ATI Evergreen code, together with a change in the format of the tables and Frank A. Stevenson announced breaks of A5/1 using the ATI generated tables.Documents leaked by Edward Snowden in 2013 state that the NSA \"can process encrypted A5/1\".See alsoA5/2KASUMI, also known as A5/3Cellular Message Encryption AlgorithmNotesReferencesRose, Greg (10 September 2003). \"A precis of the new attacks on GSM encryption\" (PDF). QUALCOMM Australia. Archived from the original (PDF) on 27 September 2011. Retrieved 17 October 2004.Maximov, Alexander; Thomas Johansson; Steve Babbage (2004). \"An Improved Correlation Attack on A5/1\". Selected Areas in Cryptography 2004: 1–18.External linksBriceno, Marc; Ian Goldberg; David Wagner (23 October 1999). \"A pedagogical implementation of the GSM A5/1 and A5/2 \"voice privacy\" encryption algorithms\". Archived from the original on 8 October 2018. Retrieved 23 January 2017.\"Huge GSM flaw allows hackers to listen in on voice calls\". 25 August 2009. Archived from the original on 14 October 2009.Horesh, Hadar (3 September 2003). \"Technion team cracks GSM cellular phone encryption\" (PDF). Haaretz. Archived from the original (PDF) on 3 March 2016. Retrieved 15 September 2019.Barkan, Elad; Eli Biham; Nathan Keller (July 2006). \"Instant Ciphertext-Only Cryptanalysis of GSM Encrypted Communication (Technical Report CS-2006-07)\".\"Nathan Keller\\'s Homepage\". Archived from the original on 4 June 2008.\"Animated SVG showing A5/1 stream cypher\". Archived from the original on 26 March 2012.',\n",
              " '= A5/2 =A5/2 is a stream cipher used to provide voice privacy in the GSM cellular telephone protocol. It was designed in 1992-1993 (finished March 1993) as a replacement for the relatively stronger (but still weak) A5/1, to allow the GSM standard to be exported to countries \"with restrictions on the import of products with cryptographic security features\".The cipher is based on a combination of four linear-feedback shift registers with irregular clocking and a non-linear combiner.In 1999, Ian Goldberg and David A. Wagner cryptanalyzed A5/2 in the same month it was reverse engineered, and showed that it was extremely weak – so much so that low end equipment can probably break it in real time.In 2003, Elad Barkan, Eli Biham and Nathan Keller presented a ciphertext-only attack based on the error correcting codes used in GSM communication. They also demonstrated a vulnerability in the GSM protocols that allows a man-in-the-middle attack to work whenever the mobile phone supports A5/2, regardless of whether is was actually being used.Since July 1, 2006, the GSMA (GSM Association) mandated that GSM Mobile Phones will not support the A5/2 Cipher any longer, due to its weakness, and the fact that A5/1 is deemed mandatory by the 3GPP association. In July 2007, the 3GPP has approved a change request to prohibit the implementation of A5/2 in any new mobile phones, stating: \"It is mandatory for A5/1 and non encrypted mode to be implemented in mobile stations. It is prohibited to implement A5/2 in mobile stations.\" If the network does not support A5/1 then an unencrypted connection can be used.See alsoA5/1KASUMI, also known as A5/3ReferencesExternal linksA5/2 at CryptoDoxA5/2 withdrawal at security.osmocom.orgIan Goldberg, David Wagner, Lucky Green. The (Real-Time) Cryptanalysis of A5/2. Rump session of Crypto\\'99, 1999.Barkam, Elad; Biham, Eli; Keller, Nathan (2008), \"Instant Ciphertext-Only Cryptanalysis of GSM Encrypted Communication\" (PDF), Journal of Cryptology, 21 (3): 392–429, doi:10.1007/s00145-007-9001-y, S2CID 459117Tool for cracking the GSM A5/2 cipher, written by Nicolas Paglieri and Olivier Benjamin: A52HackTool (with full source code – C language – GNU GPL)',\n",
              " '= ABC (stream cipher) =In cryptography, ABC is a stream cypher algorithm developed by Vladimir Anashin, Andrey Bogdanov, Ilya Kizhvatov, and Sandeep Kumar. It has been submitted to the eSTREAM Project of the eCRYPT network.== References ==',\n",
              " '= ADFGVX cipher =In cryptography, the ADFGVX cipher was a manually applied field cipher used by the Imperial German Army during World War I. It was used to transmit messages secretly using wireless telegraphy. ADFGVX was in fact an extension of an earlier cipher called ADFGX which was first used on 1 March 1918 on the German Western Front. ADFGVX was applied from 1 June 1918 on both the Western Front and Eastern Front.Invented by the Germans signal corps officers Lieutenant Fritz Nebel (1891–1977) and introduced in March 1918 with the designation \"Secret Cipher of the Radio Operators 1918\" (Geheimschrift der Funker 1918, in short GedeFu 18), the cipher was a fractionating transposition cipher which combined a modified Polybius square with a single columnar transposition.The cipher is named after the six possible letters used in the ciphertext: A, D, F, G, V and X. The letters were chosen deliberately because they are very different from one another in the Morse code. That reduced the possibility of operator error.Nebel designed the cipher to provide an army on the move with encryption that was more convenient than trench codes but was still secure. In fact, the Germans believed the ADFGVX cipher was unbreakable.OperationFor the plaintext message, \"Attack at once\", a secret mixed alphabet is first filled into a 5 × 5 Polybius square:i and j have been combined to make the alphabet fit into a 5 × 5 grid.By using the square, the message is converted to fractionated form:The first letter of each ciphertext pair is the row, and the second ciphertext letter is the column, of the plaintext letter in the grid (e.g., \"AF\" means \"row A, column F, in the grid\").Next, the fractionated message is subject to a columnar transposition. The message is written in rows under a transposition key (here \"CARGO\"):C A R G O_________A F A D AD A F G FD X A F AD D F F XG F X FNext, the letters are sorted alphabetically in the transposition key (changing CARGO to ACGOR) by rearranging the columns beneath the letters along with the letters themselves:A C G O R_________F A D A AA D G F FX D F A AD D F X FF G F   XThen, it is read off in columns, in keyword order, which yields the ciphertext:FAXDF ADDDG DGFFF AFAX AFAFXIn practice, the transposition keys were about two dozen characters long. Long messages sent in the ADFGX cipher were broken into sets of messages of different and irregular lengths to make it invulnerable to multiple anagramming. Both the transposition keys and the fractionation keys were changed daily.ADFGVXIn June 1918, an additional letter, V, was added to the cipher. That expanded the grid to 6 × 6, allowing 36 characters to be used. That allowed the full alphabet (instead of combining I and J) and the digits from 0 to 9. That mainly had the effect of considerably shortening messages containing many numbers.The cipher is based on the 6 letters ADFGVX. In the following example the alphabet is coded with the Dutch codeword \\'nachtbommenwerper\\'. This results in the alphabet: NACHTBOMEWRPDFGIJKLQSUVXYZ. This creates the table below with the letters ADFGVX as column headings and row identifiers:The text \\'attack at 1200am\\' translates to this:Then, a new table is created with a key as a heading. Let\\'s use \\'PRIVACY\\' as a key. Usually much longer keys or even phrases were used.The columns are sorted alphabetically, based on the keyword, and the table changes to this:Then, appending the columns to each other results in this ciphertext:DGDD DAGD DGAF ADDF DADV DVFA ADVXWith the keyword, the columns can be reconstructed and placed in the correct order. When using the original table containing the secret alphabet, the text can be deciphered.This cipher might be modified by transposing the rows as well as the columns, creating a harder but improved cipher.CryptanalysisADFGVX was cryptanalysed by French Army Lieutenant Georges Painvin, and the cipher was broken in early June 1918. The work was exceptionally difficult by the standards of classical cryptography, and Painvin became physically ill during it. His method of solution relied on finding messages with stereotyped beginnings, which would fractionate them and then form similar patterns in the positions in the ciphertext that had corresponded to column headings in the transposition table. (Considerable statistical analysis was required after that step had been reached, all done by hand.) It was thus effective only during times of very high traffic, but that was also when the most important messages were sent.However, that was not the only trick that Painvin used to crack the ADFGX cipher. He also used repeating sections of ciphertext to derive information about the likely length of the key that was being used. Where the key was an even number of letters in length he knew, by the way the message was enciphered, that each column consisted entirely of letter coordinates taken from the top of the Polybius Square or from the left of the Square, not a mixture of the two. Also, after substitution but before transposition, the columns would alternately consist entirely of \"top\" and \"side\" letters. One of the characteristics of frequency analysis of letters is that while the distributions of individual letters may vary widely from the norm, the law of averages dictates that groups of letters vary less. With the ADFGX cipher, each \"side\" letter or \"top\" letter is associated with five plaintext letters. In the example above, the \"side\" letter \"D\" is associated with the plaintext letters \"d h o z k\", and the \"top\" letter \"D\" is associated with the plaintext letters \"t h f j r\". Since the two groups of five letters have different cumulative frequency distributions, a frequency analysis of the \"D\" letter in columns consisting of \"side\" letters has a distinctively different result from those of the \"D\" letter in columns consisting of \"top\" letters. That trick allowed Painvin to guess which columns consisted of \"side\" letters and which columns consisted of \"top\" letters. He could then pair them up and perform a frequency analysis on the pairings to see if the pairings were only noise or corresponding to plaintext letters. Once he had the proper pairings, he could then use frequency analysis to figure out the actual plaintext letters. The result was still transposed, but to unscramble a simple transposition was all that he still had to do. Once he determined the transposition scheme for one message, he would then be able to crack any other message that was enciphered with the same transposition key.Painvin broke the ADFGX cipher in April 1918, a few weeks after the Germans launched their Spring Offensive. As a direct result, the French army discovered where Erich Ludendorff intended to attack. The French concentrated their forces at that point, which has been claimed to have stopped the Spring Offensive.However, the claim that Painvin\\'s breaking of the ADFGX cipher stopped the German spring offensive of 1918, while frequently made, is disputed by some. In his 2002 review of Sophie de Lastours\\' book on the subject, La France gagne la guerre des codes secrets 1914-1918, in the Journal of Intelligence History, (Journal of Intelligence History: volume 2, Number 2, Winter 2002)  Hilmar-Detlef Brückner stated:Regrettably, Sophie de Lastours subscribes to the traditional French view that the solving of a German ADFGVX-telegram by Painvin at the beginning of June 1918 was decisive for the Allied victory in the First World War because it gave timely warning of a forthcoming German offensive meant to reach Paris and to inflict a critical defeat on the Allies. However, it has been known for many years, that the German Gneisenau attack of 11 June was staged to induce the French High Command to rush in reserves from the area up north, where the Germans intended to attack later on.Its aim had to be grossly exaggerated, which the German High Command did by spreading rumors that the attack was heading for Paris and beyond; the disinformation was effective and apparently still is. However, the German offensive was not successful because the French had enough reserves at hand to stop the assault and so did not need to bring in additional reinforcements.Moreover, it is usually overlooked that the basic version of the ADFGVX cipher had been created especially for the German Spring Offensive in 1918, meant to deal the Allies a devastating blow. It was hoped that the cipher ADFGX would protect German communications against Allied cryptographers during the assault, which happened.Telegrams in ADFGX appeared for the first time on 5 March, and the German attack started on 21 March. When Painvin presented his first solution of the code on 5 April, the German offensive had already petered out.The ADFGX and ADFGVX ciphers are now regarded as insecure.ReferencesSourcesChilds, J. Rives, General Solution of the ADFGVX Cipher System, Aegean Park Press, ISBN 0-89412-284-3.Friedman, William F. Military Cryptanalysis, Part IV: Transposition and Fractionating Systems. Laguna Hills, California: Aegean Park Press, 1992.External linksA JavaScript implementation of the ADFGVX cipherAnother JavaScript implementationA C implementation of the ADFGVX cipher']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = input()\n",
        "do_search(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsX0XY_pnXOx",
        "outputId": "bd059703-8169-4f4a-83c3-61bacc64f510"
      },
      "execution_count": 41,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spanning Tree\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['= Kruskal\\'s algorithm =Kruskal\\'s algorithm finds a minimum spanning forest of an undirected edge-weighted graph. If the graph is connected, it finds a minimum spanning tree. (A minimum spanning tree of a connected graph is a subset of the edges that forms a tree that includes every vertex, where the sum of the weights of all the edges in the tree is minimized. For a disconnected graph, a minimum spanning forest is composed of a minimum spanning tree for each connected component.) It is a greedy algorithm in graph theory as in each step it adds the next lowest-weight edge that will not form a cycle to the minimum spanning forest.This algorithm first appeared in Proceedings of the American Mathematical Society, pp. 48–50 in 1956, and was written by Joseph Kruskal.Other algorithms for this problem include Prim\\'s algorithm, the reverse-delete algorithm, and Borůvka\\'s algorithm.Algorithmcreate a forest F (a set of trees), where each vertex in the graph is a separate treecreate a set S containing all the edges in the graphwhile S is nonempty and F is not yet spanningremove an edge with minimum weight from Sif the removed edge connects two different trees then add it to the forest F, combining two trees into a single treeAt the termination of the algorithm, the forest forms a minimum spanning forest of the graph. If the graph is connected, the forest has a single component and forms a minimum spanning tree.PseudocodeThe following code is implemented with a disjoint-set data structure. Here, we represent our forest F as a set of edges, and use the disjoint-set data structure to efficiently determine whether two vertices are part of the same tree.algorithm Kruskal(G) isF:= ∅for each v ∈ G.V doMAKE-SET(v)for each (u, v) in G.E ordered by weight(u, v), increasing doif FIND-SET(u) ≠ FIND-SET(v) thenF:= F ∪ {(u, v)} ∪ {(v, u)}UNION(FIND-SET(u), FIND-SET(v))return FComplexityFor a graph with E edges and V vertices, Kruskal\\'s algorithm can be shown to run in O(E log E) time, or equivalently, O(E log V) time, all with simple data structures. These running times are equivalent because:E is at mostV2{\\\\displaystyle V^{2}}andlog\\u2061V2=2log\\u2061V∈O(log\\u2061V){\\\\displaystyle \\\\log V^{2}=2\\\\log V\\\\in O(\\\\log V)}.Each isolated vertex is a separate component of the minimum spanning forest. If we ignore isolated vertices we obtain V ≤ 2E, so log V isO(log\\u2061E){\\\\displaystyle O(\\\\log E)}.We can achieve this bound as follows: first sort the edges by weight using a comparison sort in O(E log E) time; this allows the step \"remove an edge with minimum weight from S\" to operate in constant time. Next, we use a disjoint-set data structure to keep track of which vertices are in which components. We place each vertex into its own disjoint set, which takes O(V) operations. Finally, in worst case, we need to iterate through all edges, and for each edge we need to do two \\'find\\' operations and possibly one union. Even a simple disjoint-set data structure such as disjoint-set forests with union by rank can perform O(E) operations in O(E log V) time. Thus the total time is O(E log E) = O(E log V).Provided that the edges are either already sorted or can be sorted in linear time (for example with counting sort or radix sort), the algorithm can use a more sophisticated disjoint-set data structure to run in O(E α(V)) time, where α is the extremely slowly growing inverse of the single-valued Ackermann function.ExampleProof of correctnessThe proof consists of two parts. First, it is proved that the algorithm produces a spanning tree. Second, it is proved that the constructed spanning tree is of minimal weight.Spanning treeLetG{\\\\displaystyle G}be a connected, weighted graph and letY{\\\\displaystyle Y}be the subgraph ofG{\\\\displaystyle G}produced by the algorithm.Y{\\\\displaystyle Y}cannot have a cycle, as by definition an edge is not added if it results in a cycle.Y{\\\\displaystyle Y}cannot be disconnected, since the first encountered edge that joins two components ofY{\\\\displaystyle Y}would have been added by the algorithm. Thus,Y{\\\\displaystyle Y}is a spanning tree ofG{\\\\displaystyle G}.MinimalityWe show that the following proposition P is true by induction: If F is the set of edges chosen at any stage of the algorithm, then there is some minimum spanning tree that contains F and none of the edges rejected by the algorithm.Clearly P is true at the beginning, when F is empty: any minimum spanning tree will do, and there exists one because a weighted connected graph always has a minimum spanning tree.Now assume P is true for some non-final edge set F and let T be a minimum spanning tree that contains F.If the next chosen edge e is also in T, then P is true for F + e.Otherwise, if e is not in T then T + e has a cycle C. This cycle contains edges which do not belong to F, since e does not form a cycle when added to F but does in T.  Let f be an edge which is in C but not in F + e.  Note that f also belongs to T, and by  P has not been considered by the algorithm. f must therefore have a weight at least as large as e. Then T − f + e is a tree, and it has the same or less weight as T.  So T − f + e is a minimum spanning tree containing F + e and again P holds.Therefore, by the principle of induction, P holds when F has become a spanning tree, which is only possible if F is a minimum spanning tree itself.Parallel algorithmKruskal\\'s algorithm is inherently sequential and hard to parallelize. It is, however, possible to perform the initial sorting of the edges in parallel or, alternatively, to use a parallel implementation of a binary heap to extract the minimum-weight edge in every iteration.As parallel sorting is possible in timeO(n){\\\\displaystyle O(n)}onO(log\\u2061n){\\\\displaystyle O(\\\\log n)}processors, the runtime of Kruskal\\'s algorithm can be reduced to O(E α(V)), where α again is the inverse of the single-valued Ackermann function.A variant of Kruskal\\'s algorithm, named Filter-Kruskal, has been described by Osipov et al. and is better suited for parallelization. The basic idea behind Filter-Kruskal is to partition the edges in a similar way to quicksort and filter out edges that connect vertices of the same tree to reduce the cost of sorting. The following pseudocode demonstrates this.function filter_kruskal(G) isif |G.E| < kruskal_threshold:return kruskal(G)pivot = choose_random(G.E)E≤{\\\\displaystyle E_{\\\\leq }},E>{\\\\displaystyle E_{>}}= partition(G.E, pivot)A = filter_kruskal(E≤{\\\\displaystyle E_{\\\\leq }})E>{\\\\displaystyle E_{>}}= filter(E>{\\\\displaystyle E_{>}})A = A ∪ filter_kruskal(E>{\\\\displaystyle E_{>}})return Afunction partition(E, pivot) isE≤{\\\\displaystyle E_{\\\\leq }}= ∅,E>{\\\\displaystyle E_{>}}= ∅foreach (u, v) in E doif weight(u, v) <= pivot thenE≤{\\\\displaystyle E_{\\\\leq }}=E≤{\\\\displaystyle E_{\\\\leq }}∪ {(u, v)}elseE>{\\\\displaystyle E_{>}}=E>{\\\\displaystyle E_{>}}∪ {(u, v)}returnE≤{\\\\displaystyle E_{\\\\leq }},E>{\\\\displaystyle E_{>}}function filter(E) isEfiltered{\\\\displaystyle E_{\\\\text{filtered}}}= ∅foreach (u, v) in E doif find_set(u) ≠ find_set(v) thenEfiltered{\\\\displaystyle E_{\\\\text{filtered}}}=Efiltered{\\\\displaystyle E_{\\\\text{filtered}}}∪ {(u, v)}returnEfiltered{\\\\displaystyle E_{\\\\text{filtered}}}Filter-Kruskal lends itself better for parallelization as sorting, filtering, and partitioning can easily be performed in parallel by distributing the edges between the processors.Finally, other variants of a parallel implementation of Kruskal\\'s algorithm have been explored. Examples include a scheme that uses helper threads to remove edges that are definitely not part of the MST in the background, and a variant which runs the sequential algorithm on p subgraphs, then merges those subgraphs until only one, the final MST, remains.See alsoPrim\\'s algorithmDijkstra\\'s algorithmBorůvka\\'s algorithmReverse-delete algorithmSingle-linkage clusteringGreedy geometric spannerReferencesThomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to Algorithms, Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7. Section 23.2: The algorithms of Kruskal and Prim, pp. 567–574.Michael T. Goodrich and Roberto Tamassia. Data Structures and Algorithms in Java, Fourth Edition. John Wiley & Sons, Inc., 2006. ISBN 0-471-73884-0. Section 13.7.1: Kruskal\\'s Algorithm, pp. 632..External linksData for the article\\'s example.Gephi Plugin For Calculating a Minimum Spanning Tree source code.Kruskal\\'s Algorithm with example and program in c++Kruskal\\'s Algorithm code in C++ as applied to random numbersKruskal\\'s Algorithm code in Python with explanation',\n",
              " '= 1-planar graph =In topological graph theory, a 1-planar graph is a graph that can be drawn in the Euclidean plane in such a way that each edge has at most one crossing point, where it crosses a single additional edge. If a 1-planar graph, one of the most natural generalizations of planar graphs, is drawn that way, the drawing is called a 1-plane graph or 1-planar embedding of the graph.Coloring1-planar graphs were first studied by Ringel (1965), who showed that they can be colored with at most seven colors. Later, the precise number of colors needed to color these graphs, in the worst case, was shown to be six. The example of the complete graph K6, which is 1-planar, shows that 1-planar graphs may sometimes require six colors. However, the proof that six colors are always enough is more complicated.Ringel\\'s motivation was in trying to solve a variation of total coloring for planar graphs, in which one simultaneously colors the vertices and faces of a planar graph in such a way that no two adjacent vertices have the same color, no two adjacent faces have the same color, and no vertex and face that are adjacent to each other have the same color. This can obviously be done using eight colors by applying the four color theorem to the given graph and its dual graph separately, using two disjoint sets of four colors. However, fewer colors may be obtained by forming an auxiliary graph that has a vertex for each vertex or face of the given planar graph, and in which two auxiliary graph vertices are adjacent whenever they correspond to adjacent features of the given planar graph. A vertex coloring of the auxiliary graph corresponds to a vertex-face coloring of the original planar graph. This auxiliary graph is 1-planar, from which it follows that Ringel\\'s vertex-face coloring problem may also be solved with six colors. The graph K6 cannot be formed as an auxiliary graph in this way, but nevertheless the vertex-face coloring problem also sometimes requires six colors; for instance, if the planar graph to be colored is a triangular prism, then its eleven vertices and faces require six colors, because no three of them may be given a single color.Edge densityEvery 1-planar graph with n vertices has at most 4n − 8 edges. More strongly, each 1-planar drawing has at most n − 2 crossings; removing one edge from each crossing pair of edges leaves a planar graph, which can have at most 3n − 6 edges, from which the 4n − 8 bound on the number of edges in the original 1-planar graph immediately follows.  However, unlike planar graphs (for which all maximal planar graphs on a given vertex set have the same number of edges as each other), there exist maximal 1-planar graphs (graphs to which no additional edges can be added while preserving 1-planarity) that have significantly fewer than 4n − 8 edges. The bound of 4n − 8 on the maximum possible number of edges in a 1-planar graph can be used to show that the complete graph K7 on seven vertices is not 1-planar, because this graph has 21 edges and in this case 4n − 8 = 20 < 21.A 1-planar graph is said to be an optimal 1-planar graph if it has exactly 4n − 8 edges, the maximum possible. In a 1-planar embedding of an optimal 1-planar graph, the uncrossed edges necessarily form a quadrangulation (a polyhedral graph in which every face is a quadrilateral). Every quadrangulation gives rise to an optimal 1-planar graph in this way, by adding the two diagonals to each of its quadrilateral faces. It follows that every optimal 1-planar graph is Eulerian (all of its vertices have even degree), that the minimum degree in such a graph is six, and that every optimal 1-planar graph has at least eight vertices of degree exactly six. Additionally, every optimal 1-planar graph is 4-vertex-connected, and every 4-vertex cut in such a graph is a separating cycle in the underlying quadrangulation.The graphs that have straight 1-planar drawings (that is, drawings in which each edge is represented by a line segment, and in which each line segment is crossed by at most one other edge) have a slightly tighter bound of 4n − 9 on the maximum number of edges, achieved by infinitely many graphs.Complete multipartite graphsA complete classification of the 1-planar complete graphs, complete bipartite graphs, and more generally complete multipartite graphs is known. Every complete bipartite graph of the form K2,n is 1-planar, as is every complete tripartite graph of the form K1,1,n. Other than these infinite sets of examples, the only complete multipartite 1-planar graphs are K6, K1,1,1,6, K1,1,2,3, K2,2,2,2, K1,1,1,2,2, and their subgraphs. The minimal non-1-planar complete multipartite graphs are K3,7, K4,5, K1,3,4, K2,3,3, and K1,1,1,1,3.For instance, the complete bipartite graph K3,6 is 1-planar because it is a subgraph of K1,1,1,6, but K3,7 is not 1-planar.Computational complexityIt is NP-complete to test whether a given graph is 1-planar, and it remains NP-complete even for the graphs formed from planar graphs by adding a single edge and for graphs of bounded bandwidth. The problem is fixed-parameter tractable when parameterized by cyclomatic number or by tree-depth, so it may be solved in polynomial time when those parameters are bounded.In contrast to Fáry\\'s theorem for planar graphs, not every 1-planar graph may be drawn 1-planarly with straight line segments for its edges. However, testing whether a 1-planar drawing may be straightened in this way can be done in polynomial time. Additionally, every 3-vertex-connected 1-planar graph has a 1-planar drawing in which at most one edge, on the outer face of the drawing, has a bend in it. This drawing can be constructed in linear time from a 1-planar embedding of the graph. The 1-planar graphs have bounded book thickness, but some 1-planar graphs including K2,2,2,2 have book thickness at least four.1-planar graphs have bounded local treewidth, meaning that there is a (linear) function f such that the 1-planar graphs of diameter d have treewidth at most f(d); the same property holds more generally for the graphs that can be embedded onto a surface of bounded genus with a bounded number of crossings per edge. They also have separators, small sets of vertices the removal of which decomposes the graph into connected components whose size is a constant fraction of the size of the whole graph. Based on these properties, numerous algorithms for planar graphs, such as Baker\\'s technique for designing approximation algorithms, can be extended to 1-planar graphs. For instance, this method leads to a polynomial-time approximation scheme for the maximum independent set of a 1-planar graph.Generalizations and related conceptsThe class of graphs analogous to outerplanar graphs for 1-planarity are called the outer-1-planar graphs. These are graphs that can be drawn in a disk, with the vertices on the boundary of the disk, and with at most one crossing per edge. These graphs can always be drawn (in an outer-1-planar way) with straight edges and right angle crossings. By using dynamic programming on the SPQR tree of a given graph, it is possible to test whether it is outer-1-planar in linear time. The triconnected components of the graph (nodes of the SPQR tree) can consist only of cycle graphs, bond graphs, and four-vertex complete graphs, from which it also follows that outer-1-planar graphs are planar and have treewidth at most three.The 1-planar graphs include the 4-map graphs, graphs formed from the adjacencies of regions in the plane with at most four regions meeting in any point. Conversely, every optimal 1-planar graph is a 4-map graph. However, 1-planar graphs that are not optimal 1-planar may not be map graphs.1-planar graphs have been generalized to k-planar graphs, graphs for which each edge is crossed at most k times (0-planar graphs are exactly the planar graphs). Ringel defined the local crossing number of G to be the least non-negative integer k such that G has a k-planar drawing. Because the local crossing number is the maximum degree of the intersection graph of the edges of an optimal drawing, and the thickness (minimum number of planar graphs into which the edges can be partitioned) can be seen as the chromatic number of an intersection graph of an appropriate drawing, it follows from Brooks\\' theorem that the thickness is at most one plus the local crossing number. The k-planar graphs with n vertices have at most O(k1/2n) edges, and treewidth O((kn)1/2). A shallow minor of a k-planar graph, with depth d, is itself a (2d + 1)k-planar graph, so the shallow minors of 1-planar graphs and of k-planar graphs are also sparse graphs, implying that the 1-planar and k-planar graphs have bounded expansion.Nonplanar graphs may also be parameterized by their crossing number, the minimum number of pairs of edges that cross in any drawing of the graph. A graph with crossing number k is necessarily k-planar, but not necessarily vice versa. For instance, the Heawood graph has crossing number 3, but it is not necessary for its three crossings to all occur on the same edge of the graph, so it is 1-planar, and can in fact be drawn in a way that simultaneously optimizes the total number of crossings and the crossings per edge.Another related concept for nonplanar graphs is graph skewness, the minimal number of edges that must be removed to make a graph planar.ReferencesFurther readingKobourov, Stephen; Liotta, Giuseppe; Montecchiani, Fabrizio (2017), \"An annotated bibliography on 1-planarity\", Computer Science Review, 25: 49–67, arXiv:1703.02261, Bibcode:2017arXiv170302261K, doi:10.1016/j.cosrev.2017.06.002, S2CID 7732463',\n",
              " '= 26-fullerene graph =In the mathematical field of graph theory, the 26-fullerene graph is a polyhedral graph with V = 26 vertices and E = 39 edges. Its planar embedding has three hexagonal faces (including the one shown as the external face of the illustration) and twelve pentagonal faces. As a planar graph with only pentagonal and hexagonal faces, meeting in three faces per vertex, this graph is a fullerene. The existence of this fullerene has been known since at least 1968.PropertiesThe 26-fullerene graph hasD3h{\\\\displaystyle D_{3h}}prismatic symmetry, the same group of symmetries as the triangular prism. This symmetry group has 12 elements; it has six symmetries that arbitrarily permute the three hexagonal faces of the graph and preserve the orientation of its planar embedding, and another six orientation-reversing symmetries.The number of fullerenes with a given even number of vertices grows quickly in the number of vertices; 26 is the largest number of vertices for which the fullerene structure is unique. The only two smaller fullerenes are the graph of the regular dodecahedron (a fullerene with 20 vertices) and the graph of the truncated hexagonal trapezohedron (a 24-vertex fullerene), which are the two types of cells in the Weaire–Phelan structure.The 26-fullerene graph has many perfect matchings. One must remove at least five edges from the graph in order to obtain a subgraph that has exactly one perfect matching. This is a unique property of this graph among fullerenes in the sense that, for every other number of vertices of a fullerene, there exists at least one fullerene from which one can remove four edges to obtain a subgraph with a unique perfect matching.The vertices of the 26-fullerene graph can be labeled with sequences of 12 bits, in such a way that distance in the graph equals half of the Hamming distance between these bitvectors.This can also be interpreted as an isometric embedding from the graph into a 12-dimensional taxicab geometry. The 26-fullerene graph is one of only five fullerenes with such an embedding.In popular cultureIn 2009, The New York Times published a puzzle involving Hamiltonian paths in this graph, taking advantage of the correspondence between its 26 vertices and the 26 letters of the English alphabet.== References ==',\n",
              " '= 3-Way =In cryptography, 3-Way is a block cipher designed in 1994 by Joan Daemen. It is closely related to BaseKing; the two are variants of the same general cipher technique.3-Way has a block size of 96 bits, notably not a power of two such as the more common 64 or 128 bits. The key length is also 96 bits. The figure 96 arises from the use of three 32 bit words in the algorithm, from which also is derived the cipher\\'s name. When 3-Way was invented, 96-bit keys and blocks were quite strong, but more recent ciphers have a 128-bit block, and few now have keys shorter than 128 bits. 3-Way is an 11-round substitution–permutation network.3-Way is designed to be very efficient in a wide range of platforms from 8-bit processors to specialized hardware, and has some elegant mathematical features which enable nearly all the decryption to be done in exactly the same circuits as did the encryption.3-Way, just as its counterpart BaseKing, is vulnerable to related key cryptanalysis. John Kelsey, Bruce Schneier, and David Wagner showed how it can be broken with one related key query and about222{\\\\displaystyle 2^{22}}chosen plaintexts.ReferencesJ. Daemen; R. Govaerts; Joos Vandewalle (1993). \"A New Approach to Block Cipher Design\". Fast Software Encryption (FSE) 1993. Springer-Verlag. pp. 18–32.J. Kelsey; B. Schneier; D. Wagner (November 1997). \"Related-Key Cryptanalysis of 3-WAY, Biham-DES, CAST, DES-X, NewDES, RC2, and TEA\" (PDF/PostScript). ICICS \\'97 Proceedings. Springer-Verlag. pp. 233–246. Retrieved 2007-02-14.External linksSCAN\\'s entry for 3-WayChapter 7 of Daemen\\'s thesis (gzipped Postscript)',\n",
              " '= 3D Topicscape =3D Topicscape, a software application, is a Personal Information Manager that provides a template loosely based on mind-mapping  or concept mapping.  It presents the mind map as a 3D scene where each node is a cone (or pyramid, or variation on such a shape).  It can also display in a 2D format.  Nodes are arranged in a way that indicates how they are related in much the same way as a mind map.  In addition to its use for information management it is claimed to be suitable as a task manager, and for use in project management.A Topicscape is created by importing folders (by Drag-and-drop or menus), importing from other mind mapping software including FreeMind, PersonalBrain and MindManager or by hand with mouse clicks or keyboard shortcuts.  Import sources may be converted to a new Topicscape or added as a portion of an existing one.The number of levels that can be stored is not limited, but up to seven levels of the hierarchy may be viewed at once.  Any node may be chosen as the centre of the 3D scene and choosing one at the edge will cause more to come into view.Topicscape\\'s most obvious difference from 2D mind mapping software is that it provides a zooming interface and simulates flying as noted by Wall Street Journal columnist Jeremy Wagstaff in his column \"Fly through your computer.\"  The BBC World Service and PC World  have also reviewed 3D Topicscape.Versions3D Topicscape public Beta in Jan 20063D Topicscape 1.0, May 20063D Topicscape Lite 1.05; 1.07, Dec 2007; 1.2, Aug 20083D Topicscape Pro 1.2, Feb 2007; 1.3, May 2007; 1.56, Dec 2007; 1.59, May 2008; 1.6, Jul 2008;  1.63, Sep 2008; 2.0, Apr 2009; 2.5 Dec 2009;  2.6 Feb 2010; 2.7 April 20103D Topicscape Student Edition Beta, Sep 2007; 1.0, Feb 2008; 2.0, Dec 2009File FormatUses an embedded Firebird relational database to store user-provided and operational metadata.  Files attached to nodes (topics) may be linked to in their original location or be held in a folder (directory) associated with a given Topicscape.  Links to files in a Topicscape\\'s folder are relative.  Topicscape folders may therefore be moved without breaking such links.Import file formats supported include FreeMind, OML, MindManager versions 5-8, PersonalBrain, and text (outline-numbered);Export file formats can be those for FreeMind, OPML, HTML and text structured for re-import, or text for reading.See alsoBrainstormingList of concept- and mind-mapping softwarePersonal information managersMind map== References ==',\n",
              " '= A* search algorithm =A* (pronounced \"A-star\") is a graph traversal and path search algorithm, which is often used in many fields of computer science due to its completeness, optimality, and optimal efficiency. One major practical drawback is itsO(bd){\\\\displaystyle O(b^{d})}space complexity, as it stores all generated nodes in memory. Thus, in practical travel-routing systems, it is generally outperformed by algorithms which can pre-process the graph to attain better performance, as well as memory-bounded approaches; however, A* is still the best solution in many cases.Peter Hart, Nils Nilsson and Bertram Raphael of Stanford Research Institute (now SRI International) first published the algorithm in 1968. It can be seen as an extension of Dijkstra\\'s algorithm. A* achieves better performance by using heuristics to guide its search.HistoryA* was created as part of the Shakey project, which had the aim of building a mobile robot that could plan its own actions. Nils Nilsson originally proposed using the Graph Traverser algorithm for Shakey\\'s path planning. Graph Traverser is guided by a heuristic function h(n), the estimated distance from node n to the goal node: it entirely ignores g(n), the distance from the start node to n. Bertram Raphael suggested using the sum, g(n) + h(n). Peter Hart invented the concepts we now call admissibility and consistency of heuristic functions. A* was originally designed for finding least-cost paths when the cost of a path is the sum of its  costs, but it has been shown that A* can be used to find optimal paths for any problem satisfying the conditions of a cost algebra.The original 1968 A* paper contained a theorem stating that no A*-like algorithm could expand fewer nodes than A* if the heuristic function is consistent and A*\\'s tie-breaking rule is suitably chosen. A ″correction″ was published a few years later claiming that consistency was not required, but this was shown to be false in Dechter and Pearl\\'s definitive study of A*\\'s optimality (now called optimal efficiency), which gave an example of A* with a heuristic that was admissible but not consistent expanding arbitrarily more nodes than an alternative A*-like algorithm.DescriptionA* is an informed search algorithm, or a best-first search, meaning that it is formulated in terms of weighted graphs: starting from a specific starting node of a graph, it aims to find a path to the given goal node having the smallest cost (least distance travelled, shortest time, etc.).  It does this by maintaining a tree of paths originating at the start node and extending those paths one edge at a time until its termination criterion is satisfied.At each iteration of its main loop, A* needs to determine which of its paths to extend. It does so based on the cost of the path and an estimate of the cost required to extend the path all the way to the goal. Specifically, A* selects the path that minimizesf(n)=g(n)+h(n){\\\\displaystyle f(n)=g(n)+h(n)}where n is the next node on the path, g(n) is the cost of the path from the start node to n, and h(n) is a heuristic function that estimates the cost of the cheapest path from n to the goal. A* terminates when the path it chooses to extend is a path from start to goal or if there are no paths eligible to be extended. The heuristic function is problem-specific. If the heuristic function is admissible, meaning that it never overestimates the actual cost to get to the goal, A* is guaranteed to return a least-cost path from start to goal.Typical implementations of A* use a priority queue to perform the repeated selection of minimum (estimated) cost nodes to expand. This priority queue is known as the open set or fringe. At each step of the algorithm, the node with the lowest f(x) value is removed from the queue, the f and g values of its neighbors are updated accordingly, and these neighbors are added to the queue. The algorithm continues until a removed node (thus the node with the lowest f value out of all fringe nodes) is a goal node. The f value of that goal is then also the cost of the shortest path, since h at the goal is zero in an admissible heuristic.The algorithm described so far gives us only the length of the shortest path. To find the actual sequence of steps, the algorithm can be easily revised so that each node on the path keeps track of its predecessor. After this algorithm is run, the ending node will point to its predecessor, and so on, until some node\\'s predecessor is the start node.As an example, when searching for the shortest route on a map, h(x) might represent the straight-line distance to the goal, since that is physically the smallest possible distance between any two points. For a grid map from a video game, using the Manhattan distance or the octile distance becomes better depending on the set of movements available (4-way or 8-way).If the heuristic h satisfies the additional condition h(x) ≤ d(x, y) + h(y) for every edge (x, y) of the graph (where d denotes the length of that edge), then h is called monotone, or consistent. With a consistent heuristic, A* is guaranteed to find an optimal path without processing any node more than once and A* is equivalent to running Dijkstra\\'s algorithm with the reduced cost d\\'(x, y) = d(x, y) + h(y) − h(x).PseudocodeThe following pseudocode describes the algorithm:Remark: In this pseudocode, if a node is reached by one path, removed from openSet, and subsequently reached by a cheaper path, it will be added to openSet again. This is essential to guarantee that the path returned is optimal if the heuristic function is admissible but not  consistent.   If the heuristic is consistent, when a node is removed from openSet the path to it is guaranteed to be optimal so the test ‘tentative_gScore < gScore[neighbor]’ will always fail if the node is reached again.ExampleAn example of an A* algorithm in action where nodes are cities connected with roads and h(x) is the     straight-line distance to target point:Key: green: start; blue: goal; orange: visitedThe A* algorithm also has real-world applications. In this example, edges are railroads and h(x) is the great-circle distance (the shortest possible distance on a sphere) to the target. The algorithm is searching for a path between Washington, D.C. and Los Angeles.Implementation detailsThere are a number of simple optimizations or implementation details that can significantly affect the performance of an A* implementation.  The first detail to note is that the way the priority queue handles ties can have a significant effect on performance in some situations.  If ties are broken so the queue behaves in a LIFO manner, A* will behave like depth-first search among equal cost paths (avoiding exploring more than one equally optimal solution).When a path is required at the end of the search, it is common to keep with each node a reference to that node\\'s parent.  At the end of the search these references can be used to recover the optimal path.  If these references are being kept then it can be important that the same node doesn\\'t appear in the priority queue more than once (each entry corresponding to a different path to the node, and each with a different cost).  A standard approach here is to check if a node about to be added already appears in the priority queue.  If it does, then the priority and parent pointers are changed to correspond to the lower cost path. A standard binary heap based priority queue does not directly support the operation of searching for one of its elements, but it can be augmented with a hash table that maps elements to their position in the heap, allowing this decrease-priority operation to be performed in logarithmic time. Alternatively, a Fibonacci heap can perform the same decrease-priority operations in constant amortized time.Special casesDijkstra\\'s algorithm, as another example of a uniform-cost search algorithm, can be viewed as a special case of A* whereh(x)=0{\\\\displaystyle h(x)=0}for all x. General depth-first search can be implemented using A* by considering that there is a global counter C initialized with a very large value. Every time we process a node we assign C to all of its newly discovered neighbors. After each single assignment, we decrease the counter C by one. Thus the earlier a node is discovered, the higher itsh(x){\\\\displaystyle h(x)}value. Both Dijkstra\\'s algorithm and depth-first search can be implemented more efficiently without including anh(x){\\\\displaystyle h(x)}value at each node.PropertiesTermination and CompletenessOn finite graphs with non-negative edge weights A* is guaranteed to terminate and is complete, i.e. it will always find a solution (a path from start to goal) if one exists. On infinite graphs with a finite branching factor and edge costs that are bounded away from zero (d(x,y)>ε>0{\\\\textstyle d(x,y)>\\\\varepsilon >0}for some fixedε{\\\\displaystyle \\\\varepsilon }), A* is guaranteed to terminate only if there exists a solution.AdmissibilityA search algorithm is said to be admissible if it is guaranteed to return an optimal solution. If the heuristic function used by A* is admissible, then A* is admissible. An intuitive ″proof″ of this  is as follows:When A* terminates its search, it has found a path from start to goal whose actual cost is lower than the estimated cost of any path from start to goal through any open node (the node\\'sf{\\\\displaystyle f}value). When the heuristic is admissible, those estimates are optimistic (not quite—see the next paragraph), so A* can safely ignore those nodes because they cannot possibly lead to a cheaper solution than the one it already has. In other words, A* will never overlook the possibility of a lower-cost path from start to goal and so it will continue to search until no such possibilities exist.The actual proof is a bit more involved because thef{\\\\displaystyle f}values of open nodes are not guaranteed to be optimistic even if the heuristic is admissible. This is because theg{\\\\displaystyle g}values of open nodes are not guaranteed to be optimal, so the sumg+h{\\\\displaystyle g+h}is not guaranteed to be optimistic.Optimality and ConsistencyAlgorithm A is optimally efficient with respect to a set of alternative algorithms Alts on a set of problems P if for every problem P in P and every algorithm A′ in Alts, the set of nodes expanded by A in solving P is a subset (possibly equal) of the set of nodes expanded by A′ in solving P. The definitive study of the optimal efficiency of A* is due to Rina Dechter and Judea Pearl.They considered a variety of  definitions of Alts and P  in combination with A*\\'s heuristic being merely admissible or being both consistent and admissible.  The most interesting positive result they proved is that A*, with a consistent heuristic, is optimally efficient with respect to all admissible A*-like search algorithms on all ″non-pathological″ search problems.  Roughly speaking, their notion of non-pathological problem is what we now mean by ″up to tie-breaking″.  This result does not hold if A*\\'s heuristic is admissible but not consistent. In that case, Dechter and Pearl showed there exist admissible A*-like algorithms that can expand arbitrarily fewer nodes than A* on some non-pathological problems.Optimal efficiency is about the set of nodes expanded, not the number of node expansions (the number of iterations of A*\\'s main loop).  When the heuristic being used is admissible but not consistent, it is possible for a node to be expanded by A* many times, an exponential number of times in the worst case.In such circumstances Dijkstra\\'s algorithm could outperform A* by a large margin. However, more recent research found that this pathological case only occurs in certain contrived situations where the edge weight of the search graph is exponential in the size of the graph, and that certain inconsistent (but admissible) heuristics can lead to a reduced number of node expansions in A* searches.Bounded relaxationWhile the admissibility criterion guarantees an optimal solution path, it also means that A* must examine all equally meritorious paths to find the optimal path. To compute approximate shortest paths, it is possible to speed up the search at the expense of optimality by relaxing the admissibility criterion. Oftentimes we want to bound this relaxation, so that we can guarantee that the solution path is no worse than (1 + ε) times the optimal solution path. This new guarantee is referred to as ε-admissible.There are a number of ε-admissible algorithms:Weighted A*/Static Weighting\\'s. If ha(n) is an admissible heuristic function, in the weighted version of the A* search one uses hw(n) = ε ha(n), ε > 1 as the heuristic function, and perform the A* search as usual (which eventually happens faster than using ha since fewer nodes are expanded). The path hence found by the search algorithm can have a cost of at most ε times that of the least cost path in the graph.Dynamic Weighting uses the cost functionf(n)=g(n)+(1+εw(n))h(n){\\\\displaystyle f(n)=g(n)+(1+\\\\varepsilon w(n))h(n)}, wherew(n)={1−d(n)Nd(n)≤N0otherwise{\\\\displaystyle w(n)={\\\\begin{cases}1-{\\\\frac {d(n)}{N}}&d(n)\\\\leq N\\\\\\\\0&{\\\\text{otherwise}}\\\\end{cases}}}, and whered(n){\\\\displaystyle d(n)}is the depth of the search and N is the anticipated length of the solution path.Sampled Dynamic Weighting uses sampling of nodes to better estimate and debias the heuristic error.Aε∗{\\\\displaystyle A_{\\\\varepsilon }^{*}}. uses two heuristic functions. The first is the FOCAL list, which is used to select candidate nodes, and the second hF is used to select the most promising node from the FOCAL list.Aε selects nodes with the functionAf(n)+BhF(n){\\\\displaystyle Af(n)+Bh_{F}(n)}, where A and B are constants. If no nodes can be selected, the algorithm will backtrack with the functionCf(n)+DhF(n){\\\\displaystyle Cf(n)+Dh_{F}(n)}, where C and D are constants.AlphA* attempts to promote depth-first exploitation by preferring recently expanded nodes. AlphA* uses the cost functionfα(n)=(1+wα(n))f(n){\\\\displaystyle f_{\\\\alpha }(n)=(1+w_{\\\\alpha }(n))f(n)}, wherewα(n)={λg(π(n))≤g(n~)Λotherwise{\\\\displaystyle w_{\\\\alpha }(n)={\\\\begin{cases}\\\\lambda &g(\\\\pi (n))\\\\leq g({\\\\tilde {n}})\\\\\\\\\\\\Lambda &{\\\\text{otherwise}}\\\\end{cases}}}, where λ and Λ are constants withλ≤Λ{\\\\displaystyle \\\\lambda \\\\leq \\\\Lambda }, π(n) is the parent of n, and ñ is the most recently expanded node.ComplexityThe time complexity of A* depends on the heuristic. In the worst case of an unbounded search space, the number of nodes expanded is exponential in the depth of the solution (the shortest path) d: O(bd), where b is the branching factor (the average number of successors per state). This assumes that a goal state exists at all, and is reachable from the start state; if it is not, and the state space is infinite, the algorithm will not terminate.The heuristic function has a major effect on the practical performance of A* search, since a good heuristic allows A* to prune away many of the bd nodes that an uninformed search would expand. Its quality can be expressed in terms of the effective branching factor b*, which can be determined empirically for a problem instance by measuring the number of nodes generated by expansion, N, and the depth of the solution, then solvingN+1=1+b∗+(b∗)2+⋯+(b∗)d.{\\\\displaystyle N+1=1+b^{*}+(b^{*})^{2}+\\\\dots +(b^{*})^{d}.}Good heuristics are those with low effective branching factor (the optimal being b* = 1).The time complexity is polynomial when the search space is a tree, there is a single goal state, and the heuristic function h meets the following condition:|h(x)−h∗(x)|=O(log\\u2061h∗(x)){\\\\displaystyle |h(x)-h^{*}(x)|=O(\\\\log h^{*}(x))}where h* is the optimal heuristic, the exact cost to get from x to the goal. In other words, the error of h will not grow faster than the logarithm of the \"perfect heuristic\" h* that returns the true distance from x to the goal.The space complexity of A* is roughly the same as that of all other graph search algorithms, as it keeps all generated nodes in memory. In practice, this turns out to be the biggest drawback of A* search, leading to the development of memory-bounded heuristic searches, such as Iterative deepening A*, memory bounded A*, and SMA*.ApplicationsA* is often used for the common pathfinding problem in applications such as video games, but was originally designed as a general graph traversal algorithm.It finds applications in diverse problems, including the problem of parsing using stochastic grammars in NLP.Other cases include an Informational search with online learning.Relations to other algorithmsWhat sets A* apart from a greedy best-first search algorithm is that it takes the cost/distance already traveled, g(n), into account.Some common variants of Dijkstra\\'s algorithm can be viewed as a special case of A* where the heuristich(n)=0{\\\\displaystyle h(n)=0}for all nodes; in turn, both Dijkstra and A* are special cases of dynamic programming.A* itself is a special case of a generalization of branch and bound.VariantsAnytime A*Block A*D*Field D*FringeFringe Saving A* (FSA*)Generalized Adaptive A* (GAA*)Incremental heuristic searchReduced A*Iterative deepening A* (IDA*)Jump point searchLifelong Planning A* (LPA*)New Bidirectional A* (NBA*)Simplified Memory bounded A* (SMA*)Theta*A* can also be adapted to a bidirectional search algorithm. Special care needs to be taken for the stopping criterion.See alsoBreadth-first searchDepth-first searchAny-angle path planning, search for paths that are not limited to move along graph edges but rather can take on any angleNotesReferencesFurther readingNilsson, N. J. (1980). Principles of Artificial Intelligence. Palo Alto, California: Tioga Publishing Company. ISBN 978-0-935382-01-3.External linksClear visual A* explanation, with advice and thoughts on path-findingVariation on A* called Hierarchical Path-Finding A* (HPA*)Brian Grinstead. \"A* Search Algorithm in JavaScript (Updated)\". Archived from the original on 15 February 2020. Retrieved 8 February 2021.',\n",
              " '= A5/1 =A5/1 is a stream cipher used to provide over-the-air communication privacy in the GSM cellular telephone standard. It is one of several implementations of the A5 security protocol. It was initially kept secret, but became public knowledge through leaks and reverse engineering. A number of serious weaknesses in the cipher have been identified.History and usageA5/1 is used in Europe and the United States. A5/2 was a deliberate weakening of the algorithm for certain export regions. A5/1 was developed in 1987, when GSM was not yet considered for use outside Europe, and A5/2 was developed in 1989. Though both were initially kept secret, the general design was leaked in 1994 and the algorithms were entirely reverse engineered in 1999 by Marc Briceno from a GSM telephone. In 2000, around 130 million GSM customers relied on A5/1 to protect the confidentiality of their voice communications.Security researcher Ross Anderson reported in 1994 that \"there was a terrific row between the NATO signal intelligence agencies in the mid-1980s over whether GSM encryption should be strong or not. The Germans said it should be, as they shared a long border with the Warsaw Pact; but the other countries didn\\'t feel this way, and the algorithm as now fielded is a French design.\"DescriptionA GSM transmission is organised as sequences of bursts. In a typical channel and in one direction, one burst is sent every 4.615 milliseconds and contains 114 bits available for information. A5/1 is used to produce for each burst a 114 bit sequence of keystream which is XORed with the 114 bits prior to modulation. A5/1 is initialised using a 64-bit key together with a publicly known 22-bit frame number. Older fielded GSM implementations using Comp128v1 for key generation, had 10 of the key bits fixed at zero, resulting in an effective key length of 54 bits. This weakness was rectified with the introduction of Comp128v3 which yields proper 64 bits keys. When operating in GPRS / EDGE mode, higher bandwidth radio modulation allows for larger 348 bits frames, and A5/3 is then used in a stream cipher mode to maintain confidentiality.A5/1 is based around a combination of three linear-feedback shift registers (LFSRs) with irregular clocking.  The three shift registers are specified as follows:The bits are indexed with the least significant bit (LSB) as 0.The registers are clocked in a stop/go fashion using a majority rule. Each register has an associated clocking bit. At each cycle, the clocking bit of all three registers is examined and the majority bit is determined. A register is clocked if the clocking bit agrees with the majority bit. Hence at each step at least two or three registers are clocked, and each register steps with probability 3/4.Initially, the registers are set to zero. Then for 64 cycles, the 64-bit secret key K is mixed in according to the following scheme: in cycle0≤i<64{\\\\displaystyle 0\\\\leq {i}<64}, the ith key bit is added to the least significant bit of each register using XOR —R[0]=R[0]⊕K[i].{\\\\displaystyle R[0]=R[0]\\\\oplus K[i].}Each register is then clocked.Similarly, the 22-bits of the frame number are added in 22 cycles. Then the entire system is clocked using the normal majority clocking mechanism for 100 cycles, with the output discarded. After this is completed, the cipher is ready to produce two 114 bit sequences of output keystream, first 114 for downlink, last 114 for uplink.SecurityA number of attacks on A5/1 have been published, and the American National Security Agency is able to routinely decrypt A5/1 messages according to released internal documents.Some attacks require an expensive preprocessing stage after which the cipher can be broken in minutes or seconds. Originally, the weaknesses were passive attacks using the known plaintext assumption. In 2003, more serious weaknesses were identified which can be exploited in the ciphertext-only scenario, or by an active attacker. In 2006 Elad Barkan, Eli Biham and Nathan Keller demonstrated attacks against A5/1, A5/3, or even GPRS that allow attackers to tap GSM mobile phone conversations and decrypt them either in real-time, or at any later time.According to professor Jan Arild Audestad, at the standardization process which started in 1982, A5/1 was originally proposed to have a key length of 128 bits. At that time, 128 bits was projected to be secure for at least 15 years. It is now believed that 128 bits would in fact also still be secure until the advent of quantum computing. Audestad, Peter van der Arend, and Thomas Haug says that the British insisted on weaker encryption, with Haug saying he was told by the British delegate that this was to allow the British secret service to eavesdrop more easily. The British proposed a key length of 48 bits, while the West Germans wanted stronger encryption to protect against East German spying, so the compromise became a key length of 54 bits.Known-plaintext attacksThe first attack on the A5/1 was proposed by Ross Anderson in 1994. Anderson\\'s basic idea was to guess the complete content of the registers R1 and R2 and about half of the register R3. In this way the clocking of all three registers is determined and the second half of R3 can be computed.In 1997, Golic presented an attack based on solving sets of linear equations which has a time complexity of 240.16 (the units are in terms of number of solutions of a system of linear equations which are required).In 2000, Alex Biryukov, Adi Shamir and David Wagner showed that A5/1 can be cryptanalysed in real time using a time-memory tradeoff attack, based on earlier work by Jovan Golic. One tradeoff allows an attacker to reconstruct the key in one second from two minutes of known plaintext or in several minutes from two seconds of known plain text, but he must first complete an expensive preprocessing stage which requires 248 steps to compute around 300 GB of data. Several tradeoffs between preprocessing, data requirements, attack time and memory complexity are possible.The same year, Eli Biham and Orr Dunkelman also published an attack on A5/1 with a total work complexity of 239.91 A5/1 clockings given 220.8 bits of known plaintext. The attack requires 32 GB of data storage after a precomputation stage of 238.Ekdahl and Johansson published an attack on the initialisation procedure which breaks A5/1 in a few minutes using two to five minutes of conversation plaintext. This attack does not require a preprocessing stage. In 2004, Maximov et al. improved this result to an attack requiring \"less than one minute of computations, and a few seconds of known conversation\". The attack was further improved by Elad Barkan and Eli Biham in 2005.Attacks on A5/1 as used in GSMIn 2003, Barkan et al. published several attacks on GSM encryption. The first is an active attack. GSM phones can be convinced to use the much weaker A5/2 cipher briefly. A5/2 can be broken easily, and the phone uses the same key as for the stronger A5/1 algorithm. A second attack on A5/1 is outlined, a ciphertext-only time-memory tradeoff attack which requires a large amount of precomputation.In 2006, Elad Barkan, Eli Biham, Nathan Keller published the full version of their 2003 paper, with attacks against A5/X сiphers. The authors claim: We present a very practical ciphertext-only cryptanalysis of GSM encrypted communication, and various active attacks on the GSM protocols. These attacks can even break into GSM networks that use \"unbreakable\" ciphers. We first describe a ciphertext-only attack on A5/2 that requires a few dozen milliseconds of encrypted off-the-air cellular conversation and finds the correct key in less than a second on a personal computer. We extend this attack to a (more complex) ciphertext-only attack on A5/1. We then describe new (active) attacks on the protocols of networks that use A5/1, A5/3, or even GPRS. These attacks exploit flaws in the GSM protocols, and they work whenever the mobile phone supports a weak cipher such as A5/2. We emphasize that these attacks are on the protocols, and are thus applicable whenever the cellular phone supports a weak cipher, for example, they are also applicable for attacking A5/3 networks using the cryptanalysis of A5/1. Unlike previous attacks on GSM that require unrealistic information, like long known plaintext periods, our attacks are very practical and do not require any knowledge of the content of the conversation. Furthermore, we describe how to fortify the attacks to withstand reception errors. As a result, our attacks allow attackers to tap conversations and decrypt them either in real-time, or at any later time.In 2007 Universities of Bochum and Kiel started a research project to create a massively parallel FPGA-based cryptographic accelerator COPACOBANA. COPACOBANA was the first commercially available solution using fast time-memory trade-off techniques that could be used to attack the popular A5/1 and A5/2 algorithms, used in GSM voice encryption, as well as the Data Encryption Standard (DES). It also enables brute force attacks against GSM eliminating the need of large precomputed lookup tables.In 2008, the group The Hackers Choice launched a project to develop a practical attack on A5/1. The attack requires the construction of a large look-up table of approximately 3 terabytes. Together with the scanning capabilities developed as part of the sister project, the group expected to be able to record any GSM call or SMS encrypted with A5/1, and within about 3–5 minutes derive the encryption key and hence listen to the call and read the SMS in clear.  But the tables weren\\'t released.A similar effort, the A5/1 Cracking Project, was announced at the 2009 Black Hat security conference by cryptographers Karsten Nohl and Sascha Krißler. It created the look-up tables using Nvidia GPGPUs via a peer-to-peer distributed computing architecture.  Starting in the middle of September 2009, the project ran the equivalent of 12 Nvidia GeForce GTX 260. According to the authors, the approach can be used on any cipher with key size up to 64-bits.In December 2009, the A5/1 Cracking Project attack tables for A5/1 were announced by Chris Paget and Karsten Nohl. The tables use a combination of compression techniques, including rainbow tables and distinguished point chains. These tables constituted only parts of the 1.7 TB completed table and had been computed during three months using 40 distributed CUDA nodes and then published over BitTorrent. More recently the project has announced a switch to faster ATI Evergreen code, together with a change in the format of the tables and Frank A. Stevenson announced breaks of A5/1 using the ATI generated tables.Documents leaked by Edward Snowden in 2013 state that the NSA \"can process encrypted A5/1\".See alsoA5/2KASUMI, also known as A5/3Cellular Message Encryption AlgorithmNotesReferencesRose, Greg (10 September 2003). \"A precis of the new attacks on GSM encryption\" (PDF). QUALCOMM Australia. Archived from the original (PDF) on 27 September 2011. Retrieved 17 October 2004.Maximov, Alexander; Thomas Johansson; Steve Babbage (2004). \"An Improved Correlation Attack on A5/1\". Selected Areas in Cryptography 2004: 1–18.External linksBriceno, Marc; Ian Goldberg; David Wagner (23 October 1999). \"A pedagogical implementation of the GSM A5/1 and A5/2 \"voice privacy\" encryption algorithms\". Archived from the original on 8 October 2018. Retrieved 23 January 2017.\"Huge GSM flaw allows hackers to listen in on voice calls\". 25 August 2009. Archived from the original on 14 October 2009.Horesh, Hadar (3 September 2003). \"Technion team cracks GSM cellular phone encryption\" (PDF). Haaretz. Archived from the original (PDF) on 3 March 2016. Retrieved 15 September 2019.Barkan, Elad; Eli Biham; Nathan Keller (July 2006). \"Instant Ciphertext-Only Cryptanalysis of GSM Encrypted Communication (Technical Report CS-2006-07)\".\"Nathan Keller\\'s Homepage\". Archived from the original on 4 June 2008.\"Animated SVG showing A5/1 stream cypher\". Archived from the original on 26 March 2012.',\n",
              " '= A5/2 =A5/2 is a stream cipher used to provide voice privacy in the GSM cellular telephone protocol. It was designed in 1992-1993 (finished March 1993) as a replacement for the relatively stronger (but still weak) A5/1, to allow the GSM standard to be exported to countries \"with restrictions on the import of products with cryptographic security features\".The cipher is based on a combination of four linear-feedback shift registers with irregular clocking and a non-linear combiner.In 1999, Ian Goldberg and David A. Wagner cryptanalyzed A5/2 in the same month it was reverse engineered, and showed that it was extremely weak – so much so that low end equipment can probably break it in real time.In 2003, Elad Barkan, Eli Biham and Nathan Keller presented a ciphertext-only attack based on the error correcting codes used in GSM communication. They also demonstrated a vulnerability in the GSM protocols that allows a man-in-the-middle attack to work whenever the mobile phone supports A5/2, regardless of whether is was actually being used.Since July 1, 2006, the GSMA (GSM Association) mandated that GSM Mobile Phones will not support the A5/2 Cipher any longer, due to its weakness, and the fact that A5/1 is deemed mandatory by the 3GPP association. In July 2007, the 3GPP has approved a change request to prohibit the implementation of A5/2 in any new mobile phones, stating: \"It is mandatory for A5/1 and non encrypted mode to be implemented in mobile stations. It is prohibited to implement A5/2 in mobile stations.\" If the network does not support A5/1 then an unencrypted connection can be used.See alsoA5/1KASUMI, also known as A5/3ReferencesExternal linksA5/2 at CryptoDoxA5/2 withdrawal at security.osmocom.orgIan Goldberg, David Wagner, Lucky Green. The (Real-Time) Cryptanalysis of A5/2. Rump session of Crypto\\'99, 1999.Barkam, Elad; Biham, Eli; Keller, Nathan (2008), \"Instant Ciphertext-Only Cryptanalysis of GSM Encrypted Communication\" (PDF), Journal of Cryptology, 21 (3): 392–429, doi:10.1007/s00145-007-9001-y, S2CID 459117Tool for cracking the GSM A5/2 cipher, written by Nicolas Paglieri and Olivier Benjamin: A52HackTool (with full source code – C language – GNU GPL)',\n",
              " '= ABC (stream cipher) =In cryptography, ABC is a stream cypher algorithm developed by Vladimir Anashin, Andrey Bogdanov, Ilya Kizhvatov, and Sandeep Kumar. It has been submitted to the eSTREAM Project of the eCRYPT network.== References ==',\n",
              " '= ADFGVX cipher =In cryptography, the ADFGVX cipher was a manually applied field cipher used by the Imperial German Army during World War I. It was used to transmit messages secretly using wireless telegraphy. ADFGVX was in fact an extension of an earlier cipher called ADFGX which was first used on 1 March 1918 on the German Western Front. ADFGVX was applied from 1 June 1918 on both the Western Front and Eastern Front.Invented by the Germans signal corps officers Lieutenant Fritz Nebel (1891–1977) and introduced in March 1918 with the designation \"Secret Cipher of the Radio Operators 1918\" (Geheimschrift der Funker 1918, in short GedeFu 18), the cipher was a fractionating transposition cipher which combined a modified Polybius square with a single columnar transposition.The cipher is named after the six possible letters used in the ciphertext: A, D, F, G, V and X. The letters were chosen deliberately because they are very different from one another in the Morse code. That reduced the possibility of operator error.Nebel designed the cipher to provide an army on the move with encryption that was more convenient than trench codes but was still secure. In fact, the Germans believed the ADFGVX cipher was unbreakable.OperationFor the plaintext message, \"Attack at once\", a secret mixed alphabet is first filled into a 5 × 5 Polybius square:i and j have been combined to make the alphabet fit into a 5 × 5 grid.By using the square, the message is converted to fractionated form:The first letter of each ciphertext pair is the row, and the second ciphertext letter is the column, of the plaintext letter in the grid (e.g., \"AF\" means \"row A, column F, in the grid\").Next, the fractionated message is subject to a columnar transposition. The message is written in rows under a transposition key (here \"CARGO\"):C A R G O_________A F A D AD A F G FD X A F AD D F F XG F X FNext, the letters are sorted alphabetically in the transposition key (changing CARGO to ACGOR) by rearranging the columns beneath the letters along with the letters themselves:A C G O R_________F A D A AA D G F FX D F A AD D F X FF G F   XThen, it is read off in columns, in keyword order, which yields the ciphertext:FAXDF ADDDG DGFFF AFAX AFAFXIn practice, the transposition keys were about two dozen characters long. Long messages sent in the ADFGX cipher were broken into sets of messages of different and irregular lengths to make it invulnerable to multiple anagramming. Both the transposition keys and the fractionation keys were changed daily.ADFGVXIn June 1918, an additional letter, V, was added to the cipher. That expanded the grid to 6 × 6, allowing 36 characters to be used. That allowed the full alphabet (instead of combining I and J) and the digits from 0 to 9. That mainly had the effect of considerably shortening messages containing many numbers.The cipher is based on the 6 letters ADFGVX. In the following example the alphabet is coded with the Dutch codeword \\'nachtbommenwerper\\'. This results in the alphabet: NACHTBOMEWRPDFGIJKLQSUVXYZ. This creates the table below with the letters ADFGVX as column headings and row identifiers:The text \\'attack at 1200am\\' translates to this:Then, a new table is created with a key as a heading. Let\\'s use \\'PRIVACY\\' as a key. Usually much longer keys or even phrases were used.The columns are sorted alphabetically, based on the keyword, and the table changes to this:Then, appending the columns to each other results in this ciphertext:DGDD DAGD DGAF ADDF DADV DVFA ADVXWith the keyword, the columns can be reconstructed and placed in the correct order. When using the original table containing the secret alphabet, the text can be deciphered.This cipher might be modified by transposing the rows as well as the columns, creating a harder but improved cipher.CryptanalysisADFGVX was cryptanalysed by French Army Lieutenant Georges Painvin, and the cipher was broken in early June 1918. The work was exceptionally difficult by the standards of classical cryptography, and Painvin became physically ill during it. His method of solution relied on finding messages with stereotyped beginnings, which would fractionate them and then form similar patterns in the positions in the ciphertext that had corresponded to column headings in the transposition table. (Considerable statistical analysis was required after that step had been reached, all done by hand.) It was thus effective only during times of very high traffic, but that was also when the most important messages were sent.However, that was not the only trick that Painvin used to crack the ADFGX cipher. He also used repeating sections of ciphertext to derive information about the likely length of the key that was being used. Where the key was an even number of letters in length he knew, by the way the message was enciphered, that each column consisted entirely of letter coordinates taken from the top of the Polybius Square or from the left of the Square, not a mixture of the two. Also, after substitution but before transposition, the columns would alternately consist entirely of \"top\" and \"side\" letters. One of the characteristics of frequency analysis of letters is that while the distributions of individual letters may vary widely from the norm, the law of averages dictates that groups of letters vary less. With the ADFGX cipher, each \"side\" letter or \"top\" letter is associated with five plaintext letters. In the example above, the \"side\" letter \"D\" is associated with the plaintext letters \"d h o z k\", and the \"top\" letter \"D\" is associated with the plaintext letters \"t h f j r\". Since the two groups of five letters have different cumulative frequency distributions, a frequency analysis of the \"D\" letter in columns consisting of \"side\" letters has a distinctively different result from those of the \"D\" letter in columns consisting of \"top\" letters. That trick allowed Painvin to guess which columns consisted of \"side\" letters and which columns consisted of \"top\" letters. He could then pair them up and perform a frequency analysis on the pairings to see if the pairings were only noise or corresponding to plaintext letters. Once he had the proper pairings, he could then use frequency analysis to figure out the actual plaintext letters. The result was still transposed, but to unscramble a simple transposition was all that he still had to do. Once he determined the transposition scheme for one message, he would then be able to crack any other message that was enciphered with the same transposition key.Painvin broke the ADFGX cipher in April 1918, a few weeks after the Germans launched their Spring Offensive. As a direct result, the French army discovered where Erich Ludendorff intended to attack. The French concentrated their forces at that point, which has been claimed to have stopped the Spring Offensive.However, the claim that Painvin\\'s breaking of the ADFGX cipher stopped the German spring offensive of 1918, while frequently made, is disputed by some. In his 2002 review of Sophie de Lastours\\' book on the subject, La France gagne la guerre des codes secrets 1914-1918, in the Journal of Intelligence History, (Journal of Intelligence History: volume 2, Number 2, Winter 2002)  Hilmar-Detlef Brückner stated:Regrettably, Sophie de Lastours subscribes to the traditional French view that the solving of a German ADFGVX-telegram by Painvin at the beginning of June 1918 was decisive for the Allied victory in the First World War because it gave timely warning of a forthcoming German offensive meant to reach Paris and to inflict a critical defeat on the Allies. However, it has been known for many years, that the German Gneisenau attack of 11 June was staged to induce the French High Command to rush in reserves from the area up north, where the Germans intended to attack later on.Its aim had to be grossly exaggerated, which the German High Command did by spreading rumors that the attack was heading for Paris and beyond; the disinformation was effective and apparently still is. However, the German offensive was not successful because the French had enough reserves at hand to stop the assault and so did not need to bring in additional reinforcements.Moreover, it is usually overlooked that the basic version of the ADFGVX cipher had been created especially for the German Spring Offensive in 1918, meant to deal the Allies a devastating blow. It was hoped that the cipher ADFGX would protect German communications against Allied cryptographers during the assault, which happened.Telegrams in ADFGX appeared for the first time on 5 March, and the German attack started on 21 March. When Painvin presented his first solution of the code on 5 April, the German offensive had already petered out.The ADFGX and ADFGVX ciphers are now regarded as insecure.ReferencesSourcesChilds, J. Rives, General Solution of the ADFGVX Cipher System, Aegean Park Press, ISBN 0-89412-284-3.Friedman, William F. Military Cryptanalysis, Part IV: Transposition and Fractionating Systems. Laguna Hills, California: Aegean Park Press, 1992.External linksA JavaScript implementation of the ADFGVX cipherAnother JavaScript implementationA C implementation of the ADFGVX cipher']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "search_engine_ok.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "91d1f1e3490647d09158101bfd7cca54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_45230a2b1ea1404a82b323bde2304476",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_555f2e5a4c454f7f81aea5a01b50d106",
              "IPY_MODEL_6c3d6da05e924be0bdcf51251514978c",
              "IPY_MODEL_bb528ae73b124df983afd63b308c81e3"
            ]
          }
        },
        "45230a2b1ea1404a82b323bde2304476": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "555f2e5a4c454f7f81aea5a01b50d106": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a75cdf6f70724adb893cd1d236da334c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bad162f4812b4cdeac29fe30b0b1b8d6"
          }
        },
        "6c3d6da05e924be0bdcf51251514978c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_916667e7b92c4a36b3413470f2ff47b2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1047,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1047,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b9e5eeafc92848419a0df84e24bbfbad"
          }
        },
        "bb528ae73b124df983afd63b308c81e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7b86f8725b2c49ae87d4a5c1aa0e3337",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1047/1047 [00:00&lt;00:00, 4178.59it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0fb6516d15cb44438c429ae9a427dda2"
          }
        },
        "a75cdf6f70724adb893cd1d236da334c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bad162f4812b4cdeac29fe30b0b1b8d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "916667e7b92c4a36b3413470f2ff47b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b9e5eeafc92848419a0df84e24bbfbad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7b86f8725b2c49ae87d4a5c1aa0e3337": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0fb6516d15cb44438c429ae9a427dda2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}